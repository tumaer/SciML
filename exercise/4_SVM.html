

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>4. Support Vector Machines &#8212; Introduction to Scientific Machine Learning for Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'exercise/4_SVM';</script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Gaussian Processes" href="5_GPs.html" />
    <link rel="prev" title="3. Optimization" href="3_optimization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../about.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lecture/introduction.html">Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lecture/cc-1-0-basics.html">Core Content 1: Basics</a><input checked class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-1-1-linear.html">Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-1-2-gmm-mcmc.html">GMM and MC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-1-3-bayes.html">Bayesian methods</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lecture/cc-2-0-optim.html">Core Content 2: Optimization</a><input checked class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-2-1-algorithms.html">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-2-2-tricks.html">Tricks of Optimization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lecture/cc-3-0-ml.html">Core Content 3: Classic ML</a><input checked class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-3-1-svm.html">Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-3-2-gp.html">Gaussian Processes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../lecture/cc-4-0-dl.html">Core Content 4: Deep Learning</a><input checked class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-4-1-gradients.html">Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-4-2-mlp.html">Multilayer Perceptron</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-4-3-cnn.html">Convolutional Neural Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-4-4-rnn.html">Recurrent Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lecture/cc-4-5-ae.html">Encoder-Decoder Models</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercise</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_linReg_logReg.html">1. Linear Regression and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_BayesianInference.html">2. Bayesian Linear and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_optimization.html">3. Optimization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_GPs.html">5. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_CNNs.html">6. CNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_RNNs.html">7. RNNs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../admin.html">Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preliminary_knowledge.html">Preliminary Knowledge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software.html">Software Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../practical_exam.html">Practical Exam WS22/23</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/arturtoshev/SciML22-23/blob/master/exercise/4_SVM.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/arturtoshev/SciML22-23" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/arturtoshev/SciML22-23/issues/new?title=Issue%20on%20page%20%2Fexercise/4_SVM.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/exercise/4_SVM.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>4. Support Vector Machines</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-dataset">4.1 Artificial Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-minimal-optimization-smo">4.2 Sequential Minimal Optimization (SMO)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-classification-with-svm-and-smo">4.3 Multiclass Classification with SVM and SMO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-optimization-of-soft-margin-classifier">4.4 Gradient Descent Optimization of Soft Margin Classifier</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="support-vector-machines">
<h1>4. Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this heading">#</a></h1>
<p>At the end of this exercise you will know:</p>
<ul class="simple">
<li><p>How to train a SVM using Sequential Minimal Optimization (SMO)</p></li>
<li><p>How to train a SVM using Gradient Descent (GD)</p></li>
<li><p>How different SVM Kernels perform</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Summary of the mathematical formalism</strong></p>
<p>Soft Margin SVM Lagrangian:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\omega, b, \xi, \alpha, \mu)=\frac{1}{2} \omega^{T} \omega+C \sum_{i=1}^{m} \xi_{i} -\sum_{i=1}^{m} \alpha_{i}\left[y^{(i)}\left(\omega^{\top} x^{(i)}+b\right)-1+\xi_{i}\right]-\sum_{i=1}^{m} \mu_{i} \xi_{i}.
\]</div>
<p>Primal problem:</p>
<div class="math notranslate nohighlight">
\[
\min _{\omega, b} \left( \frac{1}{2}\|\omega\|^{2}+C \sum_{i=1}^{m} \xi_{i}\right)
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{s.t.}\left\{\begin{array}{l}y^{(i)}\left(\omega^{\top} x^{(i)}+b\right) \geq 1-\xi_{i}, \quad i=1, \ldots, m \\ \xi_{i} \ge 0, \quad i=1, \ldots, \mathrm{m}\end{array}\right.
\end{split}\]</div>
<p>Dual problem:</p>
<div class="math notranslate nohighlight">
\[
\max_{\alpha} \left( \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i,j=1}^{m} y^{(i)} y^{(j)} \alpha_{i} \alpha_{j} K(x^{(i)}, x^{(j)}) \right)
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\text { s.t. }\left\{\begin{array}{l}
0 \leq \alpha_{i} \leq C, \quad i=1, \ldots, m \\
\sum_{i=1}^{m} \alpha_{i} y^{(i)}=0
\end{array}\right.
\end{split}\]</div>
<p>In the dual problem, we have used that <span class="math notranslate nohighlight">\(w=\sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}\)</span> and <span class="math notranslate nohighlight">\(\sum_{i=1}^{m} \alpha_{i} y^{(i)}=0\)</span> to get rid of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. To then find <span class="math notranslate nohighlight">\(b\)</span>, we use the heuristic <span class="math notranslate nohighlight">\(b^* = \frac{1}{m_{\Sigma}} \sum_{j=1}^{m_{\Sigma}}\left(y^{(j)}-\sum_{i=1}^{m_{\Sigma}} \alpha_{i}^{*} y^{(i)}K(x^{(i)}, x^{(j)})\right)\)</span> over the support vectors <span class="math notranslate nohighlight">\(m_{\Sigma}\)</span>. Note, only in the linear case, the kernel becomes <span class="math notranslate nohighlight">\(K(x^{(i)}, x^{(j)}) = \left\langle x^{(i)}, x^{(j)}\right\rangle\)</span>. Also note, that only in the dual problem definition we encounter the kernel and can use the kernel trick.</p>
<blockquote>
<div><p>Note: for <span class="math notranslate nohighlight">\(C \to \infty\)</span> this problem ends up being the Hard Margin SVM.</p>
</div></blockquote>
<section id="artificial-dataset">
<h2>4.1 Artificial Dataset<a class="headerlink" href="#artificial-dataset" title="Permalink to this heading">#</a></h2>
<p>We use scikit-learn and <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html"><code class="docutils literal notranslate"><span class="pre">make_blobs</span></code></a> to generate a binary dataset with input features <span class="math notranslate nohighlight">\(x\in \mathbb{R}^2\)</span> and labels <span class="math notranslate nohighlight">\(y\in \{-1, +1\}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># X as features and Y as labels</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="c1"># by default the labels are {0, 1}, so we change them to {-1,1}</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># we also center the input data (per dimension) and scale it to unit variance to make trainig more efficient</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x7f847ea27af0&gt;
</pre></div>
</div>
<img alt="../_images/9460151d768c541068b7652ad6bb53081b1326e36dfedabbe8d4272d8292db58.png" src="../_images/9460151d768c541068b7652ad6bb53081b1326e36dfedabbe8d4272d8292db58.png" />
</div>
</div>
</section>
<section id="sequential-minimal-optimization-smo">
<h2>4.2 Sequential Minimal Optimization (SMO)<a class="headerlink" href="#sequential-minimal-optimization-smo" title="Permalink to this heading">#</a></h2>
<p>This algorithm was originally developed by <a class="reference external" href="http://research.microsoft.com/pubs/69644/tr-98-14.pdf">John Platt in 1998</a> and is optimized for SVM optimization. This algorithm solves the dual problem in a gradient-free manner. It selects two multiplier <span class="math notranslate nohighlight">\(\alpha_i\)</span> and <span class="math notranslate nohighlight">\(\alpha_j\)</span> and optimizes them while keeping all other <span class="math notranslate nohighlight">\(\alpha\)</span>s constant. And then itertively repeats the procedure over all <span class="math notranslate nohighlight">\(\alpha\)</span>s. The efficiency lies in the heuristic used for selecting two <span class="math notranslate nohighlight">\(\alpha\)</span> values, which is based on information from previous iterations. In the end we obtain a vector of <span class="math notranslate nohighlight">\(M\)</span> values for <span class="math notranslate nohighlight">\(\alpha\)</span> corresponding to each training data point, for which most of the <span class="math notranslate nohighlight">\(\alpha\)</span> values are <span class="math notranslate nohighlight">\(0\)</span> and only the non-zero values contribute to the predictions made by the model.</p>
<p>We adapt the implementation of the SMO algorithm from <a class="reference external" href="https://jonchar.net/notebooks/SVM/">this</a> reference code by Jon Charest.</p>
<p><strong>Visualization Utils</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plots the model&#39;s decision boundary on the input axes object.</span>
<span class="sd">        Range of decision boundary grid is determined by the training data.</span>
<span class="sd">        Returns decision boundary grid and axes object (`grid`, `ax`).&quot;&quot;&quot;</span>

    <span class="c1"># Generate coordinate grid of shape [resolution x resolution]</span>
    <span class="c1"># and evaluate the model over the entire space</span>
    <span class="n">xrange</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">resolution</span><span class="p">)</span>
    <span class="n">yrange</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">resolution</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="p">[[</span><span class="n">decision_function</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span>
                               <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">,</span>
                               <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xr</span><span class="p">,</span> <span class="n">yr</span><span class="p">]),</span> <span class="n">model</span><span class="o">.</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">xr</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">]</span> <span class="k">for</span> <span class="n">yr</span> <span class="ow">in</span> <span class="n">yrange</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xrange</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">yrange</span><span class="p">))</span>

    <span class="c1"># Plot decision contours using grid and</span>
    <span class="c1"># make a scatter plot of training data</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xrange</span><span class="p">,</span> <span class="n">yrange</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">levels</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="n">linestyles</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
               <span class="n">c</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

    <span class="c1"># Plot support vectors (non-zero alphas)</span>
    <span class="c1"># as circled points (linewidth &gt; 0)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">!=</span> <span class="mf">0.0</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
               <span class="n">c</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grid</span><span class="p">,</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<p>As a first step, we define a generic SMO model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SMOModel</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Container object for the model used for sequential minimal optimization.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">alphas</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">errors</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>               <span class="c1"># training data vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span>               <span class="c1"># class label vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>               <span class="c1"># regularization parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>     <span class="c1"># kernel function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="n">alphas</span>     <span class="c1"># lagrange multiplier vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>               <span class="c1"># scalar bias term</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">errors</span> <span class="o">=</span> <span class="n">errors</span>     <span class="c1"># error cache used for selection of alphas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_obj</span> <span class="o">=</span> <span class="p">[]</span>           <span class="c1"># record of objective function value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>     <span class="c1"># store size of training set</span>
</pre></div>
</div>
</div>
</div>
<p>The next thing we need to define is the kernel. We start with the simplest linear kernel</p>
<div class="math notranslate nohighlight">
\[K(x,x') = x^{\top} x' + b.\]</div>
<p>The implementation of the radial basis function</p>
<div class="math notranslate nohighlight">
\[K(x,x') = \exp \left\{ - \gamma ||x-x'||_2^2 \right\} \]</div>
<p>we leave as an exercise for you.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the linear combination of arrays `x` and `y` with</span>
<span class="sd">    the optional bias term `b` (set to 1 by default).&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">b</span>  <span class="c1"># Note the @ operator for matrix multiplication</span>

<span class="k">def</span> <span class="nf">gaussian_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the gaussian similarity of arrays `x` and `y` with</span>
<span class="sd">    kernel inverse width parameter `gamma` (set to 1 by default).&quot;&quot;&quot;</span>
    
    <span class="c1">######################</span>
    <span class="c1"># TODO: you might find this helpful: https://jonchar.net/notebooks/SVM/</span>
    
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">-</span>
                        <span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">result</span>
    <span class="c1">#######################</span>
</pre></div>
</div>
</div>
</div>
<p>Now, using the dual problem formulation and a <code class="docutils literal notranslate"><span class="pre">kernel</span></code>, we define the objective and decision functions.
The decision function simple imlements <span class="math notranslate nohighlight">\((\omega x + b)\)</span> by using the kernel trick and the relation <span class="math notranslate nohighlight">\(w=\sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Objective function to optimize, i.e. loss function</span>

<span class="k">def</span> <span class="nf">objective_function</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">X_train</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the SVM objective function based in the input model defined by:</span>
<span class="sd">    `alphas`: vector of Lagrange multipliers</span>
<span class="sd">    `target`: vector of class labels (-1 or 1) for training data</span>
<span class="sd">    `kernel`: kernel function</span>
<span class="sd">    `X_train`: training data for model.&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">target</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">target</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">alphas</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">alphas</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]))</span>


<span class="c1"># Decision function, i.e. forward model evaluation</span>

<span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Applies the SVM decision function to the input feature vectors in `x_test`.&quot;&quot;&quot;</span>

    <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">alphas</span> <span class="o">*</span> <span class="n">target</span><span class="p">)</span> <span class="o">@</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<p><strong>The SMO algorithm</strong></p>
<p>We are now ready to implement the SMO algorithm as given in Platt’s paper. The implementation is split into three functions: <code class="docutils literal notranslate"><span class="pre">take_step</span></code>, <code class="docutils literal notranslate"><span class="pre">examine_example</span></code>, and <code class="docutils literal notranslate"><span class="pre">train</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">train</span></code> is the main training loop and also implements the selection of the first of the two <span class="math notranslate nohighlight">\(\alpha\)</span> values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">examine_example</span></code> implements the selection of the second <span class="math notranslate nohighlight">\(\alpha\)</span> value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">take_step</span></code> optimizes the two <span class="math notranslate nohighlight">\(\alpha\)</span> values, the bias <span class="math notranslate nohighlight">\(b\)</span>, and the cache.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">take_step</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>

    <span class="c1"># Skip if chosen alphas are the same</span>
    <span class="k">if</span> <span class="n">i1</span> <span class="o">==</span> <span class="n">i2</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="n">model</span>

    <span class="n">alph1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">[</span><span class="n">i1</span><span class="p">]</span>
    <span class="n">alph2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">[</span><span class="n">i1</span><span class="p">]</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span>
    <span class="n">E1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="n">i1</span><span class="p">]</span>
    <span class="n">E2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">y1</span> <span class="o">*</span> <span class="n">y2</span>

    <span class="c1"># Compute L &amp; H, the bounds on new possible alpha values</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">y1</span> <span class="o">!=</span> <span class="n">y2</span><span class="p">):</span>
        <span class="n">L</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">alph2</span> <span class="o">-</span> <span class="n">alph1</span><span class="p">)</span>
        <span class="n">H</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span> <span class="o">+</span> <span class="n">alph2</span> <span class="o">-</span> <span class="n">alph1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">y1</span> <span class="o">==</span> <span class="n">y2</span><span class="p">):</span>
        <span class="n">L</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">alph1</span> <span class="o">+</span> <span class="n">alph2</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
        <span class="n">H</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="n">alph1</span> <span class="o">+</span> <span class="n">alph2</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">L</span> <span class="o">==</span> <span class="n">H</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="n">model</span>

    <span class="c1"># Compute kernel &amp; 2nd derivative eta</span>
    <span class="n">k11</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">i1</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">i1</span><span class="p">])</span>
    <span class="n">k12</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">i1</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">i2</span><span class="p">])</span>
    <span class="n">k22</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">i2</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">i2</span><span class="p">])</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k12</span> <span class="o">-</span> <span class="n">k11</span> <span class="o">-</span> <span class="n">k22</span>

    <span class="c1"># Compute new alpha 2 (a2) if eta is negative</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">eta</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">alph2</span> <span class="o">-</span> <span class="n">y2</span> <span class="o">*</span> <span class="p">(</span><span class="n">E1</span> <span class="o">-</span> <span class="n">E2</span><span class="p">)</span> <span class="o">/</span> <span class="n">eta</span>
        <span class="c1"># Clip a2 based on bounds L &amp; H</span>
        <span class="k">if</span> <span class="n">L</span> <span class="o">&lt;</span> <span class="n">a2</span> <span class="o">&lt;</span> <span class="n">H</span><span class="p">:</span>
            <span class="n">a2</span> <span class="o">=</span> <span class="n">a2</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">a2</span> <span class="o">&lt;=</span> <span class="n">L</span><span class="p">):</span>
            <span class="n">a2</span> <span class="o">=</span> <span class="n">L</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">a2</span> <span class="o">&gt;=</span> <span class="n">H</span><span class="p">):</span>
            <span class="n">a2</span> <span class="o">=</span> <span class="n">H</span>

    <span class="c1"># If eta is non-negative, move new a2 to bound with greater objective function value</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">alphas_adj</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">alphas_adj</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span> <span class="o">=</span> <span class="n">L</span>
        <span class="c1"># objective function output with a2 = L</span>
        <span class="n">Lobj</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">alphas_adj</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
        <span class="n">alphas_adj</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span>
        <span class="c1"># objective function output with a2 = H</span>
        <span class="n">Hobj</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">(</span><span class="n">alphas_adj</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">Lobj</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">Hobj</span> <span class="o">+</span> <span class="n">eps</span><span class="p">):</span>
            <span class="n">a2</span> <span class="o">=</span> <span class="n">L</span>
        <span class="k">elif</span> <span class="n">Lobj</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">Hobj</span> <span class="o">-</span> <span class="n">eps</span><span class="p">):</span>
            <span class="n">a2</span> <span class="o">=</span> <span class="n">H</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">a2</span> <span class="o">=</span> <span class="n">alph2</span>

    <span class="c1"># Push a2 to 0 or C if very close</span>
    <span class="k">if</span> <span class="n">a2</span> <span class="o">&lt;</span> <span class="mf">1e-8</span><span class="p">:</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">elif</span> <span class="n">a2</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">C</span> <span class="o">-</span> <span class="mf">1e-8</span><span class="p">):</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span>

    <span class="c1"># If examples can&#39;t be optimized within epsilon (eps), skip this pair</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">a2</span> <span class="o">-</span> <span class="n">alph2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">eps</span> <span class="o">*</span> <span class="p">(</span><span class="n">a2</span> <span class="o">+</span> <span class="n">alph2</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)):</span>
        <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="n">model</span>

    <span class="c1"># Calculate new alpha 1 (a1)</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">alph1</span> <span class="o">+</span> <span class="n">s</span> <span class="o">*</span> <span class="p">(</span><span class="n">alph2</span> <span class="o">-</span> <span class="n">a2</span><span class="p">)</span>

    <span class="c1"># Update threshold b to reflect newly calculated alphas</span>
    <span class="c1"># Calculate both possible thresholds</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">E1</span> <span class="o">+</span> <span class="n">y1</span> <span class="o">*</span> <span class="p">(</span><span class="n">a1</span> <span class="o">-</span> <span class="n">alph1</span><span class="p">)</span> <span class="o">*</span> <span class="n">k11</span> <span class="o">+</span> <span class="n">y2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a2</span> <span class="o">-</span> <span class="n">alph2</span><span class="p">)</span> <span class="o">*</span> <span class="n">k12</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">b</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">E2</span> <span class="o">+</span> <span class="n">y1</span> <span class="o">*</span> <span class="p">(</span><span class="n">a1</span> <span class="o">-</span> <span class="n">alph1</span><span class="p">)</span> <span class="o">*</span> <span class="n">k12</span> <span class="o">+</span> <span class="n">y2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a2</span> <span class="o">-</span> <span class="n">alph2</span><span class="p">)</span> <span class="o">*</span> <span class="n">k22</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">b</span>

    <span class="c1"># Set new threshold based on if a1 or a2 is bound by L and/or H</span>
    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">a1</span> <span class="ow">and</span> <span class="n">a1</span> <span class="o">&lt;</span> <span class="n">C</span><span class="p">:</span>
        <span class="n">b_new</span> <span class="o">=</span> <span class="n">b1</span>
    <span class="k">elif</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">a2</span> <span class="ow">and</span> <span class="n">a2</span> <span class="o">&lt;</span> <span class="n">C</span><span class="p">:</span>
        <span class="n">b_new</span> <span class="o">=</span> <span class="n">b2</span>
    <span class="c1"># Average thresholds if both are bound</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">b1</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

    <span class="c1"># Update model object with new alphas &amp; threshold</span>
    <span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">[</span><span class="n">i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">a1</span>
    <span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span> <span class="o">=</span> <span class="n">a2</span>

    <span class="c1"># Update error cache</span>
    <span class="c1"># Error cache for optimized alphas is set to 0 if they&#39;re unbound</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">alph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">],</span> <span class="p">[</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">]):</span>
        <span class="k">if</span> <span class="mf">0.0</span> <span class="o">&lt;</span> <span class="n">alph</span> <span class="o">&lt;</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Set non-optimized errors based on equation 12.11 in Platt&#39;s book</span>
    <span class="n">non_opt</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">m</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="o">!=</span> <span class="n">i1</span> <span class="ow">and</span> <span class="n">n</span> <span class="o">!=</span> <span class="n">i2</span><span class="p">)]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="n">non_opt</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="n">non_opt</span><span class="p">]</span> <span class="o">+</span> \
        <span class="n">y1</span><span class="o">*</span><span class="p">(</span><span class="n">a1</span> <span class="o">-</span> <span class="n">alph1</span><span class="p">)</span><span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">i1</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">non_opt</span><span class="p">])</span> <span class="o">+</span> \
        <span class="n">y2</span><span class="o">*</span><span class="p">(</span><span class="n">a2</span> <span class="o">-</span> <span class="n">alph2</span><span class="p">)</span> <span class="o">*</span> \
        <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">i2</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">non_opt</span><span class="p">])</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">b</span> <span class="o">-</span> <span class="n">b_new</span>

    <span class="c1"># Update model threshold</span>
    <span class="n">model</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b_new</span>

    <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">examine_example</span><span class="p">(</span><span class="n">i2</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>

    <span class="n">y2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span>
    <span class="n">alph2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span>
    <span class="n">E2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">E2</span> <span class="o">*</span> <span class="n">y2</span>

    <span class="c1"># Proceed if error is within specified tolerance (tol)</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">r2</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">tol</span> <span class="ow">and</span> <span class="n">alph2</span> <span class="o">&lt;</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">r2</span> <span class="o">&gt;</span> <span class="n">tol</span> <span class="ow">and</span> <span class="n">alph2</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)):</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">[(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span> <span class="o">!=</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">)])</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Use 2nd choice heuristic is choose max difference in error</span>
            <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">i1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="n">i2</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">i1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">errors</span><span class="p">)</span>
            <span class="n">step_result</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">take_step</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">step_result</span><span class="p">:</span>
                <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="n">model</span>

        <span class="c1"># Loop through non-zero and non-C alphas, starting at a random point</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span> <span class="o">!=</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">))[</span><span class="mi">0</span><span class="p">],</span>
                          <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">m</span><span class="p">))):</span>
            <span class="n">step_result</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">take_step</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">step_result</span><span class="p">:</span>
                <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="n">model</span>

        <span class="c1"># loop through all alphas, starting at a random point</span>
        <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">m</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">m</span><span class="p">))):</span>
            <span class="n">step_result</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">take_step</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">step_result</span><span class="p">:</span>
                <span class="k">return</span> <span class="mi">1</span><span class="p">,</span> <span class="n">model</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>

    <span class="n">numChanged</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">examineAll</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># loop over each alpha in first round</span>

    <span class="k">while</span> <span class="p">(</span><span class="n">numChanged</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">examineAll</span><span class="p">):</span>
        <span class="n">numChanged</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">examineAll</span><span class="p">:</span>
            <span class="c1"># loop over all training examples</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">examine_result</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">examine_example</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
                <span class="n">numChanged</span> <span class="o">+=</span> <span class="n">examine_result</span>
                <span class="k">if</span> <span class="n">examine_result</span><span class="p">:</span>
                    <span class="n">obj_result</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">_obj</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obj_result</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># loop over examples where alphas are not already at their limits</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span> <span class="o">!=</span> <span class="n">model</span><span class="o">.</span><span class="n">C</span><span class="p">))[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">examine_result</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">examine_example</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
                <span class="n">numChanged</span> <span class="o">+=</span> <span class="n">examine_result</span>
                <span class="k">if</span> <span class="n">examine_result</span><span class="p">:</span>
                    <span class="n">obj_result</span> <span class="o">=</span> <span class="n">objective_function</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">_obj</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">obj_result</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">examineAll</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">examineAll</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">numChanged</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">examineAll</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<p>We are now ready to define the model (after defining some hyperparameters).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set model parameters and initial values</span>
<span class="n">C</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">initial_alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="c1"># Set tolerances</span>
<span class="n">tol</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># error tolerance</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># alpha tolerance</span>

<span class="c1"># Instantiate model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SMOModel</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> 
    <span class="n">kernel</span><span class="o">=</span><span class="n">gaussian_kernel</span><span class="p">,</span> <span class="c1"># TODO: try linear_kernel and  gaussian_kernel</span>
    <span class="n">alphas</span><span class="o">=</span><span class="n">initial_alphas</span><span class="p">,</span>
    <span class="n">b</span><span class="o">=</span><span class="n">initial_b</span><span class="p">,</span>
    <span class="n">errors</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Initialize error cache</span>
<span class="n">initial_error</span> <span class="o">=</span> <span class="n">decision_function</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                                  <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span>
<span class="n">model</span><span class="o">.</span><span class="n">errors</span> <span class="o">=</span> <span class="n">initial_error</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">grid</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/36771bdb411ae6d7d005b11a8b01929261fb20bc6cfda988aaaa4efdfbadd1ba.png" src="../_images/36771bdb411ae6d7d005b11a8b01929261fb20bc6cfda988aaaa4efdfbadd1ba.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># loss curve</span>
<span class="c1"># note: we started with all alphas = 0 and turned some of them on one by one, and then refined.</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">_obj</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7f8476f45970&gt;]
</pre></div>
</div>
<img alt="../_images/55e1f52303a36b8bb33c1718038ffc0aad399abefe2a00525abb9819e0e0087d.png" src="../_images/55e1f52303a36b8bb33c1718038ffc0aad399abefe2a00525abb9819e0e0087d.png" />
</div>
</div>
</section>
<section id="multiclass-classification-with-svm-and-smo">
<h2>4.3 Multiclass Classification with SVM and SMO<a class="headerlink" href="#multiclass-classification-with-svm-and-smo" title="Permalink to this heading">#</a></h2>
<p>We look at a problem we have seen before: the classification of the iris dataset. The task is to use two of the measured input features (“sepal_length” and “sepal_width”) and to build a classifier capable of distinguishing among the three possible flowers, which we index by [0, 1, 2].</p>
<p>Getting the data is equivalent to the process we saw in <a class="reference internal" href="1_linReg_logReg.html"><span class="doc std std-doc">exercise Nr. 1</span></a> in the Logistic Regression section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get iris dataset</span>
<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
<span class="n">iris</span> <span class="o">=</span> <span class="s1">&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#39;</span>
<span class="n">urlretrieve</span><span class="p">(</span><span class="n">iris</span><span class="p">)</span>
<span class="n">df0</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">iris</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>

<span class="c1"># name columns</span>
<span class="n">attributes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sepal_length&quot;</span><span class="p">,</span> <span class="s2">&quot;sepal_width&quot;</span><span class="p">,</span>
              <span class="s2">&quot;petal_length&quot;</span><span class="p">,</span> <span class="s2">&quot;petal_width&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">]</span>
<span class="n">df0</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">attributes</span>

<span class="c1"># add species index</span>
<span class="n">species</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">df0</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="n">df0</span><span class="p">[</span><span class="s2">&quot;class_idx&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df0</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">species</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Count occurence of each class:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df0</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>

<span class="c1"># let&#39;s extract two of the features, and the indexed classes [0,1,2]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df0</span><span class="p">[[</span><span class="s2">&quot;petal_length&quot;</span><span class="p">,</span> <span class="s2">&quot;petal_width&quot;</span><span class="p">,</span> <span class="s2">&quot;class_idx&quot;</span><span class="p">]]</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;class_idx&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training data:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Count occurence of each class:
Iris-versicolor    50
Iris-virginica     50
Iris-setosa        49
Name: class, dtype: int64
Training data:
     petal_length  petal_width  class_idx
0             1.4          0.2          0
1             1.3          0.2          0
2             1.5          0.2          0
3             1.4          0.2          0
4             1.7          0.4          0
..            ...          ...        ...
144           5.2          2.3          2
145           5.0          1.9          2
146           5.2          2.0          2
147           5.4          2.3          2
148           5.1          1.8          2

[149 rows x 3 columns]
</pre></div>
</div>
</div>
</div>
<p><strong>Exercise</strong></p>
<p>Now, your task is to implement a SVM-based multi-class classifier, which can be trained using the SMO algorithm. A solution will be presented during the next exercise session.</p>
<p>Hint: <a class="reference external" href="https://github.com/itsikad/svm-smo">this</a> repository and the one-vs-all classifier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">####################</span>

<span class="k">class</span> <span class="nc">OneVsAll</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">solver</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_binary_clf</span> <span class="o">=</span> <span class="p">[</span><span class="n">solver</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_classes</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_classes</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_binary_clf</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">scores</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">decision_function</span><span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span>
                <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span>
                <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">,</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">model</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

        <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_classes</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_binary_clf</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_binary_clf</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>  <span class="c1"># fit(x_train, y_tmp)</span>


<span class="k">def</span> <span class="nf">create_binary_clf</span><span class="p">(</span><span class="n">class_idx</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">kernel</span><span class="p">):</span>
    <span class="c1"># Set model parameters and initial values</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mf">1.</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">initial_alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">initial_b</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Set tolerances</span>
    <span class="n">tol</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># error tolerance</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># alpha tolerance</span>

    <span class="n">Y_tmp</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">==</span> <span class="n">class_idx</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">!=</span> <span class="n">class_idx</span><span class="p">)</span>

    <span class="c1"># Instantiate model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SMOModel</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">Y_tmp</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span>
        <span class="n">alphas</span><span class="o">=</span><span class="n">initial_alphas</span><span class="p">,</span>
        <span class="n">b</span><span class="o">=</span><span class="n">initial_b</span><span class="p">,</span>
        <span class="n">errors</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Initialize error cache</span>
    <span class="n">initial_error</span> <span class="o">=</span> <span class="n">decision_function</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                                      <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">Y</span>
    <span class="n">model</span><span class="o">.</span><span class="n">errors</span> <span class="o">=</span> <span class="n">initial_error</span>

    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">plot_decision_boundary_multiclass</span><span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plots the model&#39;s decision boundary on the input axes object.</span>
<span class="sd">        Range of decision boundary grid is determined by the training data.</span>
<span class="sd">        Returns decision boundary grid and axes object (`grid`, `ax`).&quot;&quot;&quot;</span>

    <span class="c1"># Generate coordinate grid of shape [resolution x resolution]</span>
    <span class="c1"># and evaluate the model over the entire space</span>
    <span class="n">model0</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">_binary_clf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">xrange</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">model0</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span>
                         <span class="n">model0</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">resolution</span><span class="p">)</span>
    <span class="n">yrange</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">model0</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span>
                         <span class="n">model0</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">resolution</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xrange</span><span class="p">,</span> <span class="n">yrange</span><span class="p">)</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])))</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># shape=(num_samples, dim)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xrange</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">yrange</span><span class="p">))</span>

    <span class="c1"># Plot decision contours using grid and</span>
    <span class="c1"># make a scatter plot of training data</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xrange</span><span class="p">,</span> <span class="n">yrange</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model0</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">model0</span><span class="o">.</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grid</span><span class="p">,</span> <span class="n">ax</span>


<span class="n">solver</span> <span class="o">=</span> <span class="n">OneVsAll</span><span class="p">(</span>
    <span class="n">solver</span><span class="o">=</span><span class="n">create_binary_clf</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">C</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span>
    <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">,</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">gaussian_kernel</span>  <span class="c1"># linear_kernel vs gaussian kernel</span>
<span class="p">)</span>

<span class="n">solver</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">grid</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_decision_boundary_multiclass</span><span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1">####################</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e159cd41c4af2871317532d5fbaa893017108dea6958af6dde113e89eebe6bee.png" src="../_images/e159cd41c4af2871317532d5fbaa893017108dea6958af6dde113e89eebe6bee.png" />
</div>
</div>
</section>
<section id="gradient-descent-optimization-of-soft-margin-classifier">
<h2>4.4 Gradient Descent Optimization of Soft Margin Classifier<a class="headerlink" href="#gradient-descent-optimization-of-soft-margin-classifier" title="Permalink to this heading">#</a></h2>
<p>We can also directly solve the primal problem with gradient-based optimization, if we slightly reformulate it. This reformulation requires using the <a class="reference external" href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{hinge}(x) = \max(0, 1-x)\]</div>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Hinge_loss_vs_zero_one_loss.svg/1280px-Hinge_loss_vs_zero_one_loss.svg.png" alt="drawing" width="500"/>
<p>What we would give as an input to the hinge loss is the “raw” output of the classifier, e.g. for linear SVMs <span class="math notranslate nohighlight">\(out= \omega x + b\)</span>, multiplied with the correct output <span class="math notranslate nohighlight">\(y\)</span>. Thus, the hinge loss of a single sample <span class="math notranslate nohighlight">\(i\)</span> for a linear SVM becomes</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_i = \max \left( 0, \; 1 - y^{(i)}(\omega^{\top} x^{(i)} + b) \right)\]</div>
<p>To fully recover the Soft Margin Classifier, we simply add the squared L2 regularization to this loss, thus the total loss becomes</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \frac{1}{M}\sum_{i=1}^M \max \left( 0, \; 1 - y^{(i)}(\omega^{\top} x^{(i)} + b) \right) + \lambda ||w||_2^2\]</div>
<p>Here, the parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is inversely proportional to the <span class="math notranslate nohighlight">\(C\)</span> from the Soft Margin Classifier formalism.</p>
<p>If we want to do something like kernels (although there is no dot product here), the best we can do is directly applying the input feature transformation <span class="math notranslate nohighlight">\(x \to \varphi(x)\)</span> which leads to the loss</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \frac{1}{M}\sum_{i=1}^M \max \left( 0, \; 1 - y^{(i)}(\omega^{\top} \varphi(x^{(i)}) + b) \right) + \lambda ||w||_2^2\]</div>
<p>However, as you might remember, there is no practically useful <span class="math notranslate nohighlight">\(\varphi\)</span> corresponding to the RBF kernel - there is one, but it is <a class="reference external" href="https://stats.stackexchange.com/questions/123413/using-a-gaussian-kernel-in-svm-how-exactly-is-this-then-written-as-a-dot-produc">an infinitely long sum</a>. Thus, the hinge loss approach is somewhat restrictive.</p>
<p><strong>Visualization Utils</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">visualize_torch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">linear</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    based on </span>
<span class="sd">    https://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html#sphx-glr-auto-examples-svm-plot-svm-margin-py</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">0.02</span>

    <span class="k">if</span> <span class="n">linear</span><span class="p">:</span>

        <span class="c1"># extend bounds by &quot;delta&quot; to improve the plot</span>
        <span class="n">x_min</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">delta</span>
        <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">delta</span>

        <span class="c1"># solving $w0+x1 + w1*x2 + b = 0$ for $x2$ leads to $x2 = -w0/w1 - b/w1$</span>
        <span class="n">a</span> <span class="o">=</span> <span class="o">-</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        <span class="n">yy</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">xx</span> <span class="o">-</span> <span class="n">b</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># $margin = 1 / ||w||_2$</span>
        <span class="c1"># Why? Recall that the distance between a point (x_p, y_P) and a line</span>
        <span class="c1"># $ax+by+c=0$ is given by $|ax_p+by_p+c|/\sqrt{a^2+b^2}$. As we set the</span>
        <span class="c1"># functional margin to 1, i.e. $|ax_i+by_i+c|=1$ for a support vector</span>
        <span class="c1"># point, then the total margin becomes $1 / ||w||_2$.</span>
        <span class="n">margin</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">yy_up</span> <span class="o">=</span> <span class="n">yy</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">a</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">margin</span>
        <span class="n">yy_down</span> <span class="o">=</span> <span class="n">yy</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">a</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">margin</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_up</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_down</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">)</span>
        
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">delta</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">delta</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">xy</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]))</span>
        <span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xy</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        
        <span class="n">cs0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">cs0</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;decision boundary&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">+</span> <span class="n">delta</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">delta</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">+</span> <span class="n">delta</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">delta</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s first define a base SVM class, a linear kernel, the hinge loss, and the regularization over weights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SupportVectorMachine</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">kernel</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># X_train.shape should be (num_samples, dim x)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">phi_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">phi_x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">PhiLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    

<span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hinge loss&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span> <span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">sq_l2_reg</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Squared L2 regularization of weights&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">square</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Exercise</strong></p>
<p>Implement the Radial Basis Function kernel. After that, use your new kernel implementation to run a training process and visualize results.</p>
<p>Hint: <a class="reference external" href="https://gist.github.com/mlaves/c98cd4e6bcb9dbd4d0c03b34bacb0f65">this</a> reference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PhiRBF</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Something like a Radial Basis Function feature map</span>
<span class="sd">    Lifts the dimension from X.shape[-1] to X.shape[0]&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1">##############################</span>
        <span class="c1"># TODO: implement the forward methods</span>
        <span class="c1"># The choice of this specific phi is arbitrary and is not the true RBF.</span>
        <span class="c1"># We lift the input space from the space of `x` to a space of </span>
        <span class="c1"># dimension equal to the number of training data points.</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out</span>
        <span class="c1">##############################</span>
</pre></div>
</div>
</div>
</div>
<p>Some preparation before we train the model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># prepare the data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># set hyperparameters</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">reg_lambda</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">kernel_type</span> <span class="o">=</span> <span class="s1">&#39;rbf&#39;</span>  <span class="c1"># TODO: try both &quot;lin&quot; and &quot;rbf&quot;</span>

<span class="k">if</span> <span class="n">kernel_type</span> <span class="o">==</span> <span class="s1">&#39;lin&#39;</span><span class="p">:</span>
  <span class="n">kernel</span> <span class="o">=</span> <span class="n">PhiLinear</span><span class="p">()</span> 
  <span class="n">input_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">elif</span> <span class="n">kernel_type</span> <span class="o">==</span> <span class="s1">&#39;rbf&#39;</span><span class="p">:</span>
  <span class="n">kernel</span> <span class="o">=</span> <span class="n">PhiRBF</span><span class="p">(</span><span class="n">X_train</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> 
  <span class="n">input_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SupportVectorMachine</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span> 
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># print initial parameters</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;: &quot;</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>linear.weight :  tensor([[ 0.0342,  0.0371, -0.0105,  0.0411, -0.0098,  0.0090, -0.0218,  0.0263,
          0.0394, -0.0328,  0.0389,  0.0084,  0.0330,  0.0061,  0.0216, -0.0063,
          0.0345,  0.0066, -0.0209,  0.0114, -0.0206, -0.0052, -0.0182,  0.0297,
         -0.0353, -0.0206, -0.0126, -0.0269,  0.0042, -0.0442,  0.0404, -0.0380,
          0.0345,  0.0074, -0.0145,  0.0276,  0.0070,  0.0361,  0.0049, -0.0141,
          0.0120, -0.0121,  0.0188,  0.0399,  0.0259, -0.0196,  0.0258,  0.0080,
          0.0227, -0.0273, -0.0443, -0.0173, -0.0343,  0.0367,  0.0129,  0.0185,
          0.0141, -0.0008,  0.0350, -0.0318,  0.0028, -0.0305,  0.0138, -0.0154,
          0.0137, -0.0093,  0.0371, -0.0265, -0.0267, -0.0267,  0.0402,  0.0149,
          0.0430, -0.0369, -0.0444, -0.0350, -0.0301,  0.0181,  0.0160,  0.0372,
         -0.0231, -0.0305,  0.0237, -0.0181,  0.0271, -0.0106,  0.0256, -0.0347,
         -0.0226,  0.0136,  0.0095, -0.0114,  0.0267,  0.0304, -0.0324, -0.0239,
          0.0409, -0.0151, -0.0159, -0.0433, -0.0256,  0.0112, -0.0059, -0.0325,
          0.0010, -0.0305, -0.0379, -0.0246, -0.0391, -0.0285,  0.0447,  0.0084,
          0.0138, -0.0417, -0.0294, -0.0149,  0.0070, -0.0394, -0.0193, -0.0268,
          0.0001, -0.0166, -0.0031, -0.0303, -0.0307, -0.0261, -0.0153, -0.0353,
          0.0375, -0.0089,  0.0385,  0.0139, -0.0379,  0.0309, -0.0123, -0.0171,
         -0.0371, -0.0445,  0.0128, -0.0098,  0.0174, -0.0367,  0.0332, -0.0328,
         -0.0077,  0.0093,  0.0231,  0.0361,  0.0407, -0.0355,  0.0113, -0.0192,
         -0.0049, -0.0335,  0.0407, -0.0328,  0.0239,  0.0157,  0.0145, -0.0242,
          0.0406,  0.0098,  0.0058, -0.0394,  0.0188, -0.0067, -0.0205,  0.0384,
          0.0100, -0.0247, -0.0226, -0.0021,  0.0250, -0.0114, -0.0255, -0.0153,
         -0.0334,  0.0159,  0.0346, -0.0421,  0.0104,  0.0231,  0.0081, -0.0159,
          0.0233,  0.0235,  0.0167, -0.0079, -0.0118,  0.0048, -0.0079, -0.0133,
          0.0286,  0.0384, -0.0044, -0.0100,  0.0007, -0.0027,  0.0108,  0.0125,
         -0.0406, -0.0165,  0.0377,  0.0174, -0.0022, -0.0270, -0.0274, -0.0401,
         -0.0146,  0.0151,  0.0285,  0.0206, -0.0395, -0.0269, -0.0071,  0.0433,
          0.0065, -0.0116,  0.0185, -0.0170, -0.0289,  0.0326, -0.0203, -0.0090,
         -0.0445,  0.0299,  0.0339,  0.0163, -0.0312, -0.0441, -0.0363,  0.0333,
          0.0215,  0.0376,  0.0234,  0.0113, -0.0004, -0.0340, -0.0383, -0.0418,
          0.0183, -0.0220, -0.0090, -0.0257, -0.0081, -0.0315, -0.0292,  0.0148,
         -0.0133,  0.0276, -0.0143, -0.0328, -0.0079, -0.0217, -0.0137, -0.0426,
          0.0250, -0.0311,  0.0225,  0.0203,  0.0320, -0.0343,  0.0322, -0.0211,
          0.0166,  0.0420, -0.0063, -0.0003, -0.0103, -0.0373,  0.0215, -0.0444,
          0.0278,  0.0335,  0.0423, -0.0105, -0.0367,  0.0101,  0.0247, -0.0445,
         -0.0102, -0.0268, -0.0039, -0.0220, -0.0183, -0.0142, -0.0425,  0.0367,
          0.0375, -0.0070, -0.0051, -0.0183, -0.0404, -0.0435,  0.0166, -0.0246,
         -0.0288, -0.0035, -0.0149, -0.0145,  0.0014, -0.0095, -0.0154, -0.0214,
         -0.0364,  0.0375, -0.0179,  0.0119, -0.0155,  0.0036,  0.0417,  0.0206,
         -0.0388,  0.0178,  0.0425,  0.0118,  0.0300,  0.0441, -0.0069,  0.0093,
         -0.0311, -0.0092,  0.0331,  0.0229, -0.0283, -0.0359, -0.0306, -0.0441,
         -0.0345, -0.0111,  0.0302,  0.0075, -0.0340, -0.0359,  0.0222, -0.0333,
         -0.0055,  0.0215, -0.0207, -0.0049, -0.0039, -0.0106, -0.0227, -0.0399,
         -0.0362, -0.0239,  0.0432, -0.0216, -0.0300,  0.0108,  0.0123,  0.0245,
          0.0340,  0.0249, -0.0443,  0.0040,  0.0271, -0.0041, -0.0264,  0.0426,
         -0.0167, -0.0255, -0.0403,  0.0020,  0.0198,  0.0099,  0.0088, -0.0339,
         -0.0418,  0.0008,  0.0408,  0.0258, -0.0260, -0.0058, -0.0330, -0.0216,
          0.0081,  0.0244,  0.0370, -0.0411,  0.0299, -0.0315,  0.0167,  0.0378,
          0.0006,  0.0407, -0.0381, -0.0171,  0.0261, -0.0097, -0.0092, -0.0186,
          0.0308,  0.0219,  0.0143, -0.0251, -0.0363,  0.0048,  0.0132, -0.0206,
         -0.0125,  0.0302,  0.0036,  0.0020, -0.0110, -0.0405, -0.0420, -0.0214,
         -0.0227,  0.0139, -0.0130, -0.0175,  0.0426,  0.0156,  0.0319, -0.0217,
         -0.0183,  0.0164, -0.0298, -0.0292, -0.0022, -0.0164, -0.0335,  0.0265,
          0.0360,  0.0073, -0.0078, -0.0414, -0.0163,  0.0114,  0.0211, -0.0057,
         -0.0177,  0.0249, -0.0356,  0.0283, -0.0173,  0.0007, -0.0088,  0.0054,
         -0.0135,  0.0325, -0.0012,  0.0349,  0.0430, -0.0218, -0.0326,  0.0359,
          0.0350, -0.0341, -0.0035, -0.0441, -0.0366,  0.0086,  0.0119,  0.0095,
         -0.0122,  0.0413,  0.0064, -0.0264, -0.0025,  0.0107,  0.0157, -0.0316,
          0.0168, -0.0228, -0.0372, -0.0244,  0.0431,  0.0382,  0.0400,  0.0263,
          0.0338, -0.0060, -0.0246,  0.0223, -0.0232, -0.0302, -0.0143,  0.0094,
          0.0230, -0.0174, -0.0263,  0.0060, -0.0264, -0.0291,  0.0233, -0.0075,
          0.0409,  0.0435,  0.0134,  0.0154,  0.0103,  0.0007, -0.0033,  0.0006,
          0.0167,  0.0416, -0.0116, -0.0189]])
linear.bias :  tensor([-0.0108])
</pre></div>
</div>
</div>
</div>
<p>Now, we can train the model.</p>
<p>We iterate over the data by following these two steps in each epoch:</p>
<ol class="arabic simple">
<li><p>randomly permute all indices up to the number of training data points at each epoch.</p></li>
<li><p>iterate over all batches by picking the samples corresponding to the current subset of indices</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">random_nums</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Iterate over the individual batches</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">random_nums</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">random_nums</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]]</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">hinge_loss</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">output</span><span class="p">)</span> <span class="o">+</span> \
            <span class="n">reg_lambda</span> <span class="o">*</span> <span class="n">sq_l2_reg</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch </span><span class="si">{}</span><span class="s1">, loss </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch 0, loss 0.05415971204638481
epoch 1, loss 0.08875705301761627
epoch 2, loss 0.1126236617565155
epoch 3, loss 0.13219766318798065
epoch 4, loss 0.14138911664485931
epoch 5, loss 0.1433056890964508
epoch 6, loss 0.14052040874958038
epoch 7, loss 0.1448531150817871
epoch 8, loss 0.12890547513961792
epoch 9, loss 0.12236513197422028
epoch 10, loss 0.11469624191522598
epoch 11, loss 0.106291264295578
epoch 12, loss 0.09812000393867493
epoch 13, loss 0.09035123884677887
epoch 14, loss 0.0829516127705574
epoch 15, loss 0.07602420449256897
epoch 16, loss 0.07078631967306137
epoch 17, loss 0.06397834420204163
epoch 18, loss 0.058583278208971024
epoch 19, loss 0.052927661687135696
epoch 20, loss 0.04738983139395714
epoch 21, loss 0.042254768311977386
epoch 22, loss 0.03802109509706497
epoch 23, loss 0.034848302602767944
epoch 24, loss 0.03150230273604393
epoch 25, loss 0.02812996506690979
epoch 26, loss 0.02496222034096718
epoch 27, loss 0.022303862497210503
epoch 28, loss 0.020066168159246445
epoch 29, loss 0.017848048359155655
epoch 30, loss 0.01584838330745697
epoch 31, loss 0.014373302459716797
epoch 32, loss 0.012867775745689869
epoch 33, loss 0.011364003643393517
epoch 34, loss 0.009949270635843277
epoch 35, loss 0.008899270556867123
epoch 36, loss 0.008043870329856873
epoch 37, loss 0.0072169071063399315
epoch 38, loss 0.0063956985250115395
epoch 39, loss 0.005880264099687338
epoch 40, loss 0.005400832276791334
epoch 41, loss 0.004710076842457056
epoch 42, loss 0.004057060461491346
epoch 43, loss 0.003996427170932293
epoch 44, loss 0.004805156961083412
epoch 45, loss 0.013885999098420143
epoch 46, loss 0.00432458845898509
epoch 47, loss 0.004056410398334265
epoch 48, loss 0.004148656036704779
epoch 49, loss 0.013044895604252815
epoch 50, loss 0.011855831369757652
epoch 51, loss 0.004851495381444693
epoch 52, loss 0.0053099109791219234
epoch 53, loss 0.004917889833450317
epoch 54, loss 0.0043554482981562614
epoch 55, loss 0.003712121397256851
epoch 56, loss 0.003309423103928566
epoch 57, loss 0.002882568631321192
epoch 58, loss 0.008506173267960548
epoch 59, loss 0.005900952965021133
epoch 60, loss 0.007477398030459881
epoch 61, loss 0.004249017219990492
epoch 62, loss 0.005088319070637226
epoch 63, loss 0.005071698222309351
epoch 64, loss 0.004510215483605862
epoch 65, loss 0.003907344304025173
epoch 66, loss 0.00342568545602262
epoch 67, loss 0.004069694317877293
epoch 68, loss 0.003282055491581559
epoch 69, loss 0.010641190223395824
epoch 70, loss 0.01600613072514534
epoch 71, loss 0.0032594420481473207
epoch 72, loss 0.017606129869818687
epoch 73, loss 0.025161584839224815
epoch 74, loss 0.006000804249197245
epoch 75, loss 0.006126889493316412
epoch 76, loss 0.006311774719506502
epoch 77, loss 0.005662098061293364
epoch 78, loss 0.0048826225101947784
epoch 79, loss 0.00418464420363307
epoch 80, loss 0.003608378116041422
epoch 81, loss 0.011905599385499954
epoch 82, loss 0.00473553454503417
epoch 83, loss 0.004329765681177378
epoch 84, loss 0.003587295999750495
epoch 85, loss 0.01240506675094366
epoch 86, loss 0.0025662784464657307
epoch 87, loss 0.003784271888434887
epoch 88, loss 0.0032284725457429886
epoch 89, loss 0.006317158229649067
epoch 90, loss 0.003414803883060813
epoch 91, loss 0.00331181101500988
epoch 92, loss 0.0037104329094290733
epoch 93, loss 0.005531912203878164
epoch 94, loss 0.01368728093802929
epoch 95, loss 0.005813860334455967
epoch 96, loss 0.006151061505079269
epoch 97, loss 0.008078338578343391
epoch 98, loss 0.008208931423723698
epoch 99, loss 0.007173060905188322
epoch 100, loss 0.00561945466324687
epoch 101, loss 0.00937388464808464
epoch 102, loss 0.0032532145269215107
epoch 103, loss 0.0049604917876422405
epoch 104, loss 0.006896927021443844
epoch 105, loss 0.0052629150450229645
epoch 106, loss 0.007397735491394997
epoch 107, loss 0.005149292293936014
epoch 108, loss 0.00612332159653306
epoch 109, loss 0.005896161310374737
epoch 110, loss 0.005114298779517412
epoch 111, loss 0.004301111213862896
epoch 112, loss 0.003764063585549593
epoch 113, loss 0.006216623820364475
epoch 114, loss 0.003225889289751649
epoch 115, loss 0.004104942549020052
epoch 116, loss 0.00991000421345234
epoch 117, loss 0.0061779017560184
epoch 118, loss 0.0056005516089499
epoch 119, loss 0.0047055864706635475
epoch 120, loss 0.003976325038820505
epoch 121, loss 0.004297405481338501
epoch 122, loss 0.0025613459292799234
epoch 123, loss 0.002466461854055524
epoch 124, loss 0.0024955079425126314
epoch 125, loss 0.0026622649747878313
epoch 126, loss 0.003137133317068219
epoch 127, loss 0.004980923142284155
epoch 128, loss 0.005081053823232651
epoch 129, loss 0.005332843866199255
epoch 130, loss 0.004696915857493877
epoch 131, loss 0.0036783197429031134
epoch 132, loss 0.002669460140168667
epoch 133, loss 0.004531405866146088
epoch 134, loss 0.006377999670803547
epoch 135, loss 0.0033579207956790924
epoch 136, loss 0.00451698387041688
epoch 137, loss 0.0058397529646754265
epoch 138, loss 0.022355569526553154
epoch 139, loss 0.008871170692145824
epoch 140, loss 0.011588789522647858
epoch 141, loss 0.007792602758854628
epoch 142, loss 0.011243870481848717
epoch 143, loss 0.007032301276922226
epoch 144, loss 0.006971883121877909
epoch 145, loss 0.00573373818770051
epoch 146, loss 0.004517002496868372
epoch 147, loss 0.0037776611279696226
epoch 148, loss 0.003175431862473488
epoch 149, loss 0.0024718979839235544
epoch 150, loss 0.016900233924388885
epoch 151, loss 0.019907420501112938
epoch 152, loss 0.00695432722568512
epoch 153, loss 0.0188466664403677
epoch 154, loss 0.02594698779284954
epoch 155, loss 0.007484631147235632
epoch 156, loss 0.006976043339818716
epoch 157, loss 0.03697647899389267
epoch 158, loss 0.005826330743730068
epoch 159, loss 0.004735896829515696
epoch 160, loss 0.009253201074898243
epoch 161, loss 0.004606031812727451
epoch 162, loss 0.01492708083242178
epoch 163, loss 0.0038773149717599154
epoch 164, loss 0.013574575074017048
epoch 165, loss 0.0033574129920452833
epoch 166, loss 0.006845803000032902
epoch 167, loss 0.0053221858106553555
epoch 168, loss 0.005877820774912834
epoch 169, loss 0.0089229391887784
epoch 170, loss 0.005957988556474447
epoch 171, loss 0.005800919607281685
epoch 172, loss 0.004848082084208727
epoch 173, loss 0.020616278052330017
epoch 174, loss 0.004416092298924923
epoch 175, loss 0.004749130457639694
epoch 176, loss 0.004407023545354605
epoch 177, loss 0.00378055009059608
epoch 178, loss 0.003011750988662243
epoch 179, loss 0.0021394011564552784
epoch 180, loss 0.024940621107816696
epoch 181, loss 0.0038922596722841263
epoch 182, loss 0.00594549672678113
epoch 183, loss 0.020991526544094086
epoch 184, loss 0.02204880863428116
epoch 185, loss 0.014903571456670761
epoch 186, loss 0.030539056286215782
epoch 187, loss 0.015723541378974915
epoch 188, loss 0.013078338466584682
epoch 189, loss 0.010941260494291782
epoch 190, loss 0.009803036227822304
epoch 191, loss 0.008592799305915833
epoch 192, loss 0.015661418437957764
epoch 193, loss 0.0064293378964066505
epoch 194, loss 0.005487862974405289
epoch 195, loss 0.027330059558153152
epoch 196, loss 0.02143961563706398
epoch 197, loss 0.010604110546410084
epoch 198, loss 0.07423557341098785
epoch 199, loss 0.010484580881893635
epoch 200, loss 0.012499330565333366
epoch 201, loss 0.013805459253489971
epoch 202, loss 0.012651843950152397
epoch 203, loss 0.009917331859469414
epoch 204, loss 0.00756493303924799
epoch 205, loss 0.0061864363960921764
epoch 206, loss 0.005440902430564165
epoch 207, loss 0.004369636066257954
epoch 208, loss 0.00344272144138813
epoch 209, loss 0.005343682132661343
epoch 210, loss 0.0035651116631925106
epoch 211, loss 0.010621583089232445
epoch 212, loss 0.012989994138479233
epoch 213, loss 0.0017937791999429464
epoch 214, loss 0.0018070624209940434
epoch 215, loss 0.0026890586595982313
epoch 216, loss 0.0038579516112804413
epoch 217, loss 0.020123112946748734
epoch 218, loss 0.0040177879855036736
epoch 219, loss 0.00330486916936934
epoch 220, loss 0.005367794074118137
epoch 221, loss 0.023846793919801712
epoch 222, loss 0.0042882454581558704
epoch 223, loss 0.004916752222925425
epoch 224, loss 0.012850597500801086
epoch 225, loss 0.004462102428078651
epoch 226, loss 0.004093927796930075
epoch 227, loss 0.003874824848026037
epoch 228, loss 0.006610059179365635
epoch 229, loss 0.003912767861038446
epoch 230, loss 0.0053192139603197575
epoch 231, loss 0.007368464022874832
epoch 232, loss 0.0074350591748952866
epoch 233, loss 0.011795477010309696
epoch 234, loss 0.00836212933063507
epoch 235, loss 0.008261000737547874
epoch 236, loss 0.0076533216051757336
epoch 237, loss 0.006299566477537155
epoch 238, loss 0.014610830694437027
epoch 239, loss 0.019269835203886032
epoch 240, loss 0.004386672284454107
epoch 241, loss 0.0052340044640004635
epoch 242, loss 0.006514270324259996
epoch 243, loss 0.00748200761154294
epoch 244, loss 0.0068216328509151936
epoch 245, loss 0.0066338470205664635
epoch 246, loss 0.008637834340333939
epoch 247, loss 0.005251921713352203
epoch 248, loss 0.016850033774971962
epoch 249, loss 0.0046746921725571156
epoch 250, loss 0.031959984451532364
epoch 251, loss 0.06566871702671051
epoch 252, loss 0.019664442166686058
epoch 253, loss 0.019518211483955383
epoch 254, loss 0.016177531331777573
epoch 255, loss 0.01313022244721651
epoch 256, loss 0.010745222680270672
epoch 257, loss 0.00762099027633667
epoch 258, loss 0.005466696806252003
epoch 259, loss 0.004974849056452513
epoch 260, loss 0.004499075468629599
epoch 261, loss 0.03673580288887024
epoch 262, loss 0.05703691393136978
epoch 263, loss 0.019375905394554138
epoch 264, loss 0.025183947756886482
epoch 265, loss 0.028498856350779533
epoch 266, loss 0.02577020786702633
epoch 267, loss 0.019715411588549614
epoch 268, loss 0.0139712393283844
epoch 269, loss 0.009598005563020706
epoch 270, loss 0.007214172277599573
epoch 271, loss 0.005485358648002148
epoch 272, loss 0.004702379927039146
epoch 273, loss 0.004852518904954195
epoch 274, loss 0.030736638233065605
epoch 275, loss 0.005574408452957869
epoch 276, loss 0.00647409912198782
epoch 277, loss 0.0061776721850037575
epoch 278, loss 0.005932479631155729
epoch 279, loss 0.013546410016715527
epoch 280, loss 0.005884531419724226
epoch 281, loss 0.020281143486499786
epoch 282, loss 0.03533952683210373
epoch 283, loss 0.042395126074552536
epoch 284, loss 0.015671314671635628
epoch 285, loss 0.04478507861495018
epoch 286, loss 0.01497228629887104
epoch 287, loss 0.028339650481939316
epoch 288, loss 0.03526763990521431
epoch 289, loss 0.012707943096756935
epoch 290, loss 0.010042952373623848
epoch 291, loss 0.009998263791203499
epoch 292, loss 0.00890736747533083
epoch 293, loss 0.010326412506401539
epoch 294, loss 0.015256933867931366
epoch 295, loss 0.01648404635488987
epoch 296, loss 0.017790909856557846
epoch 297, loss 0.010292204096913338
epoch 298, loss 0.007859306409955025
epoch 299, loss 0.005711264908313751
epoch 300, loss 0.005097352433949709
epoch 301, loss 0.005353521090000868
epoch 302, loss 0.006119459867477417
epoch 303, loss 0.013285793364048004
epoch 304, loss 0.006344382185488939
epoch 305, loss 0.005926938261836767
epoch 306, loss 0.005616486072540283
epoch 307, loss 0.004735135473310947
epoch 308, loss 0.0043733688071370125
epoch 309, loss 0.005349306855350733
epoch 310, loss 0.0037055579014122486
epoch 311, loss 0.004258520435541868
epoch 312, loss 0.01378003228455782
epoch 313, loss 0.016204608604311943
epoch 314, loss 0.00517955282703042
epoch 315, loss 0.015224460512399673
epoch 316, loss 0.0045760395005345345
epoch 317, loss 0.004303738009184599
epoch 318, loss 0.018805650994181633
epoch 319, loss 0.004878450650721788
epoch 320, loss 0.005379114765673876
epoch 321, loss 0.005247726570814848
epoch 322, loss 0.006013454869389534
epoch 323, loss 0.007728966884315014
epoch 324, loss 0.008648413233458996
epoch 325, loss 0.008057630620896816
epoch 326, loss 0.0066008116118609905
epoch 327, loss 0.0074350484646856785
epoch 328, loss 0.005269917193800211
epoch 329, loss 0.013302911072969437
epoch 330, loss 0.0040214089676737785
epoch 331, loss 0.003640280570834875
epoch 332, loss 0.011867107823491096
epoch 333, loss 0.005665271542966366
epoch 334, loss 0.018378544598817825
epoch 335, loss 0.03675803542137146
epoch 336, loss 0.014370149932801723
epoch 337, loss 0.011651748791337013
epoch 338, loss 0.008175668306648731
epoch 339, loss 0.007918511517345905
epoch 340, loss 0.003870527260005474
epoch 341, loss 0.004393043927848339
epoch 342, loss 0.006112305913120508
epoch 343, loss 0.0071122907102108
epoch 344, loss 0.00632244860753417
epoch 345, loss 0.00899002980440855
epoch 346, loss 0.01808249205350876
epoch 347, loss 0.006502140779048204
epoch 348, loss 0.007572186179459095
epoch 349, loss 0.009206761606037617
epoch 350, loss 0.00809461809694767
epoch 351, loss 0.021346110850572586
epoch 352, loss 0.008599111810326576
epoch 353, loss 0.005877796560525894
epoch 354, loss 0.003710032906383276
epoch 355, loss 0.007414966821670532
epoch 356, loss 0.0026837128680199385
epoch 357, loss 0.006490842439234257
epoch 358, loss 0.004463088233023882
epoch 359, loss 0.00702831894159317
epoch 360, loss 0.009233908727765083
epoch 361, loss 0.03602401167154312
epoch 362, loss 0.007290523964911699
epoch 363, loss 0.006817083805799484
epoch 364, loss 0.007201642729341984
epoch 365, loss 0.006116476375609636
epoch 366, loss 0.006371400319039822
epoch 367, loss 0.010623310692608356
epoch 368, loss 0.020257096737623215
epoch 369, loss 0.026632867753505707
epoch 370, loss 0.026461035013198853
epoch 371, loss 0.020755887031555176
epoch 372, loss 0.013447273522615433
epoch 373, loss 0.008195647038519382
epoch 374, loss 0.0322684571146965
epoch 375, loss 0.11181202530860901
epoch 376, loss 0.04048766940832138
epoch 377, loss 0.013175664469599724
epoch 378, loss 0.0764947384595871
epoch 379, loss 0.015530582517385483
epoch 380, loss 0.01322854682803154
epoch 381, loss 0.012092593125998974
epoch 382, loss 0.027623958885669708
epoch 383, loss 0.020033005625009537
epoch 384, loss 0.02209165133535862
epoch 385, loss 0.02157418243587017
epoch 386, loss 0.01736043021082878
epoch 387, loss 0.011425057426095009
epoch 388, loss 0.021948857232928276
epoch 389, loss 0.006873559672385454
epoch 390, loss 0.005920307710766792
epoch 391, loss 0.0052154515869915485
epoch 392, loss 0.005563836079090834
epoch 393, loss 0.005379042588174343
epoch 394, loss 0.006763825658708811
epoch 395, loss 0.019605521112680435
epoch 396, loss 0.009276164695620537
epoch 397, loss 0.004953228402882814
epoch 398, loss 0.015161196701228619
epoch 399, loss 0.030796196311712265
epoch 400, loss 0.01263749785721302
epoch 401, loss 0.042872704565525055
epoch 402, loss 0.02564178593456745
epoch 403, loss 0.022402232512831688
epoch 404, loss 0.01644125021994114
epoch 405, loss 0.03800664842128754
epoch 406, loss 0.009189814329147339
epoch 407, loss 0.01412130892276764
epoch 408, loss 0.006560218520462513
epoch 409, loss 0.005037759896367788
epoch 410, loss 0.0040908316150307655
epoch 411, loss 0.08531695604324341
epoch 412, loss 0.020795810967683792
epoch 413, loss 0.007031773217022419
epoch 414, loss 0.008523572236299515
epoch 415, loss 0.009712881408631802
epoch 416, loss 0.008819016627967358
epoch 417, loss 0.006795593537390232
epoch 418, loss 0.005035454872995615
epoch 419, loss 0.031018530949950218
epoch 420, loss 0.005228052847087383
epoch 421, loss 0.00798235647380352
epoch 422, loss 0.01152915321290493
epoch 423, loss 0.011704321019351482
epoch 424, loss 0.010531838983297348
epoch 425, loss 0.008795537985861301
epoch 426, loss 0.00629134988412261
epoch 427, loss 0.0045960890129208565
epoch 428, loss 0.016991019248962402
epoch 429, loss 0.00592399388551712
epoch 430, loss 0.021516859531402588
epoch 431, loss 0.010312345810234547
epoch 432, loss 0.010331726633012295
epoch 433, loss 0.009181329980492592
epoch 434, loss 0.006806895602494478
epoch 435, loss 0.0044844914227724075
epoch 436, loss 0.006018470041453838
epoch 437, loss 0.0028842089232057333
epoch 438, loss 0.016033560037612915
epoch 439, loss 0.023250015452504158
epoch 440, loss 0.013835811987519264
epoch 441, loss 0.01217798050493002
epoch 442, loss 0.009323671460151672
epoch 443, loss 0.006147615611553192
epoch 444, loss 0.006945853121578693
epoch 445, loss 0.008678480982780457
epoch 446, loss 0.010503510944545269
epoch 447, loss 0.024862054735422134
epoch 448, loss 0.032646454870700836
epoch 449, loss 0.02992093190550804
epoch 450, loss 0.021548930555582047
epoch 451, loss 0.012801186181604862
epoch 452, loss 0.00640531163662672
epoch 453, loss 0.009273332543671131
epoch 454, loss 0.004061049781739712
epoch 455, loss 0.024976685643196106
epoch 456, loss 0.014626028947532177
epoch 457, loss 0.023144304752349854
epoch 458, loss 0.028669003397226334
epoch 459, loss 0.025266868993639946
epoch 460, loss 0.018535427749156952
epoch 461, loss 0.027833793312311172
epoch 462, loss 0.010312588885426521
epoch 463, loss 0.00940840132534504
epoch 464, loss 0.018184978514909744
epoch 465, loss 0.018331702798604965
epoch 466, loss 0.03574468940496445
epoch 467, loss 0.009912907145917416
epoch 468, loss 0.009244970977306366
epoch 469, loss 0.009211717173457146
epoch 470, loss 0.007769790478050709
epoch 471, loss 0.005448370240628719
epoch 472, loss 0.0037485382054001093
epoch 473, loss 0.005289551801979542
epoch 474, loss 0.011709270998835564
epoch 475, loss 0.009747467003762722
epoch 476, loss 0.02282632701098919
epoch 477, loss 0.019991276785731316
epoch 478, loss 0.010812602005898952
epoch 479, loss 0.010113672353327274
epoch 480, loss 0.004824785981327295
epoch 481, loss 0.049110397696495056
epoch 482, loss 0.01747506856918335
epoch 483, loss 0.023480471223592758
epoch 484, loss 0.03142399340867996
epoch 485, loss 0.03349684551358223
epoch 486, loss 0.026970263570547104
epoch 487, loss 0.018092045560479164
epoch 488, loss 0.02809753641486168
epoch 489, loss 0.010636191815137863
epoch 490, loss 0.02836153656244278
epoch 491, loss 0.018214086070656776
epoch 492, loss 0.02131662890315056
epoch 493, loss 0.022632358595728874
epoch 494, loss 0.013078291900455952
epoch 495, loss 0.01124949287623167
epoch 496, loss 0.009406124241650105
epoch 497, loss 0.009188555181026459
epoch 498, loss 0.008176974020898342
epoch 499, loss 0.007277435157448053
epoch 500, loss 0.008309261873364449
epoch 501, loss 0.009537190198898315
epoch 502, loss 0.010283573530614376
epoch 503, loss 0.011027362197637558
epoch 504, loss 0.009545057080686092
epoch 505, loss 0.007849412970244884
epoch 506, loss 0.00786284077912569
epoch 507, loss 0.03972645476460457
epoch 508, loss 0.018039993941783905
epoch 509, loss 0.044714756309986115
epoch 510, loss 0.011739556677639484
epoch 511, loss 0.01565725728869438
epoch 512, loss 0.014895929023623466
epoch 513, loss 0.029475640505552292
epoch 514, loss 0.024059880524873734
epoch 515, loss 0.01128652784973383
epoch 516, loss 0.01154625415802002
epoch 517, loss 0.014072222635149956
epoch 518, loss 0.007232846226543188
epoch 519, loss 0.0052558560855686665
epoch 520, loss 0.042029641568660736
epoch 521, loss 0.01643320918083191
epoch 522, loss 0.07533290982246399
epoch 523, loss 0.011748991906642914
epoch 524, loss 0.015076090581715107
epoch 525, loss 0.021032191812992096
epoch 526, loss 0.019885268062353134
epoch 527, loss 0.018719589337706566
epoch 528, loss 0.017142167314887047
epoch 529, loss 0.014233206398785114
epoch 530, loss 0.01074108388274908
epoch 531, loss 0.009322445839643478
epoch 532, loss 0.09032591432332993
epoch 533, loss 0.015197611413896084
epoch 534, loss 0.02789340727031231
epoch 535, loss 0.017235714942216873
epoch 536, loss 0.012982374988496304
epoch 537, loss 0.009037313051521778
epoch 538, loss 0.007522535976022482
epoch 539, loss 0.006285575684159994
epoch 540, loss 0.006995645817369223
epoch 541, loss 0.013196672312915325
epoch 542, loss 0.02741047739982605
epoch 543, loss 0.04185457527637482
epoch 544, loss 0.04726846516132355
epoch 545, loss 0.02872670814394951
epoch 546, loss 0.01803220808506012
epoch 547, loss 0.027774052694439888
epoch 548, loss 0.006755888927727938
epoch 549, loss 0.005869106389582157
epoch 550, loss 0.006336907856166363
epoch 551, loss 0.006653632968664169
epoch 552, loss 0.005660803057253361
epoch 553, loss 0.015895245596766472
epoch 554, loss 0.006576801184564829
epoch 555, loss 0.05033519119024277
epoch 556, loss 0.011714024469256401
epoch 557, loss 0.010771332308650017
epoch 558, loss 0.01109823863953352
epoch 559, loss 0.008527797646820545
epoch 560, loss 0.008580239489674568
epoch 561, loss 0.013440385460853577
epoch 562, loss 0.05076991766691208
epoch 563, loss 0.008607294410467148
epoch 564, loss 0.009576387703418732
epoch 565, loss 0.009604074992239475
epoch 566, loss 0.007696349639445543
epoch 567, loss 0.011963113211095333
epoch 568, loss 0.01143381092697382
epoch 569, loss 0.018808050081133842
epoch 570, loss 0.012274281121790409
epoch 571, loss 0.00794273056089878
epoch 572, loss 0.012483088299632072
epoch 573, loss 0.05450000613927841
epoch 574, loss 0.12451989203691483
epoch 575, loss 0.026851749047636986
epoch 576, loss 0.0964832752943039
epoch 577, loss 0.0306759811937809
epoch 578, loss 0.022555558010935783
epoch 579, loss 0.01688174344599247
epoch 580, loss 0.05878019332885742
epoch 581, loss 0.012888684868812561
epoch 582, loss 0.02063829079270363
epoch 583, loss 0.0076300897635519505
epoch 584, loss 0.005571968853473663
epoch 585, loss 0.014531487599015236
epoch 586, loss 0.037899263203144073
epoch 587, loss 0.028108254075050354
epoch 588, loss 0.015758192166686058
epoch 589, loss 0.015805983915925026
epoch 590, loss 0.012413986958563328
epoch 591, loss 0.00872410461306572
epoch 592, loss 0.0064809443429112434
epoch 593, loss 0.005348061211407185
epoch 594, loss 0.053092263638973236
epoch 595, loss 0.011150450445711613
epoch 596, loss 0.078421950340271
epoch 597, loss 0.032102320343256
epoch 598, loss 0.027675412595272064
epoch 599, loss 0.0185546875
epoch 600, loss 0.012098333798348904
epoch 601, loss 0.0068613579496741295
epoch 602, loss 0.007170234341174364
epoch 603, loss 0.011386990547180176
epoch 604, loss 0.02269057184457779
epoch 605, loss 0.027325397357344627
epoch 606, loss 0.022759005427360535
epoch 607, loss 0.018875937908887863
epoch 608, loss 0.018587622791528702
epoch 609, loss 0.011045030318200588
epoch 610, loss 0.009859083220362663
epoch 611, loss 0.007706989999860525
epoch 612, loss 0.012045018374919891
epoch 613, loss 0.024141496047377586
epoch 614, loss 0.0040390342473983765
epoch 615, loss 0.006405757740139961
epoch 616, loss 0.037847016006708145
epoch 617, loss 0.04935840144753456
epoch 618, loss 0.0253400057554245
epoch 619, loss 0.023907151073217392
epoch 620, loss 0.051121510565280914
epoch 621, loss 0.01923629827797413
epoch 622, loss 0.01716466061770916
epoch 623, loss 0.03898605704307556
epoch 624, loss 0.012439507059752941
epoch 625, loss 0.010575437918305397
epoch 626, loss 0.0259799063205719
epoch 627, loss 0.04913295805454254
epoch 628, loss 0.06892511993646622
epoch 629, loss 0.11727511882781982
epoch 630, loss 0.031711190938949585
epoch 631, loss 0.02944941073656082
epoch 632, loss 0.020373428240418434
epoch 633, loss 0.015326710417866707
epoch 634, loss 0.11684740334749222
epoch 635, loss 0.09319542348384857
epoch 636, loss 0.024971654638648033
epoch 637, loss 0.026865776628255844
epoch 638, loss 0.039213988929986954
epoch 639, loss 0.06997096538543701
epoch 640, loss 0.09328126907348633
epoch 641, loss 0.06237134337425232
epoch 642, loss 0.04149165377020836
epoch 643, loss 0.023836098611354828
epoch 644, loss 0.013455403037369251
epoch 645, loss 0.008120903745293617
epoch 646, loss 0.01450873538851738
epoch 647, loss 0.013168100267648697
epoch 648, loss 0.021180396899580956
epoch 649, loss 0.03234533220529556
epoch 650, loss 0.03823735564947128
epoch 651, loss 0.020366927608847618
epoch 652, loss 0.015990741550922394
epoch 653, loss 0.050561610609292984
epoch 654, loss 0.04092543199658394
epoch 655, loss 0.08537607640028
epoch 656, loss 0.03614833950996399
epoch 657, loss 0.03186114877462387
epoch 658, loss 0.03257471323013306
epoch 659, loss 0.02424544282257557
epoch 660, loss 0.04507695510983467
epoch 661, loss 0.05109775811433792
epoch 662, loss 0.0463513545691967
epoch 663, loss 0.11990156769752502
epoch 664, loss 0.02193220518529415
epoch 665, loss 0.01656980626285076
epoch 666, loss 0.010481598787009716
epoch 667, loss 0.007417099084705114
epoch 668, loss 0.005720009095966816
epoch 669, loss 0.0050032171420753
epoch 670, loss 0.005348891951143742
epoch 671, loss 0.015390308573842049
epoch 672, loss 0.006867348216474056
epoch 673, loss 0.0074475775472819805
epoch 674, loss 0.006704410072416067
epoch 675, loss 0.006964006461203098
epoch 676, loss 0.008479285053908825
epoch 677, loss 0.008345352485775948
epoch 678, loss 0.01938161626458168
epoch 679, loss 0.05792156979441643
epoch 680, loss 0.017798159271478653
epoch 681, loss 0.021097883582115173
epoch 682, loss 0.019326044246554375
epoch 683, loss 0.015798134729266167
epoch 684, loss 0.016053490340709686
epoch 685, loss 0.0184806976467371
epoch 686, loss 0.01668686978518963
epoch 687, loss 0.016501398757100105
epoch 688, loss 0.00902624987065792
epoch 689, loss 0.00868615135550499
epoch 690, loss 0.0072876447811722755
epoch 691, loss 0.018303755670785904
epoch 692, loss 0.009719309397041798
epoch 693, loss 0.010968669317662716
epoch 694, loss 0.009729534387588501
epoch 695, loss 0.009625100530683994
epoch 696, loss 0.024733133614063263
epoch 697, loss 0.04743211343884468
epoch 698, loss 0.011577862314879894
epoch 699, loss 0.08338403701782227
epoch 700, loss 0.11609020829200745
epoch 701, loss 0.024932213127613068
epoch 702, loss 0.02841085009276867
epoch 703, loss 0.026439519599080086
epoch 704, loss 0.022606613114476204
epoch 705, loss 0.017686663195490837
epoch 706, loss 0.014082704670727253
epoch 707, loss 0.0093351686373353
epoch 708, loss 0.022570939734578133
epoch 709, loss 0.03147512301802635
epoch 710, loss 0.038523294031620026
epoch 711, loss 0.02765813283622265
epoch 712, loss 0.063156358897686
epoch 713, loss 0.0185658298432827
epoch 714, loss 0.01814880222082138
epoch 715, loss 0.009537486359477043
epoch 716, loss 0.007341401651501656
epoch 717, loss 0.0836443230509758
epoch 718, loss 0.017676884308457375
epoch 719, loss 0.024914704263210297
epoch 720, loss 0.024520516395568848
epoch 721, loss 0.018320254981517792
epoch 722, loss 0.01289795059710741
epoch 723, loss 0.008763225749135017
epoch 724, loss 0.005369337275624275
epoch 725, loss 0.055689286440610886
epoch 726, loss 0.006803527474403381
epoch 727, loss 0.011084925383329391
epoch 728, loss 0.014672650955617428
epoch 729, loss 0.01767793670296669
epoch 730, loss 0.014800602570176125
epoch 731, loss 0.009302495047450066
epoch 732, loss 0.005735816899687052
epoch 733, loss 0.009932371787726879
epoch 734, loss 0.011297538876533508
epoch 735, loss 0.012612036429345608
epoch 736, loss 0.009830538183450699
epoch 737, loss 0.008160991594195366
epoch 738, loss 0.00452406844124198
epoch 739, loss 0.01618988998234272
epoch 740, loss 0.009816497564315796
epoch 741, loss 0.02133903279900551
epoch 742, loss 0.03213471546769142
epoch 743, loss 0.030082907527685165
epoch 744, loss 0.02535073272883892
epoch 745, loss 0.020006848499178886
epoch 746, loss 0.016886109486222267
epoch 747, loss 0.030224425718188286
epoch 748, loss 0.014949003234505653
epoch 749, loss 0.014594513922929764
epoch 750, loss 0.011212527751922607
epoch 751, loss 0.007213269360363483
epoch 752, loss 0.008427247405052185
epoch 753, loss 0.0041140965186059475
epoch 754, loss 0.03182259202003479
epoch 755, loss 0.017069416120648384
epoch 756, loss 0.0533510223031044
epoch 757, loss 0.04958805441856384
epoch 758, loss 0.04587932303547859
epoch 759, loss 0.03190859779715538
epoch 760, loss 0.02220516838133335
epoch 761, loss 0.009089550003409386
epoch 762, loss 0.017037635669112206
epoch 763, loss 0.0030483955051749945
epoch 764, loss 0.0067750015296041965
epoch 765, loss 0.045694947242736816
epoch 766, loss 0.043025922030210495
epoch 767, loss 0.09040586650371552
epoch 768, loss 0.0402076430618763
epoch 769, loss 0.044797442853450775
epoch 770, loss 0.040140531957149506
epoch 771, loss 0.027546046301722527
epoch 772, loss 0.014579769223928452
epoch 773, loss 0.023520493879914284
epoch 774, loss 0.18853285908699036
epoch 775, loss 0.21707122027873993
epoch 776, loss 0.20928999781608582
epoch 777, loss 0.1561470627784729
epoch 778, loss 0.0863984003663063
epoch 779, loss 0.04080962389707565
epoch 780, loss 0.017801150679588318
epoch 781, loss 0.007876351475715637
epoch 782, loss 0.03730194643139839
epoch 783, loss 0.012794640846550465
epoch 784, loss 0.024375103414058685
epoch 785, loss 0.02875000610947609
epoch 786, loss 0.031203221529722214
epoch 787, loss 0.026450229808688164
epoch 788, loss 0.01745094731450081
epoch 789, loss 0.010323785245418549
epoch 790, loss 0.006718637887388468
epoch 791, loss 0.01546496246010065
epoch 792, loss 0.007986346259713173
epoch 793, loss 0.008970607072114944
epoch 794, loss 0.011263199150562286
epoch 795, loss 0.0123897735029459
epoch 796, loss 0.020010318607091904
epoch 797, loss 0.0077502052299678326
epoch 798, loss 0.004489860497415066
epoch 799, loss 0.0029272146057337523
epoch 800, loss 0.004601829219609499
epoch 801, loss 0.03611057251691818
epoch 802, loss 0.015652257949113846
epoch 803, loss 0.016159599646925926
epoch 804, loss 0.028207551687955856
epoch 805, loss 0.01745123602449894
epoch 806, loss 0.014529583044350147
epoch 807, loss 0.039809443056583405
epoch 808, loss 0.006920292973518372
epoch 809, loss 0.0052278717048466206
epoch 810, loss 0.16136512160301208
epoch 811, loss 0.030023977160453796
epoch 812, loss 0.09390269219875336
epoch 813, loss 0.036406271159648895
epoch 814, loss 0.02723952941596508
epoch 815, loss 0.019200701266527176
epoch 816, loss 0.014832459390163422
epoch 817, loss 0.012402206659317017
epoch 818, loss 0.010289872996509075
epoch 819, loss 0.023720603436231613
epoch 820, loss 0.006327605340629816
epoch 821, loss 0.005574789829552174
epoch 822, loss 0.00530204689130187
epoch 823, loss 0.005372676532715559
epoch 824, loss 0.035194527357816696
epoch 825, loss 0.016222788020968437
epoch 826, loss 0.02010958082973957
epoch 827, loss 0.01692996546626091
epoch 828, loss 0.010999036952853203
epoch 829, loss 0.024678602814674377
epoch 830, loss 0.0037669148296117783
epoch 831, loss 0.005069973412901163
epoch 832, loss 0.05619373172521591
epoch 833, loss 0.05468013137578964
epoch 834, loss 0.05833359807729721
epoch 835, loss 0.043821338564157486
epoch 836, loss 0.025808319449424744
epoch 837, loss 0.013242409564554691
epoch 838, loss 0.006684219930320978
epoch 839, loss 0.053318530321121216
epoch 840, loss 0.02586248703300953
epoch 841, loss 0.012865147553384304
epoch 842, loss 0.020061912015080452
epoch 843, loss 0.03852515667676926
epoch 844, loss 0.03373822942376137
epoch 845, loss 0.032835762947797775
epoch 846, loss 0.022021200507879257
epoch 847, loss 0.014886678196489811
epoch 848, loss 0.012575246393680573
epoch 849, loss 0.04075959324836731
epoch 850, loss 0.007426385302096605
epoch 851, loss 0.013365745544433594
epoch 852, loss 0.007521260529756546
epoch 853, loss 0.007598962634801865
epoch 854, loss 0.00858854316174984
epoch 855, loss 0.018714167177677155
epoch 856, loss 0.008503385819494724
epoch 857, loss 0.007699847687035799
epoch 858, loss 0.01386326178908348
epoch 859, loss 0.006839994806796312
epoch 860, loss 0.027126528322696686
epoch 861, loss 0.008199015632271767
epoch 862, loss 0.017708271741867065
epoch 863, loss 0.016559241339564323
epoch 864, loss 0.013525884598493576
epoch 865, loss 0.00850001536309719
epoch 866, loss 0.009567465633153915
epoch 867, loss 0.009697549045085907
epoch 868, loss 0.013454693369567394
epoch 869, loss 0.016722356900572777
epoch 870, loss 0.017618481069803238
epoch 871, loss 0.010447060689330101
epoch 872, loss 0.0060652559623122215
epoch 873, loss 0.007891067303717136
epoch 874, loss 0.01373247429728508
epoch 875, loss 0.04525572806596756
epoch 876, loss 0.014516589231789112
epoch 877, loss 0.025623250752687454
epoch 878, loss 0.05825524404644966
epoch 879, loss 0.012140989303588867
epoch 880, loss 0.012427102774381638
epoch 881, loss 0.011613339185714722
epoch 882, loss 0.00892416201531887
epoch 883, loss 0.006601626519113779
epoch 884, loss 0.005905956961214542
epoch 885, loss 0.004598256666213274
epoch 886, loss 0.0037785933818668127
epoch 887, loss 0.027995819225907326
epoch 888, loss 0.011152478866279125
epoch 889, loss 0.013728969730436802
epoch 890, loss 0.013777426443994045
epoch 891, loss 0.014692237600684166
epoch 892, loss 0.01586141623556614
epoch 893, loss 0.014920540153980255
epoch 894, loss 0.012322650291025639
epoch 895, loss 0.01394074410200119
epoch 896, loss 0.00922049768269062
epoch 897, loss 0.00855423603206873
epoch 898, loss 0.011014841496944427
epoch 899, loss 0.01407554280012846
epoch 900, loss 0.012121795676648617
epoch 901, loss 0.011064495891332626
epoch 902, loss 0.036779697984457016
epoch 903, loss 0.014339946210384369
epoch 904, loss 0.016508085653185844
epoch 905, loss 0.020179392769932747
epoch 906, loss 0.037015289068222046
epoch 907, loss 0.06726064532995224
epoch 908, loss 0.03398817777633667
epoch 909, loss 0.028866492211818695
epoch 910, loss 0.013327877968549728
epoch 911, loss 0.011021370068192482
epoch 912, loss 0.0052902367897331715
epoch 913, loss 0.0048446012660861015
epoch 914, loss 0.004716882016509771
epoch 915, loss 0.004803754854947329
epoch 916, loss 0.005288458429276943
epoch 917, loss 0.009407108649611473
epoch 918, loss 0.012822039425373077
epoch 919, loss 0.00783598329871893
epoch 920, loss 0.019308384507894516
epoch 921, loss 0.006695288233458996
epoch 922, loss 0.005416386760771275
epoch 923, loss 0.0035379710607230663
epoch 924, loss 0.019981445744633675
epoch 925, loss 0.0296565443277359
epoch 926, loss 0.014112867414951324
epoch 927, loss 0.006847029086202383
epoch 928, loss 0.0064747934229671955
epoch 929, loss 0.035707294940948486
epoch 930, loss 0.013030488044023514
epoch 931, loss 0.007089999504387379
epoch 932, loss 0.008883141912519932
epoch 933, loss 0.011613971553742886
epoch 934, loss 0.012109935283660889
epoch 935, loss 0.009981564246118069
epoch 936, loss 0.009744617156684399
epoch 937, loss 0.03764773905277252
epoch 938, loss 0.007836307398974895
epoch 939, loss 0.022625956684350967
epoch 940, loss 0.008859056979417801
epoch 941, loss 0.042847078293561935
epoch 942, loss 0.010432968847453594
epoch 943, loss 0.01396897528320551
epoch 944, loss 0.04350229352712631
epoch 945, loss 0.06111105531454086
epoch 946, loss 0.055348508059978485
epoch 947, loss 0.04515278711915016
epoch 948, loss 0.047250378876924515
epoch 949, loss 0.04688470438122749
epoch 950, loss 0.11360086500644684
epoch 951, loss 0.02864571288228035
epoch 952, loss 0.018526820465922356
epoch 953, loss 0.0240542721003294
epoch 954, loss 0.02154414914548397
epoch 955, loss 0.018863007426261902
epoch 956, loss 0.013064569793641567
epoch 957, loss 0.013863662257790565
epoch 958, loss 0.006182831712067127
epoch 959, loss 0.005040056072175503
epoch 960, loss 0.004458665382117033
epoch 961, loss 0.014792042784392834
epoch 962, loss 0.02565544657409191
epoch 963, loss 0.022732453420758247
epoch 964, loss 0.04701027274131775
epoch 965, loss 0.0386020690202713
epoch 966, loss 0.032519374042749405
epoch 967, loss 0.022742314264178276
epoch 968, loss 0.017459329217672348
epoch 969, loss 0.012392483651638031
epoch 970, loss 0.12477677315473557
epoch 971, loss 0.02107316628098488
epoch 972, loss 0.03564466908574104
epoch 973, loss 0.03552565351128578
epoch 974, loss 0.025822144001722336
epoch 975, loss 0.016662245616316795
epoch 976, loss 0.009378193877637386
epoch 977, loss 0.020119110122323036
epoch 978, loss 0.007425157353281975
epoch 979, loss 0.022096475586295128
epoch 980, loss 0.04672776162624359
epoch 981, loss 0.026322485879063606
epoch 982, loss 0.041237588971853256
epoch 983, loss 0.035394541919231415
epoch 984, loss 0.03469017148017883
epoch 985, loss 0.02828221395611763
epoch 986, loss 0.01866401545703411
epoch 987, loss 0.010034582577645779
epoch 988, loss 0.03568647801876068
epoch 989, loss 0.013034946285188198
epoch 990, loss 0.02696228213608265
epoch 991, loss 0.05254793167114258
epoch 992, loss 0.07959985733032227
epoch 993, loss 0.0408656969666481
epoch 994, loss 0.033036623150110245
epoch 995, loss 0.026012888178229332
epoch 996, loss 0.019107194617390633
epoch 997, loss 0.012969298288226128
epoch 998, loss 0.007987146265804768
epoch 999, loss 0.018369479104876518
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print final parameters</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;: &quot;</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>linear.weight :  tensor([[-4.3840e-02,  3.7154e-02, -1.8599e-02, -2.9914e-01,  4.8049e-02,
         -3.8624e-02, -5.9737e-02, -1.8046e-02,  4.8639e-02, -3.7677e-01,
         -3.0616e-02, -5.9867e-02, -1.8603e-02,  3.0316e-01, -7.6967e-02,
          4.9941e-02, -1.4472e-02,  1.8021e-01,  6.9080e-02,  5.5923e-03,
         -2.4834e-02,  4.9265e-02,  6.8726e-02, -3.2119e-02,  6.5568e-02,
          1.5987e-01, -5.7546e-02,  9.1915e-02, -4.9658e-02, -8.7284e-03,
          8.8835e-02, -2.8591e-02,  1.1857e-02, -2.7662e-02, -1.0790e-01,
         -5.4270e-02,  7.3896e-02,  1.1921e-02, -1.4514e-02,  1.0379e-01,
         -4.4070e-02, -4.4913e-02,  7.5571e-02, -1.0362e-01,  7.2744e-02,
         -3.9318e-02,  4.8648e-02,  8.9887e-02, -1.8805e-01, -1.3462e-03,
         -2.0104e-03, -3.6487e-01, -5.4353e-02,  3.8214e-02, -1.5799e-02,
         -4.9141e-02, -1.4759e-02, -5.6240e-02, -4.1308e-02, -1.2092e-01,
          5.7894e-02,  8.9047e-02, -3.6861e-02, -3.1655e-01,  5.3951e-02,
          7.2144e-02, -1.1520e-01,  1.1261e-01, -4.0637e-02,  1.4511e-01,
         -7.5381e-02, -5.1708e-02,  1.1300e-01,  1.6554e-01,  2.9488e-02,
         -9.2482e-02,  6.8774e-02, -2.9853e-01,  1.2908e-01, -2.5613e-02,
         -5.4678e-02,  7.7007e-03, -2.5630e-01, -1.4662e-01,  1.2987e-01,
         -1.7840e-01,  2.3337e-01,  2.7036e-02, -1.9394e-02,  5.3320e-02,
         -3.8555e-02,  1.6212e-02,  1.3796e-01, -3.4649e-01,  1.1526e-01,
          3.6072e-03, -2.9360e-01,  2.0812e-01, -4.3965e-02, -5.2448e-02,
          1.0303e-01,  7.5507e-02,  1.2622e-01,  8.2226e-02,  1.1012e-01,
         -5.4091e-02, -3.1692e-01, -3.6698e-02,  1.0695e-01, -2.2638e-01,
          9.4052e-02,  1.7713e-01, -2.4419e-01, -3.1327e-01,  8.9345e-02,
          4.4047e-02,  2.8319e-02,  8.7749e-02,  6.7385e-03, -1.1540e-01,
          1.3743e-01, -1.1855e-01, -1.3031e-02, -1.5690e-02,  6.5629e-03,
         -5.3935e-02,  1.0400e-01,  1.3174e-01, -2.1443e-02,  1.9041e-02,
          4.7853e-02,  2.8003e-02, -3.7880e-01, -5.9673e-02, -1.6686e-01,
         -1.6510e-01,  7.1235e-02,  1.3888e-02, -1.3166e-02, -2.3137e-02,
         -1.2203e-02, -9.4643e-03,  7.4931e-02,  6.7022e-02,  4.6328e-02,
          4.1413e-03, -4.7664e-02,  1.1573e-01,  2.0813e-02,  7.8718e-02,
          4.6320e-02, -3.8637e-02, -2.9391e-01,  2.2724e-02, -2.3529e-01,
          1.3415e-01,  4.7424e-02, -5.7352e-02, -5.2678e-02,  1.7793e-01,
          2.2460e-03,  1.8626e-01,  1.8894e-01,  4.1398e-02,  1.8930e-01,
         -5.3658e-02, -8.8961e-02,  1.5941e-01,  4.4156e-02,  2.4779e-02,
          4.8252e-02, -1.2180e-01,  1.0094e-01,  6.3193e-02,  2.4881e-02,
         -5.3403e-02,  1.9461e-01, -5.7637e-02, -1.5025e-01, -5.4754e-03,
          7.0096e-02,  1.1126e-01, -3.9668e-01,  2.1653e-01,  1.6591e-02,
         -1.7901e-01,  1.0294e-01,  6.3018e-02, -3.0676e-01,  8.5504e-02,
          5.8542e-02,  6.1896e-02, -2.2939e-01, -3.7933e-03,  2.6520e-01,
         -2.2157e-02,  2.0982e-01,  1.2099e-01,  9.8588e-02, -2.0795e-02,
         -4.9302e-02, -5.1215e-02,  6.7354e-02,  1.8551e-02,  1.3290e-01,
          1.0492e-01, -5.4236e-02,  5.8354e-02,  1.3614e-02,  1.0947e-02,
          1.5317e-01, -8.6723e-02, -3.4239e-01, -4.3005e-02, -3.8103e-02,
          1.1201e-01,  5.0324e-02,  2.1222e-02, -1.0893e-01,  6.9251e-02,
          7.8891e-02, -5.4532e-03,  1.6121e-01,  1.9021e-01, -1.0281e-01,
         -1.0264e-01,  1.8469e-01,  1.4738e-02, -2.0191e-02,  1.0040e-01,
         -3.2994e-01,  9.8982e-02,  8.0360e-03,  1.0761e-01,  3.0289e-01,
          8.1558e-02,  1.4355e-01,  1.1201e-01, -1.1064e-02,  3.1383e-02,
          3.7764e-02, -1.7266e-02, -1.0658e-02,  3.0246e-02, -3.7480e-01,
          1.2433e-01, -1.5661e-01,  4.9313e-02, -1.1789e-01, -5.6383e-02,
          1.3488e-01, -8.4259e-02,  8.1544e-02,  1.2366e-01, -2.6412e-02,
          5.5292e-02,  1.8400e-02, -6.9494e-03,  6.8208e-02, -1.4954e-01,
         -1.4242e-02, -1.5776e-02, -5.6088e-02,  4.5916e-02, -1.3558e-02,
          2.1876e-01,  4.4712e-02,  6.1068e-02, -1.3148e-01, -1.5033e-01,
         -5.6106e-02, -3.3558e-01, -1.0317e-01,  9.8463e-02, -5.5807e-02,
          1.4955e-02,  3.6068e-02, -3.7595e-02, -3.5458e-01, -5.6479e-02,
         -5.2465e-02, -5.3361e-02,  1.0677e-01,  4.9788e-02, -6.1250e-02,
          8.9735e-02,  3.4024e-02,  2.4850e-01, -8.5774e-03, -5.8058e-02,
         -2.5688e-02, -4.8081e-02,  2.9960e-02, -2.0875e-01, -3.8518e-02,
          7.1406e-02,  1.4220e-01,  4.6614e-02, -2.0379e-01, -1.5046e-01,
         -2.0910e-02, -1.7877e-01,  1.0745e-01,  6.7212e-02,  7.4762e-02,
          1.3230e-01, -3.7537e-02, -4.8878e-02,  5.9741e-03,  9.1832e-02,
          7.7990e-02,  5.4479e-03, -3.0133e-02, -2.0697e-01,  1.5037e-01,
          3.0259e-01, -2.3600e-01, -1.7355e-03, -5.7603e-02, -2.8642e-02,
         -1.9974e-02, -3.5416e-01,  7.5613e-02,  2.8154e-02, -3.0459e-03,
          8.2929e-02,  6.1840e-02, -4.4045e-02, -5.5773e-02, -2.5357e-02,
          1.2704e-01, -4.3917e-02,  9.5720e-02, -1.1248e-01,  2.5141e-02,
         -3.2454e-01, -2.3494e-02,  2.2846e-02, -6.3667e-02, -5.0494e-02,
         -3.1114e-02, -2.5880e-02,  1.5591e-01, -5.6173e-02, -4.7467e-02,
          1.3928e-01,  9.6223e-02,  4.2254e-02, -2.7074e-02, -3.3300e-01,
         -5.0639e-02,  1.1634e-01,  1.2826e-01, -5.6380e-02,  6.9966e-02,
          1.7397e-01, -6.1729e-02, -5.6625e-02, -3.5030e-02, -1.1864e-01,
         -5.4816e-02,  5.0864e-02,  1.8117e-02,  8.7305e-02,  3.9139e-02,
         -3.2584e-02, -1.7975e-02,  1.4027e-01,  1.5126e-01,  2.6297e-02,
          6.7570e-02,  5.6564e-02,  7.6002e-02,  7.7737e-02,  6.3479e-02,
          9.3687e-02,  1.2394e-01,  8.2663e-02, -5.6240e-02, -1.2089e-01,
         -5.3539e-02, -5.7788e-02,  1.9458e-01, -1.7447e-01,  1.4206e-02,
          1.9677e-02, -1.2964e-01, -4.9498e-02,  1.2762e-01,  1.4742e-01,
          2.8410e-02, -3.1652e-01,  4.8532e-02,  7.4229e-02, -3.5664e-02,
         -1.2198e-01,  2.3752e-04,  1.3970e-01, -9.6228e-02,  7.3601e-03,
          8.6030e-02,  1.6295e-01,  9.8407e-02, -2.1554e-01,  1.8469e-02,
         -1.1367e-01, -1.1676e-01,  1.7324e-02,  1.2655e-02, -2.7465e-02,
         -2.2538e-01,  6.5080e-02,  2.0316e-01, -6.2827e-02,  2.9524e-02,
         -5.2807e-02,  5.2069e-04,  6.6638e-02, -5.3416e-02,  7.6444e-03,
          2.1932e-02,  6.3398e-03, -1.9308e-02, -1.3955e-01,  1.8698e-02,
         -5.9288e-02,  4.0943e-03, -4.5194e-02, -6.2980e-02,  6.8262e-02,
         -1.8848e-01,  6.9984e-02,  2.0808e-01, -2.2017e-02,  4.3610e-02,
         -7.8101e-02,  1.9062e-02,  6.2796e-02, -7.9894e-02,  1.3576e-01,
         -2.6056e-02,  2.3333e-02, -2.8570e-02,  1.0872e-01,  9.1267e-02,
          2.5061e-01,  1.0474e-01,  8.5392e-02,  6.8411e-02, -3.8639e-02,
          1.2945e-01, -4.1318e-02,  5.1344e-02, -2.9312e-02,  1.2537e-02,
          1.5366e-02,  1.2453e-01,  2.6893e-01, -2.4416e-02, -6.9235e-02,
          5.6739e-02, -3.9823e-02, -5.0897e-02,  6.2925e-02,  1.6061e-01,
          7.1205e-02,  3.1982e-02,  1.3859e-01,  6.9275e-02, -1.9263e-02,
          1.4228e-01,  8.7136e-02,  1.0707e-02, -2.0420e-01,  5.5799e-02,
          8.1763e-02, -1.4348e-01, -1.5002e-01,  6.0128e-02,  8.3922e-02,
         -3.9288e-01, -6.6250e-03,  2.1520e-01, -7.3047e-02,  6.2121e-02,
         -5.9529e-02, -5.4644e-02,  1.1095e-02,  1.5836e-01, -1.5995e-01,
         -2.3951e-01,  1.6795e-02, -5.3125e-02,  4.9919e-02,  6.4854e-02,
          3.3577e-04,  4.6418e-02, -5.5911e-02,  4.2853e-02, -5.6071e-02]])
linear.bias :  tensor([-0.3326])
</pre></div>
</div>
</div>
</div>
<p>In this formulation all samples are used to fit the parameters. This in contranst to the SMO solution might take longer to optimize, but is more stable because we don’t select individial samples and ignore all the others.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">visualize_torch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_24178/3784346825.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  xy = torch.tensor(xy, dtype=torch.float32).T
/tmp/ipykernel_24178/3784346825.py:49: UserWarning: The following kwargs were not used by contour: &#39;linewidth&#39;
  plt.contour(cs0, &#39;-&#39;, levels=[0], colors=&#39;r&#39;, linewidth=5)
</pre></div>
</div>
<img alt="../_images/d973409edbb8ed3f9754f4ae3bac4709b93cc8687b7a6e6984a2b244ac7d00d2.png" src="../_images/d973409edbb8ed3f9754f4ae3bac4709b93cc8687b7a6e6984a2b244ac7d00d2.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./exercise"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="3_optimization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">3. Optimization</p>
      </div>
    </a>
    <a class="right-next"
       href="5_GPs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">5. Gaussian Processes</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-dataset">4.1 Artificial Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-minimal-optimization-smo">4.2 Sequential Minimal Optimization (SMO)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-classification-with-svm-and-smo">4.3 Multiclass Classification with SVM and SMO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-optimization-of-soft-margin-classifier">4.4 Gradient Descent Optimization of Soft Margin Classifier</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By N. Adams, L. Paehler, A. Toshev
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>