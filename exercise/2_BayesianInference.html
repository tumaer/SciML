
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2. Bayesian Linear and Logistic Regression &#8212; Introduction to Scientific Machine Learning for Engineers</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Optimization" href="3_optimization.html" />
    <link rel="prev" title="1. Linear Regression and Logistic Regression" href="1_linReg_logReg.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Scientific Machine Learning for Engineers</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../lecture/preliminaries.html">
   Preliminaries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../lecture/motivation.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../lecture/cc-1-linear.html">
   Core Content 1: Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../lecture/cc-2-optimization.html">
   Core Content 2: Optimization
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../lecture/cc-3-classic-ml.html">
   Core Content 3: Classic ML
  </a>
  <input checked class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../lecture/cc-3-sub-SVM.html">
     Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../lecture/cc-3-sub-GP.html">
     Gaussian Processes
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../lecture/cc-4-dl.html">
   Core Content 4: Deep Learning
  </a>
  <input checked class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../lecture/cc-4-gradients.html">
     Gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../lecture/cc-4-sub-MLP.html">
     Multilayer Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../lecture/cc-4-sub-CNN.html">
     Convolutional Neural Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../lecture/cc-4-rnn.html">
     Recurrent Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../lecture/cc-4-ae.html">
     Encoder-Decoder Models
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exercise
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_linReg_logReg.html">
   1. Linear Regression and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Bayesian Linear and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_optimization.html">
   3. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_SVM.html">
   4. Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5_GPs.html">
   5. Gaussian Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6_CNNs.html">
   6. CNNs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Miscellaneous
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../admin.html">
   Admin
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../books.html">
   Books
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preliminary_knowledge.html">
   Preliminary Knowledge
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../software.html">
   Software Infrastructure
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../faq.html">
   Frequently Asked Questions
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://colab.research.google.com/github/arturtoshev/SciML22-23/blob/master/exercise/2_BayesianInference.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/arturtoshev/SciML22-23"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/arturtoshev/SciML22-23/issues/new?title=Issue%20on%20page%20%2Fexercise/2_BayesianInference.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/exercise/2_BayesianInference.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-linear-regression">
   2.1 Bayesian Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problem-setting">
     Problem setting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#artificial-dataset">
     2.1.1 Artificial Dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-bayesian-classical-linear-regression-revised">
     2.1.2 Non-Bayesian (Classical) Linear Regression (revised)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-linear-regression-model">
     2.1.3 Bayesian Linear Regression Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mcmc-sampling-and-inference">
     2.1.4 MCMC: Sampling and Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svi-training-sampling-and-inference">
     2.1.5 SVI: Training, Sampling and Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise">
     2.1.6 Exercise
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-logistic-regression">
   2.2 Bayesian Logistic Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iris-dataset-revised">
     2.2.1 Iris Dataset (revised)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-logistic-regression-model">
     2.2.2 Bayesian Logistic Regression Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-svi">
     2.2.3 Using SVI
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-mcmc">
     2.2.4 Using MCMC
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     2.2.5 Exercise
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>2. Bayesian Linear and Logistic Regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-linear-regression">
   2.1 Bayesian Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problem-setting">
     Problem setting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#artificial-dataset">
     2.1.1 Artificial Dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-bayesian-classical-linear-regression-revised">
     2.1.2 Non-Bayesian (Classical) Linear Regression (revised)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-linear-regression-model">
     2.1.3 Bayesian Linear Regression Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mcmc-sampling-and-inference">
     2.1.4 MCMC: Sampling and Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svi-training-sampling-and-inference">
     2.1.5 SVI: Training, Sampling and Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise">
     2.1.6 Exercise
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-logistic-regression">
   2.2 Bayesian Logistic Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#iris-dataset-revised">
     2.2.1 Iris Dataset (revised)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-logistic-regression-model">
     2.2.2 Bayesian Logistic Regression Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-svi">
     2.2.3 Using SVI
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-mcmc">
     2.2.4 Using MCMC
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     2.2.5 Exercise
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-linear-and-logistic-regression">
<h1>2. Bayesian Linear and Logistic Regression<a class="headerlink" href="#bayesian-linear-and-logistic-regression" title="Permalink to this headline">#</a></h1>
<p><strong>Software</strong></p>
<p>Similar to the <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> module in PyTorch containing the most frequently used neural network building blocks, there are also tools for Bayesian inference tasks. In this class we will use the universal probabilistic programming language (PPL) <a class="reference external" href="https://pyro.ai/">Pyro</a> which is supported by PyTorch on the backend (The correspondint tool in TensorFlow is <a class="reference external" href="https://www.tensorflow.org/probability">TensorFlow Probability</a>). At its core these probabilistic programming systems construct a domain-specific language to express express probabilistic models, sample from probability distributions and utilize their integrated inference engines such as <code class="docutils literal notranslate"><span class="pre">Hamiltonian</span> <span class="pre">Monte-Carlo</span></code>, <code class="docutils literal notranslate"><span class="pre">Sequential</span> <span class="pre">Monte-Carlo</span></code>, <code class="docutils literal notranslate"><span class="pre">Variational</span> <span class="pre">Inference</span></code>, etc. hence making it much easier for us to express probabilistic control-flows, as well as representing uncertainties in our algorithms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># If using Colab: uncomment the following line
# !pip3 install pyro-ppl
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import pandas as pd

# visualization libraries
from matplotlib import pyplot as plt
import seaborn as sns
import arviz as az

# PyTorch
import torch
from torch.autograd import Variable

# Pyro
import pyro
import pyro.distributions as dist
from pyro.nn import PyroModule, PyroSample
from pyro.infer.autoguide import AutoDiagonalNormal, AutoMultivariateNormal, init_to_mean
# SVI and Trace_ELBO below are related to Variational Inference, which you are
# not expected to understand yet.
from pyro.infer import SVI, Trace_ELBO, Predictive, MCMC, NUTS
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># visualization utilities
plt.rcParams[&#39;font.size&#39;] = &#39;16&#39;


def plot_training_curve(loss_vec):
  fig = plt.figure(figsize=(10, 6))
  plt.plot(np.arange(len(loss_vec)) + 1, loss_vec)
  plt.xlabel(&quot;Epoch&quot;)
  plt.ylabel(&quot;Loss&quot;)
  plt.yscale(&quot;log&quot;)
  plt.title(&quot;Training curve&quot;)
  plt.grid()
</pre></div>
</div>
</div>
</div>
<section id="bayesian-linear-regression">
<h2>2.1 Bayesian Linear Regression<a class="headerlink" href="#bayesian-linear-regression" title="Permalink to this headline">#</a></h2>
<section id="problem-setting">
<h3>Problem setting<a class="headerlink" href="#problem-setting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Given: given is a set of measurement pairs <span class="math notranslate nohighlight">\(\left\{x^{(i)}, y^{\text {(i)}}\right\}_{i=1,...m}\)</span> with <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n}\)</span> and <span class="math notranslate nohighlight">\(y \in \mathbb{R}\)</span></p></li>
<li><p>Question:  if a give you a novel <span class="math notranslate nohighlight">\(x\)</span>, what would be your best guess about its corresponding <span class="math notranslate nohighlight">\(y\)</span> <strong>and how certain are you about it</strong>?</p></li>
<li><p>Linear regression assumption:
$<span class="math notranslate nohighlight">\(y \approx h(x) = wx + b + \epsilon\)</span>$</p></li>
</ul>
<p>Where we have moved to the more common weights <span class="math notranslate nohighlight">\(w = [\vartheta_1, ..., \vartheta_n]\)</span> and bias <span class="math notranslate nohighlight">\(b=\vartheta_0\)</span> notation. Other than the change of notation, the only difference up until now is the random noise term <span class="math notranslate nohighlight">\(\epsilon\)</span>, which can, in general, originate from any probability distribution. The difference to the maximul likelihood interpretation on the normal linear regression we discussed in <a class="reference external" href="https://github.com/arturtoshev/SciML_22-23/blob/master/1_linReg_logReg.ipynb">Exercise 1</a>, is that here we allow for any distribution in contrast to restricting ourselves to the Gaussian.</p>
<p>The other novel part is that in the Bayesian setting the parameters of the model (<span class="math notranslate nohighlight">\(w, b\)</span>) are random variables with some prior distribution.</p>
<p><strong>References</strong></p>
<p>This part is mainly based on <a class="reference external" href="https://pyro.ai/examples/bayesian_regression.html">this</a> Pyro tutorial and <a class="reference external" href="https://github.com/csiro-mlai/hackfest-ppl/blob/c5bb342cdae3cf325ae39a226ce89e43a530b04a/primitives/05-regression_with_data.ipynb">this</a> further code.</p>
</section>
<section id="artificial-dataset">
<h3>2.1.1 Artificial Dataset<a class="headerlink" href="#artificial-dataset" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># See Exercise 1 for more details on this dataset
a = 3  # bias
b = 2  # weight
x = np.random.rand(256)

noise = np.random.randn(256) / 4

y = a + b*x + noise

plt.scatter(x, y, color=&#39;green&#39;)
plt.grid()
plt.show()

# Reshape the input variables for training
x_train = Variable(torch.from_numpy(x.reshape(-1, 1).astype(&#39;float32&#39;)))
y_train = Variable(torch.from_numpy(y.astype(&#39;float32&#39;)))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_BayesianInference_6_0.png" src="../_images/2_BayesianInference_6_0.png" />
</div>
</div>
</section>
<section id="non-bayesian-classical-linear-regression-revised">
<h3>2.1.2 Non-Bayesian (Classical) Linear Regression (revised)<a class="headerlink" href="#non-bayesian-classical-linear-regression-revised" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Regression model
linear_reg_model = PyroModule[torch.nn.Linear](1, 1)

print(&quot;Parameter initialization:&quot;)
for name, param in linear_reg_model.named_parameters():
  print(name, &quot;: &quot;, param.data)

# Define loss and optimize
loss_fn = torch.nn.MSELoss()
optim = torch.optim.Adam(linear_reg_model.parameters(), lr=0.05)
num_iterations = 1000

loss_vec = np.zeros((num_iterations,))
for j in range(num_iterations):
    # run the model forward on the data
    y_pred = linear_reg_model(x_train).squeeze(-1)
    # calculate the mse loss
    loss = loss_fn(y_pred, y_train)
    # initialize gradients to zero - has to be done before the .backward step
    optim.zero_grad()
    # backpropagate
    loss.backward()
    # take a gradient step
    optim.step()

    loss_vec[j] = loss.item()

plot_training_curve(loss_vec)

# Inspect learned parameters
print(&quot;Learned parameters:&quot;)
for name, param in linear_reg_model.named_parameters():
    print(name, param.data.numpy())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter initialization:
weight :  tensor([[0.4083]])
bias :  tensor([-0.4561])
Learned parameters:
weight [[2.0752199]]
bias [2.9260273]
</pre></div>
</div>
<img alt="../_images/2_BayesianInference_8_1.png" src="../_images/2_BayesianInference_8_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>with torch.no_grad():
    predicted = linear_reg_model(x_train).data.numpy()

plt.clf()
plt.plot(x_train, y_train, &#39;go&#39;, label=&#39;True data&#39;)
plt.plot(x_train, predicted, &#39;--&#39;, label=&#39;Predictions&#39;)
plt.legend()
plt.grid()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_BayesianInference_9_0.png" src="../_images/2_BayesianInference_9_0.png" />
</div>
</div>
</section>
<section id="bayesian-linear-regression-model">
<h3>2.1.3 Bayesian Linear Regression Model<a class="headerlink" href="#bayesian-linear-regression-model" title="Permalink to this headline">#</a></h3>
<p>And now we seek to express our Bayesian linear regression model. For a detailed explanation of the terms of this model, see <a class="reference external" href="https://pyro.ai/examples/bayesian_regression.html#Model">this</a> example. What you should know is that there are three different pieces of <code class="docutils literal notranslate"><span class="pre">model()</span></code> which are encoded via the mapping:</p>
<ol class="simple">
<li><p>Observations <span class="math notranslate nohighlight">\(\Longleftrightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">pyro.sample</span></code> with the <code class="docutils literal notranslate"><span class="pre">obs</span></code> argument.</p></li>
<li><p>Latent random variables <span class="math notranslate nohighlight">\(\Longleftrightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">pyro.sample</span></code>.</p></li>
<li><p>Parameters <span class="math notranslate nohighlight">\(\Longleftrightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">pyro.param</span></code>.</p></li>
</ol>
<p>The model below essentially makes the following prior assumptions:</p>
<p>\begin{align}
y_i &amp;\sim \mathcal{N}(\mu, \sigma)\
\mu &amp;= w \cdot x_i + b\
w &amp;\sim \mathcal{N}(0,1)\
b &amp;\sim \mathcal{N}(0,10)\
\sigma &amp;\sim U(0,1)\
\end{align}</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class BayesianLinearRegression(PyroModule):

    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.linear = PyroModule[torch.nn.Linear](input_dim, output_dim)
        self.linear.weight = PyroSample(dist.Normal(
            0., 1.).expand([output_dim, input_dim]).to_event(2))
        self.linear.bias = PyroSample(dist.Normal(
            0., 10.).expand([output_dim]).to_event(1))

    def forward(self, x, y=None):
        sigma = pyro.sample(&quot;sigma&quot;, dist.Uniform(0., 10.))
        mean = self.linear(x).squeeze(-1)
        with pyro.plate(&quot;data&quot;, x.shape[0]):
            obs = pyro.sample(&quot;obs&quot;, dist.Normal(mean, sigma), obs=y)
        return mean


model = BayesianLinearRegression(1, 1)
</pre></div>
</div>
</div>
</div>
</section>
<section id="mcmc-sampling-and-inference">
<h3>2.1.4 MCMC: Sampling and Inference<a class="headerlink" href="#mcmc-sampling-and-inference" title="Permalink to this headline">#</a></h3>
<p>In the lecture we saw different Monte Carlo sampling strategies, e.g. Acceptance-Rejection Sampling and Adaptive Rejection Sampling, but the default sampler in Pyro are different. There are multiple reasons for that, some of which include:</p>
<ul class="simple">
<li><p>in the Acceptance-Rejection approaches, we need to know the constant <span class="math notranslate nohighlight">\(M\)</span>, which we often don’t a-priori know</p></li>
<li><p>we could use the gradient information of the likelihood surface to find regions with higher probability faster</p></li>
<li><p>often we are happy if we know an approximation of the pdf, especially if computing this quantity is very fast, which is why Variational Inference approaches are so popular.</p></li>
</ul>
<p>Here, we will see how to use the two most practically relevant approaches, namely Markov Chain Monte Carlo (MCMC) and Stochastic Variational Inference (SVI). We start with MCMC.</p>
<p>The No U-turn Sampler (NUTS, for more information see <a class="reference external" href="https://arxiv.org/abs/1111.4246">The No-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo</a> by Hoffmann &amp; Gelman, 2014) is a gradient-based MCMC approach. Starting from an initial point it generates a trajectory in the domain space such that the frequency of being in a given region is guaranteed to be proportional to the probability density of that region. For a visual of the NUTS walk see <a class="reference external" href="http://chi-feng.github.io/mcmc-demo/app.html?algorithm=NaiveNUTS&amp;target=banana">this</a> website.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Sampling

nuts_kernel = NUTS(model, adapt_step_size=True)
mcmc = MCMC(nuts_kernel, num_samples=300, warmup_steps=100)
mcmc.run(x_train, y_train)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sample: 100%|██████████| 400/400 [00:08, 48.39it/s, step size=3.99e-02, acc. prob=0.966]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>mcmc_samples = mcmc.get_samples()

for k in mcmc_samples.keys():
  s = torch.squeeze(mcmc_samples[k])
  print(f&quot;{k} = {torch.mean(s).item():.3f} +/- {torch.std(s).item():.3f}&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>linear.bias = 2.931 +/- 0.032
linear.weight = 2.065 +/- 0.054
sigma = 0.253 +/- 0.011
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for k in mcmc_samples.keys():
  print(k, torch.squeeze(mcmc_samples[k]).shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>linear.bias torch.Size([300])
linear.weight torch.Size([300])
sigma torch.Size([300])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def summary(samples):
    site_stats = {}
    for k, v in samples.items():
        site_stats[k] = {
            &quot;mean&quot;: torch.mean(v, 0),
            &quot;std&quot;: torch.std(v, 0),
            &quot;5%&quot;: v.kthvalue(int(len(v) * 0.05), dim=0)[0],
            &quot;95%&quot;: v.kthvalue(int(len(v) * 0.95), dim=0)[0],
        }
    return site_stats

# Inference


predictive = Predictive(model, mcmc_samples,
                        return_sites=(&quot;linear.weight&quot;, &quot;obs&quot;, &quot;_RETURN&quot;))
samples = predictive(x_train)
pred_summary = summary(samples)

mu = pred_summary[&quot;_RETURN&quot;]
y = pred_summary[&quot;obs&quot;]
predictions = pd.DataFrame({
    &quot;x&quot;: x_train[:, 0],
    &quot;mu_mean&quot;: mu[&quot;mean&quot;],
    &quot;mu_perc_5&quot;: mu[&quot;5%&quot;],
    &quot;mu_perc_95&quot;: mu[&quot;95%&quot;],
    &quot;y_mean&quot;: y[&quot;mean&quot;],
    &quot;y_perc_5&quot;: y[&quot;5%&quot;],
    &quot;y_perc_95&quot;: y[&quot;95%&quot;],
    &quot;y_true&quot;: y_train,
})
predictions = predictions.sort_values(by=[&quot;x&quot;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig = plt.figure(figsize=(10, 6))
plt.plot(x_train, y_train, &#39;go&#39;, label=&#39;True data&#39;)
plt.plot(predictions[&quot;x&quot;], predictions[&quot;mu_mean&quot;], &#39;--&#39;, label=&#39;Predictions&#39;)
plt.fill_between(predictions[&quot;x&quot;],
                 predictions[&quot;mu_perc_5&quot;],
                 predictions[&quot;mu_perc_95&quot;],
                 alpha=0.5)
plt.fill_between(predictions[&quot;x&quot;],
                 predictions[&quot;y_perc_5&quot;],
                 predictions[&quot;y_perc_95&quot;],
                 alpha=0.5)
plt.legend()
plt.grid()
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_BayesianInference_17_0.png" src="../_images/2_BayesianInference_17_0.png" />
</div>
</div>
</section>
<section id="svi-training-sampling-and-inference">
<h3>2.1.5 SVI: Training, Sampling and Inference<a class="headerlink" href="#svi-training-sampling-and-inference" title="Permalink to this headline">#</a></h3>
<p>An alternative to MCMC (and the default method in Pyro) is Variational Inference (VI). At this point in time you are not expected to know how variational inference works. Just remember that MCMC directly gives us samples from the posterior distribution, whereas VI first fits a surrogate posterior pdf and later samples from it as much as we want.</p>
<p>This is how you would construct a training loop with Stochastic VI (yes, VI first trains a posterior pdf surrogate and then samples from it).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Training

guide = AutoDiagonalNormal(model)
adam = pyro.optim.Adam({&quot;lr&quot;: 0.03})
# Stochastic Variational Inference (SVI) does the heavy-lifting for us.
# We give it 1. a model, 2. a guide that models the distribution of unobserver
# parameters, 3. an optimization routine used to fit parameters, and 4. a loss.
svi = SVI(model, guide, adam, loss=Trace_ELBO())

pyro.clear_param_store()
loss_vec = np.zeros((num_iterations,))
for j in range(num_iterations):
    # calculate the loss and take a gradient step
    loss = svi.step(x_train, y_train)
    loss_vec[j] = loss / len(x_train)

plot_training_curve(loss_vec)

guide.requires_grad_(False)

for name, value in pyro.get_param_store().items():
    print(name, pyro.param(name))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AutoDiagonalNormal.loc Parameter containing:
tensor([-3.6259,  2.0735,  2.9365])
AutoDiagonalNormal.scale tensor([0.0486, 0.0259, 0.0138])
</pre></div>
</div>
<img alt="../_images/2_BayesianInference_19_1.png" src="../_images/2_BayesianInference_19_1.png" />
</div>
</div>
<p>Given a trained model, how to make predictions using it?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Sampling

predictive = Predictive(model, guide=guide, num_samples=800,
                        return_sites=(&quot;linear.weight&quot;, &quot;obs&quot;, &quot;_RETURN&quot;))

# Inference

samples = predictive(x_train)
pred_summary = summary(samples)

mu = pred_summary[&quot;_RETURN&quot;]
y = pred_summary[&quot;obs&quot;]
predictions = pd.DataFrame({
    &quot;x&quot;: x_train[:, 0],
    &quot;mu_mean&quot;: mu[&quot;mean&quot;],
    &quot;mu_perc_5&quot;: mu[&quot;5%&quot;],
    &quot;mu_perc_95&quot;: mu[&quot;95%&quot;],
    &quot;y_mean&quot;: y[&quot;mean&quot;],
    &quot;y_perc_5&quot;: y[&quot;5%&quot;],
    &quot;y_perc_95&quot;: y[&quot;95%&quot;],
    &quot;y_true&quot;: y_train,
})
predictions = predictions.sort_values(by=[&quot;x&quot;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6), sharey=True)

ax.plot(predictions[&quot;x&quot;],
        predictions[&quot;mu_mean&quot;])
ax.fill_between(predictions[&quot;x&quot;],
                predictions[&quot;mu_perc_5&quot;],
                predictions[&quot;mu_perc_95&quot;],
                alpha=0.5)
ax.fill_between(predictions[&quot;x&quot;],
                predictions[&quot;y_perc_5&quot;],
                predictions[&quot;y_perc_95&quot;],
                alpha=0.5)
ax.plot(predictions[&quot;x&quot;],
        predictions[&quot;y_true&quot;],
        &quot;o&quot;)
ax.set(xlabel=&quot;x&quot;,
       ylabel=&quot;y&quot;,
       title=&quot;Bayesian Linear Regression&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Text(0.5, 0, &#39;x&#39;),
 Text(0, 0.5, &#39;y&#39;),
 Text(0.5, 1.0, &#39;Bayesian Linear Regression&#39;)]
</pre></div>
</div>
<img alt="../_images/2_BayesianInference_22_1.png" src="../_images/2_BayesianInference_22_1.png" />
</div>
</div>
<p>We now need to (automatically) find the best distribution for our model and perform inference over our regression algorithm to learn the posterior distribution over our unobserved parameters.</p>
</section>
<section id="exercise">
<h3>2.1.6 Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">#</a></h3>
<p>Apply Bayesian Linear Regression with the Terrain Ruggedness vs GDP dataset from <a class="reference external" href="https://pyro.ai/examples/bayesian_regression.html">this</a> Pyro tutorial.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>####################
# TODO


####################
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="bayesian-logistic-regression">
<h2>2.2 Bayesian Logistic Regression<a class="headerlink" href="#bayesian-logistic-regression" title="Permalink to this headline">#</a></h2>
<section id="iris-dataset-revised">
<h3>2.2.1 Iris Dataset (revised)<a class="headerlink" href="#iris-dataset-revised" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn import datasets
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.datasets import load_iris

X_df, y = load_iris(as_frame=True, return_X_y=True)

target_names = [&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;]
# standardize X
X_df.columns = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]
X = X_df.apply(lambda x: (x - x.mean())/x.std(), axis=0)
X.columns = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;]

X[&#39;iris_type&#39;] = y
X[&#39;is_setosa&#39;] = np.where(X[&#39;iris_type&#39;].values == 0, 1, 0)
X.info()

data = torch.tensor(X[[&#39;petal_width&#39;]].values, dtype=torch.float)
target = torch.tensor(X[&#39;is_setosa&#39;].values, dtype=torch.float)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 150 entries, 0 to 149
Data columns (total 6 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   sepal_length  150 non-null    float64
 1   sepal_width   150 non-null    float64
 2   petal_length  150 non-null    float64
 3   petal_width   150 non-null    float64
 4   iris_type     150 non-null    int64  
 5   is_setosa     150 non-null    int64  
dtypes: float64(4), int64(2)
memory usage: 7.2 KB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X.head()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal_length</th>
      <th>sepal_width</th>
      <th>petal_length</th>
      <th>petal_width</th>
      <th>iris_type</th>
      <th>is_setosa</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.897674</td>
      <td>1.015602</td>
      <td>-1.335752</td>
      <td>-1.311052</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.139200</td>
      <td>-0.131539</td>
      <td>-1.335752</td>
      <td>-1.311052</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.380727</td>
      <td>0.327318</td>
      <td>-1.392399</td>
      <td>-1.311052</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.501490</td>
      <td>0.097889</td>
      <td>-1.279104</td>
      <td>-1.311052</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.018437</td>
      <td>1.245030</td>
      <td>-1.335752</td>
      <td>-1.311052</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.pairplot(data=X, hue=&#39;iris_type&#39;, palette=&quot;colorblind&quot;, corner=True); #setosa is easy to distinguish, versicolor and virginica are harder
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_BayesianInference_30_0.png" src="../_images/2_BayesianInference_30_0.png" />
</div>
</div>
</section>
<section id="bayesian-logistic-regression-model">
<h3>2.2.2 Bayesian Logistic Regression Model<a class="headerlink" href="#bayesian-logistic-regression-model" title="Permalink to this headline">#</a></h3>
<p>Taks:</p>
<ul class="simple">
<li><p>Given “petal_width”, determine whether the class is “setosa”.</p></li>
</ul>
<p>The tools used here are the same as for the Bayesian Linear Regression task, thus we omit lengthy explanations. Reach out to us if something is unclear!</p>
<blockquote>
<div><p>Hint: You might consider <a class="reference external" href="https://github.com/seanreed1111/colab-demos/blob/8fa0713b2fb0fd9018de9de89593cc427d6b08d2/old/pyro_logistic_regression_iris.ipynb">this</a> useful.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class BayesianLogisticRegression(PyroModule):
    def __init__(self, in_features, out_features = 1, bias = True):
        super().__init__()
        self.linear = PyroModule[torch.nn.Linear](in_features, out_features)
        self.linear.weight = PyroSample(dist.Normal(-35., 5.).expand([out_features, in_features]).to_event(2))
        if bias:
          self.linear.bias = PyroSample(dist.Uniform(-25., 5.).expand([out_features]).to_event(1))
        
    def forward(self, x, y=None):
        logits = self.linear(x).squeeze(-1)

        with pyro.plate(&quot;data&quot;, x.shape[0]):
            obs = pyro.sample(&quot;obs&quot;, dist.Bernoulli(logits=logits), obs=y)
        return logits
</pre></div>
</div>
</div>
</div>
</section>
<section id="using-svi">
<h3>2.2.3 Using SVI<a class="headerlink" href="#using-svi" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model = BayesianLogisticRegression(data.size(1))
guide = AutoMultivariateNormal(model, init_loc_fn=init_to_mean)

def train(model, guide, lr=0.01, n_steps=2000):
    pyro.set_rng_seed(1)
    pyro.clear_param_store()
    
    gamma = 0.01  # final learning rate will be gamma * initial_lr
    lrd = gamma ** (1 / n_steps)
    adam = pyro.optim.ClippedAdam({&#39;lr&#39;: lr, &#39;lrd&#39;: lrd})

    svi = SVI(model, guide, adam, loss=Trace_ELBO())

    for i in range(n_steps):
        elbo = svi.step(data, target)
        if i % 500 == 0:
          print(f&quot;Elbo loss: {elbo}&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time
train(model, guide)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Elbo loss: 9.350290536880493
Elbo loss: 2.805009201169014
Elbo loss: 1.4718698005599435
Elbo loss: 1.9484067456796765
CPU times: user 41.5 s, sys: 18.9 ms, total: 41.5 s
Wall time: 5.19 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>num_samples = 1000
predictive = Predictive(model, guide=guide, num_samples=num_samples)

svi_samples = {k: v.reshape((num_samples,-1)).detach().cpu().numpy()
               for k, v in predictive(data, target).items()
               if k != &quot;obs&quot;}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for k in svi_samples.keys():
  print(k, svi_samples[k].mean())
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>linear.weight -34.34986
linear.bias -16.362192
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>guide.quantiles([0.05,0.50,0.95])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;linear.weight&#39;: tensor([[[-37.1637]],
 
         [[-34.3472]],
 
         [[-31.5306]]]),
 &#39;linear.bias&#39;: tensor([[-20.3906],
         [-16.5955],
         [-11.3549]])}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>svi_samples[&#39;linear.weight&#39;].mean(axis=0)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-34.34986], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>samples = pd.DataFrame({&#39;bias&#39;:svi_samples[&#39;linear.bias&#39;].squeeze(), &#39;weight&#39;:svi_samples[&#39;linear.weight&#39;].squeeze()})
sns.pairplot(data=samples, corner=True)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x7feed4a9d430&gt;
</pre></div>
</div>
<img alt="../_images/2_BayesianInference_40_1.png" src="../_images/2_BayesianInference_40_1.png" />
</div>
</div>
</section>
<section id="using-mcmc">
<h3>2.2.4 Using MCMC<a class="headerlink" href="#using-mcmc" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>nuts_kernel = NUTS(model)
mcmc = MCMC(nuts_kernel, num_samples=300, warmup_steps=100)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time
mcmc.run(data, target)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sample: 100%|██████████| 400/400 [00:03, 105.97it/s, step size=2.22e-01, acc. prob=0.859]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 3.79 s, sys: 4.94 ms, total: 3.79 s
Wall time: 3.78 s
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hmc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for k in hmc_samples.keys():
  print(k, np.mean(hmc_samples[k]))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>linear.bias -17.952686
linear.weight -34.984016
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.displot(hmc_samples[&#39;linear.bias&#39;][:,0])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.FacetGrid at 0x7feefe8265b0&gt;
</pre></div>
</div>
<img alt="../_images/2_BayesianInference_46_1.png" src="../_images/2_BayesianInference_46_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sns.kdeplot(hmc_samples[&#39;linear.weight&#39;][:,0,0])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot: ylabel=&#39;Density&#39;&gt;
</pre></div>
</div>
<img alt="../_images/2_BayesianInference_47_1.png" src="../_images/2_BayesianInference_47_1.png" />
</div>
</div>
</section>
<section id="id1">
<h3>2.2.5 Exercise<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>Apply Bayesian logistic regression using all existing inputs:</p>
<p><code class="docutils literal notranslate"><span class="pre">iris_type</span> <span class="pre">~</span> <span class="pre">sepal_length</span> <span class="pre">+</span> <span class="pre">sepal_width</span> <span class="pre">+</span> <span class="pre">petal_length</span> <span class="pre">+</span> <span class="pre">petal_w</span></code></p>
<blockquote>
<div><p>Hint: Look at <a class="reference external" href="https://github.com/seanreed1111/colab-demos/blob/8fa0713b2fb0fd9018de9de89593cc427d6b08d2/old/pyro_logistic_regression_iris.ipynb">this</a> code for help.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>####################
# TODO


####################
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./exercise"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="1_linReg_logReg.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">1. Linear Regression and Logistic Regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3_optimization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3. Optimization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By N. Adams, L. Paehler, A. Toshev<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>