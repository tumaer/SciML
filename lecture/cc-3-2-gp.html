

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Gaussian Processes &#8212; Introduction to Scientific Machine Learning for Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture/cc-3-2-gp';</script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Core Content 4: Deep Learning" href="cc-4-0-dl.html" />
    <link rel="prev" title="Support Vector Machines" href="cc-3-1-svm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../about.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cc-1-0-basics.html">Core Content 1: Basics</a><input checked class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cc-1-1-linear.html">Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-1-2-gmm-mcmc.html">GMM and MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-1-3-bayes.html">Bayesian methods</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cc-2-0-optim.html">Core Content 2: Optimization</a><input checked class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cc-2-1-algorithms.html">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-2-2-tricks.html">Tricks of Optimization</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="cc-3-0-ml.html">Core Content 3: Classic ML</a><input checked class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="cc-3-1-svm.html">Support Vector Machines</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Gaussian Processes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cc-4-0-dl.html">Core Content 4: Deep Learning</a><input checked class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cc-4-1-gradients.html">Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-2-mlp.html">Multilayer Perceptron</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-3-cnn.html">Convolutional Neural Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-4-rnn.html">Recurrent Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-5-ae.html">Encoder-Decoder Models</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercise</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1_linReg_logReg.html">1. Linear Regression and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/2_BayesianInference.html">2. Bayesian Linear and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3_optimization.html">3. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4_SVM.html">4. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5_GPs.html">5. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/6_CNNs.html">6. CNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/7_RNNs.html">7. RNNs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../admin.html">Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preliminary_knowledge.html">Preliminary Knowledge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software.html">Software Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../practical_exam.html">Practical Exam WS22/23</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/arturtoshev/SciML22-23" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/arturtoshev/SciML22-23/issues/new?title=Issue%20on%20page%20%2Flecture/cc-3-2-gp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lecture/cc-3-2-gp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gaussian Processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian-distribution">Multivariate Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#st-and-2nd-moment">1st and 2nd Moment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-gaussian-pdf">Conditional Gaussian PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-gaussian-pdf">Marginal Gaussian PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-for-gaussian-variables">Bayes Theorem for Gaussian Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-for-gaussians">Maximum Likelihood for Gaussians</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-the-gaussian-process">Definition of the Gaussian Process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-process-for-regression">Gaussian Process for Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-kernel-configurations">Further Kernel Configurations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-prediction">Multivariate Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notes-on-gaussian-process-regression">Notes on Gaussian Process Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-the-hyperparameters">Learning the Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-processes-for-classification">Gaussian Processes for Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relation-of-gaussian-processes-to-neural-networks">Relation of Gaussian Processes to Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">Further References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gaussian-processes">
<h1>Gaussian Processes<a class="headerlink" href="#gaussian-processes" title="Permalink to this heading">#</a></h1>
<p>As the main mathematical construct behind Gaussian Processes, we first introduce the Multivariate Gaussian distribution. We will analyze this distribution in some more detail to provide reference results. For a more detailed derivation of the results, refer to <a class="reference external" href="https://link.springer.com/book/9780387310732">Bishop, 2006</a>, Section 2.3.</p>
<section id="multivariate-gaussian-distribution">
<h2>Multivariate Gaussian Distribution<a class="headerlink" href="#multivariate-gaussian-distribution" title="Permalink to this heading">#</a></h2>
<p>The <strong>univariate</strong> (for a scalar random variable) Gaussian distribution has the form</p>
<div class="math notranslate nohighlight">
\[\mathcal{N}(x; \underbrace{\mu, \sigma^2}_{\text{parameters}}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{ - \frac{(x-\mu)^2}{2 \sigma^2}\right\}.\]</div>
<p>The two parameters are the mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>The multivariate Gaussian distribution then has the following form</p>
<div class="math notranslate nohighlight">
\[\mathcal{N}(x; \mu, \Sigma)= \frac{1}{(2\pi)^{d/2}\sqrt{\det (\Sigma)}} \exp \left(-\frac{1}{2}(x-\mu)^{\top}\Sigma^{-1}(x-\mu)\right),\]</div>
<p>with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x \in \mathbb{R}^d \quad \)</span> - feature / sample / random vector</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu \in \mathbb{R}^d \quad \)</span> - mean vector</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma \in \mathbb{R}^{d \times d} \quad\)</span> - covariance matrix</p></li>
</ul>
<p>Properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Delta^2=(x-\mu)^{\top}\Sigma^{-1}(x-\mu)\)</span> is a quadratic form.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta\)</span> is the Mahalanobis distance from <span class="math notranslate nohighlight">\(\mu\)</span> to <span class="math notranslate nohighlight">\(x\)</span>. It collapses to the Euclidean distance for <span class="math notranslate nohighlight">\(\Sigma = I\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is symmetric positive semi-definite and its diagonal elements contain the variance, i.e. covariance with itself.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> can be diagonalized with real eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\sum u_i = \lambda_i u_i \quad i=1,...,d\]</div>
<p>and eigenvectors <span class="math notranslate nohighlight">\(u_i\)</span> forming a unitary matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}U= \left[\begin{array}{l} u_1^{\top} \\ ... \\ u_d^{\top}\\ \end{array}\right], \quad U^{\top}U=I\end{split}\]</div>
<div class="math notranslate nohighlight">
\[U\Sigma U^{\top} = \lambda, \quad \lambda \text{ diagonal matrix of eigenvalues}\]</div>
<p>If we apply the variable transformation <span class="math notranslate nohighlight">\(y=U(x-\mu)\)</span>, we can transform the Gaussian PDF to the <span class="math notranslate nohighlight">\(y\)</span> coordinates according to the change of variables rule (see preliminaries)</p>
<div class="math notranslate nohighlight">
\[p_X(x) = p_Y(y) \underbrace{\left| \frac{\partial y}{\partial x} \right|}_{det|U_{ij}|},\]</div>
<p>which leads to</p>
<div class="math notranslate nohighlight">
\[p(y) = \Pi_{i=1}^d \frac{1}{\sqrt{2 \pi \lambda_i}} \exp \left\{ - \frac{y_i^2}{2\lambda_i} \right\}\]</div>
<p>Note that the diagonalization of <span class="math notranslate nohighlight">\(\Delta\)</span> leads to a factorization of the PDF into <span class="math notranslate nohighlight">\(d\)</span> 1D PDFs.</p>
<section id="st-and-2nd-moment">
<h3>1st and 2nd Moment<a class="headerlink" href="#st-and-2nd-moment" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[E[x] = \mu\]</div>
<div class="math notranslate nohighlight">
\[E[xx^{\top}] = \mu \mu^{\top} + \Sigma\]</div>
<div class="math notranslate nohighlight">
\[Cov(x) = \Sigma\]</div>
</section>
<section id="conditional-gaussian-pdf">
<h3>Conditional Gaussian PDF<a class="headerlink" href="#conditional-gaussian-pdf" title="Permalink to this heading">#</a></h3>
<p>Consider the case <span class="math notranslate nohighlight">\(x \sim \mathcal{N}(x; \mu, \Sigma)\)</span> being a <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian random vector. We can partition <span class="math notranslate nohighlight">\(x\)</span> into two disjoint subsets <span class="math notranslate nohighlight">\(x_a\)</span> and <span class="math notranslate nohighlight">\(x_b\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
x=\left(\begin{array}{c}
x_a \\
x_b
\end{array}\right) .
\end{split}\]</div>
<p>The corresponding partitions of the mean vector <span class="math notranslate nohighlight">\(\mu\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> become</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mu=\left(\begin{array}{l}
\mu_a \\
\mu_b
\end{array}\right)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma=\left(\begin{array}{ll}
\Sigma_{a a} &amp; \Sigma_{a b} \\
\Sigma_{b a} &amp; \Sigma_{b b}
\end{array}\right) .
\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\Sigma^T=\Sigma\)</span> implies that <span class="math notranslate nohighlight">\(\Sigma_{a a}\)</span> and <span class="math notranslate nohighlight">\(\Sigma_{b b}\)</span> are symmetric, while <span class="math notranslate nohighlight">\(\Sigma_{b a}=\Sigma_{a b}^{\mathrm{T}}\)</span>.</p>
<p>We also define the precision matrix <span class="math notranslate nohighlight">\(\Lambda = \Sigma^{-1}\)</span>, being the inverse of the covariance matrix, where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Lambda=\left(\begin{array}{ll}
\Lambda_{a a} &amp; \Lambda_{a b} \\
\Lambda_{b a} &amp; \Lambda_{b b}
\end{array}\right)
\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(\Lambda_{a a}\)</span> and <span class="math notranslate nohighlight">\(\Lambda_{b b}\)</span> are symmetric, while <span class="math notranslate nohighlight">\(\Lambda_{a b}^{\mathrm{T}}=\Lambda_{b a}\)</span>. Note, <span class="math notranslate nohighlight">\(\Lambda_{a a}\)</span> is not simply the inverse of <span class="math notranslate nohighlight">\(\Sigma_{a a}\)</span>.</p>
<p>Now, we want to evaluate <span class="math notranslate nohighlight">\(\mathcal{N}(x_a| x_b; \mu, \Sigma)\)</span> and use <span class="math notranslate nohighlight">\(p(x_a, x_b) = p(x_a|x_b)p(x_b)\)</span>. We expand all terms of the pdf given the split, and consider all terms that do not involve <span class="math notranslate nohighlight">\(x_a\)</span> as constant and then we compare with the generic form of a Gaussian for <span class="math notranslate nohighlight">\(p(x_a| x_b)\)</span>. We can decompose the equation into quadratic, linear and constant terms in <span class="math notranslate nohighlight">\(x_a\)</span> and find an expression for <span class="math notranslate nohighlight">\(p(x_a|x_b)\)</span>.</p>
<blockquote>
<div><p>For all intermediate steps refer to <a class="reference external" href="https://link.springer.com/book/9780387310732">Bishop, 2006</a>.</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mu_{a \mid b} &amp; =\mu_a+\Sigma_{a b} \Sigma_{b b}^{-1}\left(x_b-\mu_b\right) \\
\Sigma_{a \mid b} &amp; =\Sigma_{a a}-\Sigma_{a b} \Sigma_{b b}^{-1} \Sigma_{b a}
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[p(x_a| x_b) = \mathcal{N(x; \mu_{a|b}, \Sigma_{a|b})}\]</div>
</section>
<section id="marginal-gaussian-pdf">
<h3>Marginal Gaussian PDF<a class="headerlink" href="#marginal-gaussian-pdf" title="Permalink to this heading">#</a></h3>
<p>For the marginal PDF we integrate out the dependence on <span class="math notranslate nohighlight">\(x_b\)</span> of the joint PDF:</p>
<div class="math notranslate nohighlight">
\[p(x_a) = \int p(x_a, x_b) dx_b.\]</div>
<p>We can follow similar steps as above for separating terms that involve <span class="math notranslate nohighlight">\(x_a\)</span> and <span class="math notranslate nohighlight">\(x_b\)</span>. After integrating out the Guassian with a quadratic term depending on <span class="math notranslate nohighlight">\(x_b\)</span> we are left with a lengthy term involving <span class="math notranslate nohighlight">\(x_a\)</span> only. By comparison with a Gaussian PDF and re-using the block relation between <span class="math notranslate nohighlight">\(\Lambda\)</span> and <span class="math notranslate nohighlight">\(\Sigma\)</span> as above we obtain for the marginal</p>
<div class="math notranslate nohighlight">
\[p(x_a) = \mathcal{N}(x_a; \mu_a, \Sigma_{a a}).\]</div>
</section>
<section id="bayes-theorem-for-gaussian-variables">
<h3>Bayes Theorem for Gaussian Variables<a class="headerlink" href="#bayes-theorem-for-gaussian-variables" title="Permalink to this heading">#</a></h3>
<p>Generative learning addresses the problem of finding a posterior PDF from a likelihood and prior. The basis is Bayes rule for conditional probabilities</p>
<div class="math notranslate nohighlight">
\[p(x|y) = \frac{p(y|x)p(x)}{p(y)}\]</div>
<p>We want to find the posterior <span class="math notranslate nohighlight">\(p(x|y)\)</span> and the evidence <span class="math notranslate nohighlight">\(p(y)\)</span> under the assumption that the likelihood <span class="math notranslate nohighlight">\(p(y|x)\)</span> and the prior are <strong>linear Gaussian models</strong>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(y|x)\)</span> is Gaussian and has a mean that depends at most linearly on <span class="math notranslate nohighlight">\(x\)</span> and a variance that is independent of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(p(x)\)</span> is Gaussian.</p></li>
</ul>
<p>These requirements correspond to the following structure of <span class="math notranslate nohighlight">\(p(x)\)</span> and <span class="math notranslate nohighlight">\(p(y|x)\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
p(x) &amp; =\mathcal{N}\left(x \mid \mu, \Lambda^{-1}\right) \\
p(y \mid x) &amp; =\mathcal{N}\left(y \mid A x+b, L^{-1}\right).
\end{aligned}
\end{split}\]</div>
<p>From that we can derive an analytical evidence (marginal) and posterior (conditional) distributions (for more details see <a class="reference external" href="https://link.springer.com/book/9780387310732">Bishop, 2006</a>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
p(y) &amp; =\mathcal{N}\left(y \mid A \mu+b, L^{-1}+A \Lambda^{-1} A^{\top}\right) \\
p(x \mid y) &amp; =\mathcal{N}\left(x \mid \Sigma\left\{A^{\top} L(y-b)+\Lambda \mu\right\}, \Sigma\right),
\end{aligned}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\Sigma=\left(\Lambda+A^{\top} L A\right)^{-1} .
\]</div>
</section>
<section id="maximum-likelihood-for-gaussians">
<h3>Maximum Likelihood for Gaussians<a class="headerlink" href="#maximum-likelihood-for-gaussians" title="Permalink to this heading">#</a></h3>
<p>In generative learning, we need to infer PDFs from data. Given a dataset <span class="math notranslate nohighlight">\(X=(x_1, ..., x_N)\)</span>, where <span class="math notranslate nohighlight">\(x_i\)</span> are i.i.d. random variables drawn from a multivariate Gaussian, we can estimate <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\Sigma\)</span> from the maximum likelihood (ML) (for more details see <a class="reference external" href="https://link.springer.com/book/9780387310732">Bishop, 2006</a>):</p>
<div class="math notranslate nohighlight">
\[\mu_{ML} = \frac{1}{N} \sum_{n=1}^N x_n\]</div>
<div class="math notranslate nohighlight">
\[\Sigma_{ML} = \frac{1}{N} \sum_{n=1}^N (x-\mu)^{\top}(x-\mu)\]</div>
<p><span class="math notranslate nohighlight">\(\mu_{ML}\)</span> and <span class="math notranslate nohighlight">\(\Sigma_{ML}\)</span> correspond to the so-called sample or empirical estimates. However, <span class="math notranslate nohighlight">\(\Sigma_{ML}\)</span> does not deliver an unbiased estimate of the covariance. The difference decreases with <span class="math notranslate nohighlight">\(N \to \infty\)</span>, but for a certain <span class="math notranslate nohighlight">\(N\)</span> we have <span class="math notranslate nohighlight">\(\Sigma_{ML}\neq \Sigma\)</span>. The practical reason used in the above derivation is that the <span class="math notranslate nohighlight">\(\mu_{ML}\)</span> estimate may occur as <span class="math notranslate nohighlight">\(\bar{x}\)</span> within the sampling of <span class="math notranslate nohighlight">\(\Sigma\)</span> <span class="math notranslate nohighlight">\(\Rightarrow\)</span> miscount by one. An unbiased sample variance can be defined as</p>
<div class="math notranslate nohighlight">
\[\widetilde{\Sigma} = \frac{1}{N-1} \sum_{n=1}^N (x_n-\mu_{ML})^{\top}(x_n-\mu_{ML})\]</div>
</section>
</section>
<section id="definition-of-the-gaussian-process">
<h2>Definition of the Gaussian Process<a class="headerlink" href="#definition-of-the-gaussian-process" title="Permalink to this heading">#</a></h2>
<p>Gaussian Processes are a generalization of the generative learning concept based on Bayes rule and Gaussian distributions as models derived from <a class="reference external" href="https://www.eecs189.org/static/notes/n18.pdf">Gaussian Discriminant Analysis</a>. We explain Gaussian Processes (GPs) on the example of regression and discuss an adaptation to discrimination in the spirit of Gaussian Discriminant Analysis (GDA), however not being identical to GDA at the same time. Consider linear regression:</p>
<div class="math notranslate nohighlight">
\[
h(x) = \omega^{\top} \varphi(x)
\]</div>
<p>here <span class="math notranslate nohighlight">\(\omega \in \mathbb{R}^{m}\)</span> are the weights, <span class="math notranslate nohighlight">\(\varphi \in \mathbb{R}^{m}\)</span> is the feature map, and <span class="math notranslate nohighlight">\(h(x)\)</span> is the hypothesis giving the probability of <span class="math notranslate nohighlight">\(y\)</span>. We now introduce an isotropic Gaussian prior on <span class="math notranslate nohighlight">\(\omega\)</span>, where isotropic is defined as being a diagonal matrix <span class="math notranslate nohighlight">\(\zeta\)</span> with some component <span class="math notranslate nohighlight">\(\alpha^{-1}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
p(\omega) = \Pi(\omega; 0, \alpha^{-1} I)
\]</div>
<p>where <span class="math notranslate nohighlight">\(0\)</span> is the zero-mean nature of the Gaussian prior, with covariance <span class="math notranslate nohighlight">\(\alpha^{-1} I\)</span>.</p>
<blockquote>
<div><p>Note that <span class="math notranslate nohighlight">\(\alpha\)</span> is a hyperparameter, i.e. responsible for the model properties, it is called hyper <span class="math notranslate nohighlight">\(\ldots\)</span> to differentiate from the weight parameters <span class="math notranslate nohighlight">\(\omega\)</span>.</p>
</div></blockquote>
<p>Now let’s consider the data samples <span class="math notranslate nohighlight">\(\left( y^{(i)} \in \mathbb{R}, x^{(i)} \in \mathbb{R}^{m} \right)_{i=1, \ldots, n}\)</span>. We are interested in leveraging the information contained in the data samples, i.e. in probabilistic lingo in the joint distribution of <span class="math notranslate nohighlight">\(y^{(1)}, \ldots, y^{(n)}\)</span>. We can then write</p>
<div class="math notranslate nohighlight">
\[
y = \phi \hspace{2pt} \omega
\]</div>
<p>where <span class="math notranslate nohighlight">\(y \in \mathbb{R}^{n}\)</span>, <span class="math notranslate nohighlight">\(\phi \in \mathbb{R}^{n \times m}\)</span>, and <span class="math notranslate nohighlight">\(\omega \in \mathbb{R}^{m}\)</span>. <span class="math notranslate nohighlight">\(\phi\)</span> is the well-known design matrix with feature map. As we know that <span class="math notranslate nohighlight">\(\omega\)</span> is Gaussian the probability density function (pdf) of <span class="math notranslate nohighlight">\(y\)</span> is also Gaussian:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \mathbb{E}[y] &amp;= \mathbb{E}[\phi \omega] = \phi \mathbb{E}[\omega] = 0 \\
    \text{Cov}[y] &amp;= \mathbb{E}[y y^{\top}] = \phi \mathbb{E}[\omega \omega^{\top}] = \frac{1}{\alpha} \phi \phi^{\top} = K
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> is the Grammian matrix which we already encountered before.</p>
<p>A Gaussian Process is now defined as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y^{(i)} = h(x^{(i)}), \quad i=1, \ldots, m\)</span> have a joint Gaussian PDF, i.e. are fully determined by their 2nd-order statistics.</p></li>
</ul>
</section>
<section id="gaussian-process-for-regression">
<h2>Gaussian Process for Regression<a class="headerlink" href="#gaussian-process-for-regression" title="Permalink to this heading">#</a></h2>
<p>Take into account Gaussian noise, a modeling assumption, on sample data:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    y^{(i)} &amp;= h(x^{(i)}) + \varepsilon^{(i)}, \quad i=1, \ldots, m \\
    p(y^{(i)} | h(x^{(i)})) &amp;= \Pi \left( y^{(i)}, h(x^{(i)}), \frac{1}{\beta} \right)
\end{align}
\end{split}\]</div>
<p>with the isotropic Gaussian <span class="math notranslate nohighlight">\(\Pi\)</span>.</p>
<div>
<center><img src = "https://i.imgur.com/Uf1YFhD.png" width = "400">
<center><img src="https://i.imgur.com/AJdLn8l.png" width="400">
</div>
<p>where <span class="math notranslate nohighlight">\(h(x^{(i)}) = h^{(i)}\)</span> becomes a <em>latent variable</em>. For the latent variables (these correspond to the noise-free regression data we considered earlier) we also assume a Gaussian model PDF as prior</p>
<div class="math notranslate nohighlight">
\[
p(h) = \Pi(y; 0, K)
\]</div>
<p>The requisite posterior PDF then becomes</p>
<div class="math notranslate nohighlight">
\[
p(y|x) = \int p(y | h, x) p(h|x) dh
\]</div>
<p>i.e. the posterior pdf is defined by the by marginalization of <span class="math notranslate nohighlight">\(p(y, h| x)\)</span> over the latent variable h. For the computation of <span class="math notranslate nohighlight">\(p(y|x)\)</span> we follow previous computations with adjustments of the notation where appropriate.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z = \left[\begin{matrix}
    h \\
    y
\end{matrix}\right], \quad h \in \mathbb{R}^{n}, y \in \mathbb{R}^{n}
\end{split}\]</div>
<p>from previous lectures we recall</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \mathbb{E} &amp;= A \mu + b \\
    \text{Cov}[y] &amp;= L^{-1} + A  \Lambda A^{\top}
\end{align}
\end{split}\]</div>
<p>into which we now substitute</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \mu &amp;= 0, \text{ by modeling assumption} \\
    b &amp;= 0, \text{ by modeling assumption} \\
    A &amp;= I \\
    L^{-1} &amp;= \frac{1}{\beta} I \\
    \Lambda^{-1} &amp;= \text{Cov}[h] = K
\end{align}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
\Longrightarrow p(y|x) = \Pi(y; 0, \frac{1}{\beta}I + K)
\]</div>
<p>Note that the kernel <span class="math notranslate nohighlight">\(K\)</span> can be presumed and is responsible for the accuracy of the prediction by the posterior. There exists a whole plethora of possible choices of K, the most commong of which are:</p>
<div>
    <center>
    <img src="https://i.imgur.com/ofLuADN.png" width="450">
</div>
<p>Looking at a practical example of a possible kernel:</p>
<div class="math notranslate nohighlight">
\[
k(x^{(i)}, x^{(j)}) = \theta_{0} e^{- \frac{\theta_{1}}{2} || x^{(i)} - x^{(j)}||^{2}} + \theta_{2} + \theta_{3} x^{(i) \top} x^{(j)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta_{0}\)</span>, <span class="math notranslate nohighlight">\(\theta_{1}\)</span>, <span class="math notranslate nohighlight">\(\theta_{2}\)</span>, <span class="math notranslate nohighlight">\(\theta_{3}\)</span> are model hyperparameters. With the above <span class="math notranslate nohighlight">\(p(y|x)\)</span> we have identified which Gaussian is inferred from the data. Moreover we have not yet formulated a predictive posterior for <em>unseen data</em>. For this purporse we extend the data set</p>
<div class="math notranslate nohighlight">
\[
\underbrace{\left( y^{(n+1)}, x^{(n+1)} \right)}_{\text{unseen data}}, \quad \underbrace{\left( y^{(n)}, x^{(n)} \right), \ldots, \left( y^{(1)}, x^{(1)} \right)}_{\text{seen data}}
\]</div>
<p>We reformulate the prediction problem as that of determining a conditional PDF, where given</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y = \left[
    \begin{matrix}
        y^{(1)}\\
        \vdots \\
        y^{(n)}
    \end{matrix}
\right]
\end{split}\]</div>
<p>we search for the conditional PDF <span class="math notranslate nohighlight">\(p(y^{(n+1)}| y)\)</span>. As intermediate we need the joint PDF</p>
<div class="math notranslate nohighlight">
\[\begin{split}
p(\tilde{y}), \quad \tilde{y} = \left[
    \begin{matrix}
        y^{(1)}\\
        \vdots \\
        y^{(n+1)}
    \end{matrix}
\right]
\end{split}\]</div>
<p>We assume that it also follows a Gaussian PDF. However, as <span class="math notranslate nohighlight">\(x^{(n+1)}\)</span> is unknown before prediction we have to make the dependence of the covariance matrix of <span class="math notranslate nohighlight">\(p(\tilde{y})\)</span> explicit:</p>
<div>
    <center>
    <img src="https://i.imgur.com/aUJvJxN.png" width="450">
</div>
<div class="math notranslate nohighlight">
\[\begin{split}
K = \left[
    \begin{matrix}
        k(x^{(1)}, x^{(n+1)}) \\
        \vdots \\
        k(x^{(n)}, x^{(n+1)})
    \end{matrix}
\right]
\end{split}\]</div>
<p>i.e. the corresponding entries in the extended <span class="math notranslate nohighlight">\(\tilde{K} = \frac{1}{\alpha} \tilde{\phi} \tilde{\phi}^{\top}\)</span> i.e. <span class="math notranslate nohighlight">\(\tilde{K}_{1, \ldots, n+1}\)</span>, where <span class="math notranslate nohighlight">\(\tilde{\phi}_{n+1} = \varphi(x^{n+1})\)</span>. The c-entry above is then given by</p>
<div class="math notranslate nohighlight">
\[
c = \tilde{K}_{n+1, n+1} + \frac{1}{\beta}
 \]</div>
<p>Using the same approach as before we can then calculate the mean and covariance of the predictive posterior for unseen data <span class="math notranslate nohighlight">\(p(y^{(n+1)}|y)\)</span>. Recalling from before</p>
<div class="math notranslate nohighlight">
\[
\mu(a | b) = \mu(a) + \Sigma_{ab} \Sigma^{-1}_{bb}(x_{b} - \mu_{b})
\]</div>
<p>where we now adjust the entries for our present case with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \mu(a|b) &amp;= \mu(y^{n+1}|y) \in \mathbb{R} \\
    \mu_{a} &amp;= 0 \\
    \Sigma_{ab} &amp;= k^{\top} \\
    \Sigma^{-1}_{bb} &amp;=  (K + \frac{1}{\beta} I)^{-1} \\
    x_{b} &amp;= y \\
    \mu_{b} &amp;= 0
\end{align}
\end{split}\]</div>
<p>and for the covariance <span class="math notranslate nohighlight">\(\Sigma\)</span> we recall</p>
<div class="math notranslate nohighlight">
\[
\Sigma_{a|b} = \Sigma_{aa} - \Sigma_{ab} \Sigma^{-1}_{bb} \Sigma_{ba}
\]</div>
<p>adjusting for the entries in our present case</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \Sigma_{aa} &amp;= c \\
    \Sigma_{ab} &amp;= k^{\top} \\
    \Sigma_{bb}^{-1} &amp;= (K + \frac{1}{\beta} I)^{-1} \\
    \Sigma_{ba} &amp;= k \\
    \Sigma_{a|b} &amp;= \Sigma_{y^{(n+1)}|y} \in \mathbb{R}
\end{align}
\end{split}\]</div>
<p>From which follows</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \mu(y^{(n+1)}|y) &amp;= k^{\top}(K + \frac{1}{\beta}I)^{-1} y \\
    \Sigma_{y^{(n+1)}|y} &amp;= c - k^{\top} (K + \frac{1}{\beta}I)^{-1} k
\end{align}
\end{split}\]</div>
<p>which fully defines the Gaussian posterior PDF of Gaussian-process regression. Looking at some sketches of GP regression, beginning with the data sample <span class="math notranslate nohighlight">\(y_{1}\)</span>, and predicting the data <span class="math notranslate nohighlight">\(y_{2}\)</span></p>
<div>
    <center>
    <img src="https://i.imgur.com/78unDSH.png" width="400">
</div>
<div>
    <center>
    <img src="https://i.imgur.com/25mZbzG.png" width="450">
</div>
<p>And in practive for a 1-dimensional function, which is being approximated with a GP, and we are monitoring the change in mean, variance and synonymously the GP-behaviour (recall, the GP is defined by the behaviour of its mean and covariance) with each successive data point.</p>
<div>
    <center>
    <img src="https://i.imgur.com/rUBB1p3.png" width="450">
</div>
<blockquote>
<div><p>This immediately shows the utility of GP-regression for engineering applications where we often have few data points but yet need to be able to provide guarantees for our predictions which GPs offer at a reasonable computational cost.</p>
</div></blockquote>
<section id="further-kernel-configurations">
<h3>Further Kernel Configurations<a class="headerlink" href="#further-kernel-configurations" title="Permalink to this heading">#</a></h3>
<p>There exist many ways in which we can extend beyond just individual kernels by multiplication and addition of simple “basis” kernels to construct better informed kernels. Just taking a quick glance at some of these possible combinations, where the following explores this combinatorial space for the following three kernels:</p>
<ul class="simple">
<li><p>Squared Exponential (SE) kernel</p></li>
<li><p>Linear (Lin) kernel</p></li>
<li><p>Periodic (Per) kernel</p></li>
</ul>
<div>
    <center>
    <img src="https://i.imgur.com/okHqR7s.png" width="450">
</div>
<div>
    <center>
    <img src="https://i.imgur.com/3IDI7FR.png" width="450">
</div>
<div>
    <center>
    <img src="https://i.imgur.com/ggMjF21.png" width="450">
</div>
<p>In higher dimensions this then takes the following shape:</p>
<div>
    <center>
    <img src="https://i.imgur.com/9RsPSLG.png" width="450">
</div>
<p>With all these possible combinations one almost begs the question if these kernels cannot be constructed automatically. The answer is, partially yes, partially no. In a sense the right kernel construction is almost like feature engineering, and while it can be automated in parts it remains a craft for the domain scientists to understand the nature of their problem to then construct the right prior distribution.</p>
<div>
    <center>
    <img src="https://i.imgur.com/tsqTFPe.png" width="450">
</div>
</section>
<section id="multivariate-prediction">
<h3>Multivariate Prediction<a class="headerlink" href="#multivariate-prediction" title="Permalink to this heading">#</a></h3>
<p>The extension to multivariate prediction is then straightforward. For the unseen data</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y' = \left[
    \begin{matrix}
        y^{(n+1)} \\
        \vdots \\
        y^{(n+l)}
    \end{matrix}
\right] \in \mathbb{R}^{l}, \quad x' = \left[
    \begin{matrix}
        x^{(n+1)}\\
        \vdots \\
        x^{(n+l)}
    \end{matrix}
\right] \in \mathbb{R}^{l \times m}
\end{split}\]</div>
<p>and for the sample data</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y = \left[
    \begin{matrix}
        y^{(1)} \\
        \vdots \\
        y^{(n)}
    \end{matrix}
\right] \in \mathbb{R}^{n}, \quad x = \left[
    \begin{matrix}
        x^{(1)}\\
        \vdots \\
        x^{(n)}
    \end{matrix}
\right] \in \mathbb{R}^{n \times m}
\end{split}\]</div>
<p>for ever larger l’s the process can then just be repeated. The mean and covariance matrix are then given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \mu(y'|y) &amp;= K(x', x) \left( K(x, x) - \frac{1}{\beta} I \right)^{-1} y \\
    \Sigma_{y'|y} &amp;= K(x', x') + \frac{1}{\beta}I - K(x', x) \left( K(x, x) + \frac{1}{\beta} I \right)^{-1} K(x, x')
\end{align}
\end{split}\]</div>
</section>
<section id="notes-on-gaussian-process-regression">
<h3>Notes on Gaussian Process Regression<a class="headerlink" href="#notes-on-gaussian-process-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K + \frac{1}{\beta}I\)</span> needs to be symmetric and positive definite, where positive definite implies that the matrix is symmetric. <span class="math notranslate nohighlight">\(K\)</span> can be constructed from valid kernel functions involving some hyperparameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p>GP regression involves a <span class="math notranslate nohighlight">\(n \times n\)</span> matrix inversion, which requires <span class="math notranslate nohighlight">\(\mathcal{O}(n^{3})\)</span> operations for learning.</p></li>
<li><p>GP prediction involves a <span class="math notranslate nohighlight">\(n \times n\)</span> matrix multiplication which requires <span class="math notranslate nohighlight">\(\mathcal{O}(n^{2})\)</span> operations.</p></li>
<li><p>The operation count can be reduced significantly when lower-dimensional projections of the kernel function on basis functions can employed, or we are able to exploit sparse matrix computations.</p></li>
</ul>
</section>
<section id="learning-the-hyperparameters">
<h3>Learning the Hyperparameters<a class="headerlink" href="#learning-the-hyperparameters" title="Permalink to this heading">#</a></h3>
<p>To infer the kernel hyperparameters from data we need to:</p>
<ol class="arabic simple">
<li><p>Introduce an appropriate likelihood function <span class="math notranslate nohighlight">\(p(y | \theta)\)</span></p></li>
<li><p>Determine the optimum <span class="math notranslate nohighlight">\(\theta\)</span> via maximum likelihood estimation (MLE) <span class="math notranslate nohighlight">\(\theta^{\star} = \arg \max \ln p(y | \theta)\)</span>, which corresponds to linear regression</p></li>
<li><p><span class="math notranslate nohighlight">\(\ln p(y|\theta) = -\frac{1}{2} \ln |K + \frac{1}{\beta}I| - \frac{1}{2} y^{\top} \left( K + \frac{1}{\beta}I \right)^{-1} y - \frac{n}{2} \ln 2 \pi\)</span></p></li>
<li><p>Use iterative gradient descent or Newton’s method to find the optimum, where you need to beware of the fact thar <span class="math notranslate nohighlight">\(p(y | \theta)\)</span> may be non-convex in <span class="math notranslate nohighlight">\(\theta\)</span>, and hence have multiple maxima</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial}{\partial \theta_{j}} \ln p(y | \theta) &amp;= - \frac{1}{2} \text{trace} \left[ \left(K + \frac{1}{\beta}I\right)^{-1} \frac{\partial(K + \frac{1}{\beta}I)}{\partial \theta_{i}} \right] \\
 &amp;+ \frac{1}{2} y^{\top} \left( K + \frac{1}{\beta} I \right)^{-1} \frac{\partial(K + \frac{1}{\beta}I)}{\partial \theta_{i}}\left(K + \frac{1}{\beta}\right)^{-1}y
\end{align*}
\end{split}\]</div>
</section>
</section>
<section id="gaussian-processes-for-classification">
<h2>Gaussian Processes for Classification<a class="headerlink" href="#gaussian-processes-for-classification" title="Permalink to this heading">#</a></h2>
<p>Without discussing all the details, we will now provide a brief sketch how Gaussian Processes can be adapted to the task of classification. Consider for simplicity the 2-class problem:</p>
<div class="math notranslate nohighlight">
\[
0 &lt; y &lt; 1, \quad h(x) = \text{sigmoid}(\varphi(x))
\]</div>
<p>the PDF for <span class="math notranslate nohighlight">\(y\)</span> conditioned on the feature <span class="math notranslate nohighlight">\(\varphi(x)\)</span> then follows a Bernoulli-distribution:</p>
<div class="math notranslate nohighlight">
\[
p(y | \varphi) = \text{sigmoid}(\varphi)^{y} \left( 1 - \text{sigmoid}(\varphi) \right)^{1-y}, \quad i=1, \ldots, n
\]</div>
<p>Finding the predictive PDF for unseen data <span class="math notranslate nohighlight">\(p(y^{(n+1)}|y)\)</span>, given the training data</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y = \left[
    \begin{matrix}
        y^{(1)} \\
        \vdots \\
        y^{(n)}
    \end{matrix}
\right]
\end{split}\]</div>
<p>we then introduce a GP prior on</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\tilde{\varphi} = \left[
    \begin{matrix}
        \varphi^{(1)} \\
        \vdots \\
        \varphi^{(n+1)}
    \end{matrix}
\right]
\end{split}\]</div>
<p>hence giving us</p>
<div class="math notranslate nohighlight">
\[
p(\tilde{\varphi}) = \Pi( \tilde{\varphi}; 0, K + \mu I)
\]</div>
<p>where <span class="math notranslate nohighlight">\(K_{ij} = k(x^{(i)}, x^{(j)})\)</span>, i.e. a Grammian matrix generated by the kernel functions from the feature map <span class="math notranslate nohighlight">\(\varphi(x)\)</span>.</p>
<blockquote>
<div><ul class="simple">
<li><p>Note that we do <strong>NOT</strong> include an explicit noise term in the data covariance as we assume that all sample data have been correctly classified.</p></li>
<li><p>For numerical reasons we introduce a noise-like form which improves conditioning of <span class="math notranslate nohighlight">\(K + \mu I\)</span></p></li>
<li><p>For two-class classification it is sufficient to predict <span class="math notranslate nohighlight">\(p(y^{(n+1)} = 1 | y)\)</span> as <span class="math notranslate nohighlight">\(p(y^{(n+1)} = 0 | y) = 1 - p(y^{(n+1)} = 1 | y)\)</span></p></li>
</ul>
</div></blockquote>
<p>Using the PDF <span class="math notranslate nohighlight">\(p(y=1|\varphi) = \text{sigmoid}(\varphi(x))\)</span> we obtain the predictive PDF:</p>
<div class="math notranslate nohighlight">
\[
p(y^{(n+1)} = 1 | y) = \int p(y^{(n+1)} = 1 | \varphi^{(n+1)})  p(\varphi^{(n+1)}| y) d\varphi^{(n+1)}
\]</div>
<p>The integration of this PDF is analytically intractable, we are hence faced with a number of choices to integrate the PDF:</p>
<ul class="simple">
<li><p>Use sampling-based methods, and Monte-Carlo approximation for the integral</p></li>
<li><p>Assume a Gaussian approximation for the posterior and evaluate the resulting convolution with the sigmoid in an approximating fashion</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(p(\varphi^{(n+1)}|y)\)</span> is the posterior PDF, which is computed from the conditional PDF <span class="math notranslate nohighlight">\(p(y| \varphi)\)</span> with a Gaussian prior <span class="math notranslate nohighlight">\(p(\varphi)\)</span>.</p>
</section>
<section id="relation-of-gaussian-processes-to-neural-networks">
<h2>Relation of Gaussian Processes to Neural Networks<a class="headerlink" href="#relation-of-gaussian-processes-to-neural-networks" title="Permalink to this heading">#</a></h2>
<p>We have not presented a definition a neural network yet, but we have already met the most primitive definition of a neural network, the perceptron, for classification. The perceptron can be thought of as a neural network with just one layer of “neurons”. On the other hand, the number of hidden units should be limited for perceptrons to limit overfitting. The essential power of neural networks  i.e. the outputs sharing the hidden units tends to get lost when the number of hidden units becomes very large. This is the core connection between neural networks and GPs, <strong>a neural network with infinite width recovers a Gaussian process</strong>.</p>
</section>
<section id="further-references">
<h2>Further References<a class="headerlink" href="#further-references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>, Section 2.3; Christopher Bishop; 2006.</p></li>
<li><p><a class="reference external" href="https://www.cs.toronto.edu/~duvenaud/thesis.pdf">Automatic Model Construction with Gaussian Processes</a>, up to page 22; David Duvenaud; 2014.</p></li>
<li><p><a class="reference external" href="https://gaussianprocess.org/gpml/">Gaussian Processes for Machine Learning</a>, Section 2.1-2.6; Carl Edward Rasmussen, and Christopher K.I. Williams; 2006.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="cc-3-1-svm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Support Vector Machines</p>
      </div>
    </a>
    <a class="right-next"
       href="cc-4-0-dl.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Core Content 4: Deep Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian-distribution">Multivariate Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#st-and-2nd-moment">1st and 2nd Moment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-gaussian-pdf">Conditional Gaussian PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-gaussian-pdf">Marginal Gaussian PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-for-gaussian-variables">Bayes Theorem for Gaussian Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-for-gaussians">Maximum Likelihood for Gaussians</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-the-gaussian-process">Definition of the Gaussian Process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-process-for-regression">Gaussian Process for Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-kernel-configurations">Further Kernel Configurations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-prediction">Multivariate Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notes-on-gaussian-process-regression">Notes on Gaussian Process Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-the-hyperparameters">Learning the Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-processes-for-classification">Gaussian Processes for Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relation-of-gaussian-processes-to-neural-networks">Relation of Gaussian Processes to Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">Further References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By N. Adams, L. Paehler, A. Toshev
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>