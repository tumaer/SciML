

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Linear Models &#8212; Introduction to Scientific Machine Learning for Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture/cc-1-1-linear';</script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="GMM and MC" href="cc-1-2-gmm-mc.html" />
    <link rel="prev" title="Core Content 1: Basics" href="cc-1-0-basics.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../about.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="cc-1-0-basics.html">Core Content 1: Basics</a><input checked class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-1-2-gmm-mc.html">GMM and MC</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-1-3-bayes.html">Bayesian methods</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cc-2-0-optim.html">Core Content 2: Optimization</a><input checked class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cc-2-1-algorithms.html">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-2-2-tricks.html">Tricks of Optimization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cc-3-0-ml.html">Core Content 3: Classic ML</a><input checked class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cc-3-1-svm.html">Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-3-2-gp.html">Gaussian Processes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cc-4-0-dl.html">Core Content 4: Deep Learning</a><input checked class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cc-4-1-gradients.html">Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-2-mlp.html">Multilayer Perceptron</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-3-cnn.html">Convolutional Neural Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-4-rnn.html">Recurrent Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-5-ae.html">Encoder-Decoder Models</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercise</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1_linReg_logReg.html">1. Linear Regression and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/2_BayesianInference.html">2. Bayesian Linear and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3_optimization.html">3. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4_SVM.html">4. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5_GPs.html">5. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/6_CNNs.html">6. CNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/7_RNNs.html">7. RNNs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../admin.html">Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preliminary_knowledge.html">Preliminary Knowledge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software.html">Software Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../practical_exam.html">Practical Exam WS22/23</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/arturtoshev/SciML22-23" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/arturtoshev/SciML22-23/issues/new?title=Issue%20on%20page%20%2Flecture/cc-1-1-linear.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lecture/cc-1-1-linear.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-interpretation">Probabilistic Interpretation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-logistic-regression">Classification &amp; Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">Further References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-models">
<h1>Linear Models<a class="headerlink" href="#linear-models" title="Permalink to this heading">#</a></h1>
<section id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h2>
<p>Linear regression belongs to the family of <strong>supervised learning</strong> approaches, as it inherently requires labeled data. With it being the simplest regression approach. The simplest example to think of would be “Given measurement pairs <span class="math notranslate nohighlight">\(\left\{x^{(i)}, y^{\text {(i)}}\right\}_{i=1,...m}\)</span>, how to fit a line <span class="math notranslate nohighlight">\(h(x)\)</span> to best approximate <span class="math notranslate nohighlight">\(y\)</span>?”</p>
<center><img src = "https://i.imgur.com/kCnveaq.png" width = "350">
<img src = "https://i.imgur.com/pqga0NA.png" width = "350"></center>
<!-- 
x = np.linspace(1.0, 5, N)
X0 = x.reshape(N, 1)
X = np.c_[np.ones((N, 1)), X0]
w = np.array([1., 1 / 9.0])
y = 15 + w[0] * x + w[1] * np.square(x)
y = y + np.random.normal(0, 1, N)  -->
<p>(Source: adapted from <a class="reference external" href="https://github.com/probml/pml-book">Murphy</a>)</p>
<p><em>How does it work?</em></p>
<ol class="arabic">
<li><p>We begin by formulating a hypothesis for the relation between input <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n}\)</span>, and output <span class="math notranslate nohighlight">\(y \in \mathbb{R}^{p}\)</span>. For conceptual clarity, we will initially set <span class="math notranslate nohighlight">\(p=1\)</span>. Then our hypothesis is represented by</p>
<div class="math notranslate nohighlight">
\[h(x)=\vartheta^{\top} x, \quad h \in \mathbb{R}^{p}\]</div>
<p>as above <span class="math notranslate nohighlight">\(p=1\)</span> in this case, and <span class="math notranslate nohighlight">\(\vartheta \in \mathbb{R}^{n}\)</span> are the parameters of our hypothesis.</p>
</li>
<li><p>Then we need a strategy to fit our hypothesis parameters <span class="math notranslate nohighlight">\(\vartheta\)</span> to the data points we have <span class="math notranslate nohighlight">\(\left\{x^{(i)}, y^{\text {(i)}}\right\}_{i=1,...m}\)</span>.</p>
<ol class="arabic">
<li><p>Define a suitable cost function <span class="math notranslate nohighlight">\(J\)</span>, which emphasizes the importance of certain traits to the model. I.e. if a certain data area is of special importance to our model we should penalize modeling failures for those points much more heavily than others. A typical choice is the <em>Least Mean Square</em> (LMS) i.e.</p>
<div class="math notranslate nohighlight">
\[J(\vartheta)=\frac{1}{2} \sum_{i=1}^{m}\left(h\left(x^{(i)}\right)-y^{(i)}\right)^{2}\]</div>
</li>
<li><p>Through an iterative application of gradient descent (more on this later in the course) find a <span class="math notranslate nohighlight">\(\vartheta\)</span> which minimizes the cost function <span class="math notranslate nohighlight">\(J(\vartheta)\)</span>. If we apply gradient descent, our update function for the hypothesis parameters then takes the following shape</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\vartheta^{(k+1)}=\vartheta^{(k)}-\alpha\frac{\partial J}{\partial \vartheta^{k}}.\]</div>
</li>
</ol>
<p>The iteration scheme can then be computed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial J}{\partial \vartheta_{j}}&amp;=\frac{\partial}{\partial \vartheta_{j}} \frac{1}{2} \sum_{i}\left(h\left(x^{(i)}\right)-y^{(i)}\right)^{2} \\
&amp;=\underset{i}{\sum} \frac{\partial }{\partial \vartheta_{j}}\left(h\left(x^{(i)}\right)-y^{(i)}\right)  \\
&amp;=\sum_{i}\left(h\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}.
\end{aligned}
\end{split}\]</div>
<p>resubstituting the iteration scheme into the update function, we then obtain the formula for <strong>batch gradient descent</strong></p>
<div class="math notranslate nohighlight">
\[\vartheta^{(k+1)}_j=\vartheta^{(k)}_j-\alpha\sum_{i}\left(h^{(k)}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}.\]</div>
<p>If we choose an alternative update rate and seek to update every hypothesis parameter individually, then we can apply stochastic gradient descent.</p>
<p>If we now utilize matrix-vector calculus, then we can find the optimal <span class="math notranslate nohighlight">\(\vartheta\)</span> in one shot. To do this we begin by defining our <strong>design matrix <span class="math notranslate nohighlight">\(X\)</span></strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}X_{m \times n}=\left[\begin{array}{c}x^{(1) \top }\\ \vdots \\ x^{(i) \top} \\ \vdots \\ x^{(m) \top}\end{array}\right]\end{split}\]</div>
<p>and then define the feature vector from all samples as</p>
<div class="math notranslate nohighlight">
\[\begin{split}Y_{m \times 1}=\left[\begin{array}{c}y^{(1)} \\ \vdots \\ y^{(i)} \\ \vdots \\ y^{(m)}\end{array}\right]\end{split}\]</div>
<p>Connecting the individual pieces we then get the update function as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left[\begin{array}{c}
h\left(x^{(0)}\right)-y^{(1)} \\
\vdots \\
h\left(x^{(i)}\right)-y^{(i)} \\
\vdots \\
h\left(x^{(m)}\right)-y^{(m)}
\end{array}\right]=X \vartheta _ {n\times 1} -Y\end{split}\]</div>
<p>According to which, the cost function then becomes</p>
<div class="math notranslate nohighlight">
\[J(\vartheta)=\frac{1}{2}(X \vartheta-Y)^{\top}(X \vartheta-Y).\]</div>
<p>As our cost function <span class="math notranslate nohighlight">\(J(\vartheta)\)</span> is convex, we now only need to check that there exists a minimum i.e.</p>
<div class="math notranslate nohighlight">
\[\nabla_{\vartheta} J(\vartheta) \stackrel{!}{=} 0.\]</div>
<p>Computing the derivative</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\nabla_{\vartheta} J(\vartheta)&amp;=\frac{1}{2} \nabla_{\vartheta}(X \vartheta-Y)^{\top}(X \vartheta-Y) \\
&amp; =\frac{1}{2} \nabla_{\vartheta}(\underbrace{\vartheta^{\top} X^{\top} X \vartheta-\vartheta^{\top} X^{\top} Y-Y^{\top} X \vartheta}_{\text {this is in fact a scalar for $p=1$}}+Y^{\top} Y) \quad \operatorname{as} {\nabla}_{\vartheta} Y^{\top} Y=0 \\
&amp;=\frac{1}{2}\left(2X^{\top} X \vartheta-2 X^{\top} Y\right)
\\
&amp;=X^{\top} X \vartheta-X^{\top} Y \stackrel{!}{=} 0
\end{align}
\end{split}\]</div>
<!-- &=\frac{1}{2} \nabla_{\vartheta} \operatorname{tr}(\cdots) -->
<p>From which follows</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\Rightarrow &amp; \quad X^{\top} X \vartheta=X^{\top} Y
\\
\Rightarrow &amp;\quad\vartheta=\left(X^{\top}X\right)^{-1}X^{\top}Y
\end{align}
\end{split}\]</div>
<p><strong>Exercise: Linear Regression Implementations</strong>
Implement the three approaches (batch gradient descent, stochastic gradient descent, and the matrix approach) to linear regression and compare their performance.</p>
<ol class="arabic simple">
<li><p>Batch Gradient Descent</p></li>
<li><p>Stochastic Gradient Descent</p></li>
<li><p>Matrix Approach</p></li>
</ol>
<section id="probabilistic-interpretation">
<h3>Probabilistic Interpretation<a class="headerlink" href="#probabilistic-interpretation" title="Permalink to this heading">#</a></h3>
<p>With much data in practice, having errors over the collected data itself, we want to be able to include a data error in our linear regression. The approach for this is <strong>Maximum Likelihood Estimation</strong> as introduced in the <em>Introduction</em> lecture. I.e. this means data points are modeled as</p>
<!-- _{\uparrow} -->
<div class="math notranslate nohighlight">
\[y^{(i)}=\vartheta^{\top} x^{(i)}+\varepsilon^{(i)}\]</div>
<p>Presuming that all our data points are <strong>independent, identically distributed (i.i.d)</strong> random variables. The noise is modeled with a normal distribution.</p>
<div class="math notranslate nohighlight">
\[p\left(\varepsilon^{(i)}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{\varepsilon^{(i) 2}}{2 \sigma^{2}}}\]</div>
<blockquote>
<div><p>While most noise distributions in practice are not normal, the normal (a.k.a. Gaussian) distribution has many nice theoretical properties making it much friendlier for theoretical derivations.</p>
</div></blockquote>
<p>Using the data error assumption, we can now derive the probability density function (pdf) for the data</p>
<div class="math notranslate nohighlight">
\[p\left(y^{(i)} \mid x^{(i)} ; \vartheta\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{\left(y^{(i)}-\vartheta^{\top} x^{(i)}\right)^{2}}{2\sigma^{2}}}\]</div>
<p>where <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is conditioned on <span class="math notranslate nohighlight">\(x^{(i)}\)</span>. If we now consider not just the individual sample <span class="math notranslate nohighlight">\(i\)</span>, but the entire dataset, we can define the likelihood for our hypothesis parameters as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
L(\vartheta) &amp;=p(Y \mid X ; \vartheta)=\prod_{i=1}^{m} p\left(y^{(i)} \mid x^{(i)} ; \vartheta\right)
\\
&amp;=\prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{\left(y^{(i)}-\vartheta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}}
\end{align}
\end{split}\]</div>
<p>The probabilistic strategy to determine the optimal hypothesis parameters <span class="math notranslate nohighlight">\(\vartheta\)</span> is then the maximum likelihood approach for which we resort to the <strong>log-likelihood</strong> as it is monotonically increasing, and easier to optimize for.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
l(\vartheta)&amp;=\log L(\vartheta)=\log \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{\left(y^{(i)}-\vartheta^{\top} x^{(i)}\right)^{2}}{2 \sigma^{2}}}\\
&amp;=\sum_{i=1}^{m} \log \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{- \frac{\left(y^{(i)}-\vartheta^{\top} x^{(i)}\right)^{2}}{2 \sigma^{2}}}\\
&amp;=m \log \frac{1}{\sqrt{2 \pi \sigma^{2}}}-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{m}\left(y^{(i)}-\vartheta^{\top} x^{(i)}\right)^{2}\\
&amp;\Rightarrow \vartheta=\operatorname{argmax} l(\vartheta)=\operatorname{argmin} \sum_{i=1}^{m}\left(y^{(i)}-\vartheta^{\top} x^{(i)}\right)^{2}\\
\end{aligned}
\end{split}\]</div>
<p><strong>This is the same result as minimizing <span class="math notranslate nohighlight">\(J(\vartheta)\)</span> from before.</strong> Interestingly enough, the Gaussian i.i.d. noise used in the maximum likelihood approach is entirely independent of <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>.</p>
<blockquote>
<div><p>Least squares method (LMS), as well as maximum likelihood regression as above are <strong>parametric learning</strong> algorithms.</p>
</div></blockquote>
<blockquote>
<div><p>If the number of parameters is <strong>not</strong> known beforehand, then the algorithms become <strong>non-parametric</strong> learning algorithms.</p>
</div></blockquote>
<p>Can maximum likelihood estimation (MLE) be made more non-parametric? Following intuition the linear MLE, as well as the LMS critically depends on the selection of the features, i.e the dimension of the parameter vector <span class="math notranslate nohighlight">\(\vartheta\)</span>. I.e. when the dimension of <span class="math notranslate nohighlight">\(\vartheta\)</span> is too low we tend to underfit, where we do not capture some of the structure of our data. An approach to cope with the problem of underfitting here is to give more weight to new, unseen data <span class="math notranslate nohighlight">\(x\)</span>. E.g. for x, where we want to estimate y:</p>
<div class="math notranslate nohighlight">
\[\hat{\delta}=\operatorname{argmax} \sum_{i=1}^{m} w^{(i)}\left(y^{(i)}-\vartheta^{\top} x^{(i)}\right)^{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\omega\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\omega^{(i)}=e^{-\frac{\left(x^{(i)}-x\right)^{2}}{2 \tau^{2}}}.\]</div>
<p>This approach naturally gives more weight to new datapoints in <span class="math notranslate nohighlight">\(x\)</span>. Hence making <span class="math notranslate nohighlight">\(\vartheta\)</span> crucially depend on <span class="math notranslate nohighlight">\(x\)</span>, and making it more non-parametric.</p>
</section>
</section>
<section id="classification-logistic-regression">
<h2>Classification &amp; Logistic Regression<a class="headerlink" href="#classification-logistic-regression" title="Permalink to this heading">#</a></h2>
<p>Summarizing the differences between regression and classification:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Regression</p></th>
<th class="head"><p>Classification</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(y \in \mathbb{R}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(y \in\{0,1\}\)</span></p></td>
</tr>
</tbody>
</table>
<div style="text-align:center">
    <img src="https://i.imgur.com/ZIb72vK.png" alt="drawing" width="500"/>
</div>
<p>(Source: <a class="reference external" href="https://github.com/probml/pyprobml/blob/master/notebooks/book1/02/iris_logreg.ipynb">Murphy</a>)</p>
<p>To achieve such classification ability we have to introduce a new hypothesis function <span class="math notranslate nohighlight">\(h(x)\)</span>. A reasonable choice would be to model the probability that <span class="math notranslate nohighlight">\(y=1\)</span> given <span class="math notranslate nohighlight">\(x\)</span> with a function <span class="math notranslate nohighlight">\(h:\mathbb{R}\rightarrow [0,1]\)</span>. The logistic regression approach chooses</p>
<div class="math notranslate nohighlight">
\[
h(x) = \varphi \left( \vartheta^{\top} x \right) = \frac{1}{1+e^{-\vartheta^{\top} x}}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\varphi(x)=\frac{1}{1+e^{-x}}=\frac{1}{2}\left(1+\tanh\frac{x}{2}\right)\]</div>
<p>is the logistic function, also called the sigmoid function.</p>
<div style="text-align:center">
    <img src="https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg" alt="drawing" width="400"/>
</div>
<p>(Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">Wikipedia</a>)</p>
<p>The advantage of this function lies in its many nice properties, such as its derivative:</p>
<div class="math notranslate nohighlight">
\[\varphi^{\prime} (x)=\frac{1}{1+e^{-x}} e^{-x}=\frac{1}{1+e^{-x}}\left(1-\frac{1}{1+e^{-x}}\right)=\varphi(x)(1-\varphi(x))\]</div>
<p>If we now want to apply the <em>Maximum Likelihood Estimation</em> approach, then we need to use our hypothesis to assign probabilities to the discrete events</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{cases}p(y=1 \mid x ; \vartheta)=h(x) &amp; \\ p(y=0 \mid x ; \vartheta)=1-h(x) &amp;
\end{cases}\end{split}\]</div>
<p>Our probability density function then becomes</p>
<div class="math notranslate nohighlight">
\[p(y \mid x ; \vartheta)=h^{y}(x)(1-h(x))^{1-y}\]</div>
<blockquote>
<div><p>This will look quite different for other types of labels, so be cautious in just copying this form of the pdf!</p>
</div></blockquote>
<p>With our pdf we can then again construct the likelihood function</p>
<div class="math notranslate nohighlight">
\[L(\vartheta) = p(y | x ; \vartheta) =\prod_{i=1}^{m} p\left(y^{(i)} \mid x^{(i)}, \vartheta\right)\]</div>
<p>Assuming the previously presumed classification buckets, and that the data is i.i.d.</p>
<div class="math notranslate nohighlight">
\[
L(\vartheta)=\prod_{i=1}^{m} h^{y^{(i)}}(x^{(i)})\left(1-h\left(x^{(i)}\right)\right)^{1-y^{(i)}}
\]</div>
<p>and then the log-likelihood decomposes to</p>
<div class="math notranslate nohighlight">
\[
l(\vartheta)=\log L(\vartheta)=\sum_{i=1}^{m}\left(y^{(i)} \log h\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h\left(x^{(i)}\right)\right)\right).
\]</div>
<p>Again we can find <span class="math notranslate nohighlight">\(\operatorname{argmax} l(\vartheta)\)</span> e.g. by gradient ascent (batch or stochastic):</p>
<div class="math notranslate nohighlight">
\[\vartheta_{j}^{(k+1)}=\vartheta_{j}^{(k)}+\left.\alpha \frac{\partial l(\vartheta)}{\partial \vartheta}\right|^{(k)}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\frac{\partial \ell(\vartheta)}{\partial \vartheta_j} &amp;=\left(y \frac{1}{h(x)}-(1-y) \frac{1}{1-h(x )}\right) \frac{\partial h(x)}{\partial \vartheta_j }\\
&amp;=\left(\frac{y-h(x)}{h(x)(1-h(x))}\right) h (x)(1-h (x)) x_j\\
&amp;=(y- h(x)) x_j
\end{align}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\Rightarrow \vartheta_{j}^{(k+1)}=\vartheta_{j}^{(k)}+\alpha \left( y^{(i)}-h(x^{(i)}) \right) x_j^{(i)}\]</div>
<p>which we can then solve with either batch gradient descent or stochastic gradient descent.</p>
<blockquote>
<div><p>The algorithm formally associated with the least squares method for logistic regression is slightly different!</p>
</div></blockquote>
<p>The alternative here is to utilize Newton’s method, which we have encountered before in numerical methods lectures. An application of Newton’s method then looks like the following:</p>
<div class="math notranslate nohighlight">
\[\vartheta_{j}^{(k+1)}=\vartheta_{j}^{(k)}\left(\left.\frac{\partial^{2} \ell}{\partial \vartheta_{i} \partial \vartheta_{j}}\right|^{(k)}\right)^{-1} \frac{\partial l}{\partial \vartheta_{j}}\]</div>
<p>Newton’s method converges quadratically, and our problem class is sufficiently smooth for Newton’s method to be applicable. The downside to this approach is that we have to compute the inverse Hessian matrix, which is an expensive computational operation.</p>
<p><strong>Exercise: Vanilla Indicator</strong></p>
<p>Using the “vanilla” indicator function instead of the sigmoid:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
g(x)= \begin{cases}1, &amp; x \geqslant 0 \\ 0, &amp; x&lt;0\end{cases}
\end{split}\]</div>
<p>derive the update functions for the gradient methods, as well as the Maximum Likelihood Estimator approach.</p>
</section>
<section id="further-references">
<h2>Further References<a class="headerlink" href="#further-references" title="Permalink to this heading">#</a></h2>
<p><strong>Linear &amp; Logistic Regression</strong></p>
<ul class="simple">
<li><p>Machine Learning Basics <a class="reference external" href="https://www.youtube.com/watch?v=73RL3WPPFE0&amp;list=PLQ8Y4kIIbzy_OaXv86lfbQwPHSomk2o2e&amp;index=2">video</a> and <a class="reference external" href="https://niessner.github.io/I2DL/slides/2.Linear.pdf">slides</a> from the “Introduction to Deep Learning” course for Informatics students at TUM.</p></li>
<li><p><a class="reference external" href="https://betanalpha.github.io/assets/case_studies/underdetermined_linear_regression.html">What happens if a linear regression is underdetermined i.e. we have fewer observations than parameters?</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="cc-1-0-basics.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Core Content 1: Basics</p>
      </div>
    </a>
    <a class="right-next"
       href="cc-1-2-gmm-mc.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">GMM and MC</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-interpretation">Probabilistic Interpretation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-logistic-regression">Classification &amp; Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">Further References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By N. Adams, L. Paehler, A. Toshev
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>