
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>1.3. Bayesian methods &#8212; Introduction to Scientific Machine Learning for Engineers</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Optimization" href="cc-2-0-optim.html" />
    <link rel="prev" title="1.2. GMM and Sampling" href="cc-1-2-gmm-mcmc.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Scientific Machine Learning for Engineers</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction.html">
   Introduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="cc-1-0-basics.html">
   1.
   <strong>
    Basics
   </strong>
  </a>
  <input checked class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="cc-1-1-linear.html">
     1.1. Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-1-2-gmm-mcmc.html">
     1.2. GMM and Sampling
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     1.3. Bayesian methods
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="cc-2-0-optim.html">
   2.
   <strong>
    Optimization
   </strong>
  </a>
  <input checked class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-2-1-algorithms.html">
     2.1. Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-2-2-tricks.html">
     2.2. Tricks of Optimization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="cc-3-0-ml.html">
   3.
   <strong>
    Machine Learning
   </strong>
  </a>
  <input checked class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-3-1-svm.html">
     3.1. Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-3-2-gp.html">
     3.2. Gaussian Processes
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="cc-4-0-dl.html">
   4.
   <strong>
    Deep Learning
   </strong>
  </a>
  <input checked class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-4-1-gradients.html">
     4.1. Gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-4-2-mlp.html">
     4.2. Multilayer Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-4-3-cnn.html">
     4.3. Convolutional Neural Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-4-4-rnn.html">
     4.4. Recurrent Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cc-4-5-ae.html">
     4.5. Encoder-Decoder Models
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exercise
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/1_linReg_logReg.html">
   1. Linear Regression and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/2_BayesianInference.html">
   2. Bayesian Linear and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/3_optimization.html">
   3. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/4_SVM.html">
   4. Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/5_GPs.html">
   5. Gaussian Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/6_CNNs.html">
   6. CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/7_RNNs.html">
   7. RNNs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Miscellaneous
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../admin.html">
   Admin
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../books.html">
   Books
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preliminary_knowledge.html">
   Preliminary Knowledge
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../software.html">
   Software Infrastructure
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../practical_exam.html">
   Practical Exam WS22/23
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../faq.html">
   Frequently Asked Questions
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/tumaer/SciML"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/tumaer/SciML/issues/new?title=Issue%20on%20page%20%2Flecture/cc-1-3-bayes.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/lecture/cc-1-3-bayes.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-inference">
   1.3.1. Bayesian Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo-integration">
     1.3.1.1. Monte Carlo Integration
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-point-estimation">
     1.3.1.2. Bayesian Point Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-interval-estimation">
     1.3.1.3. Bayesian Interval Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predictive-distribution-of-a-new-observation">
     1.3.1.4. Predictive Distribution of a New Observation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-inference-from-a-posterior-random-sample">
     1.3.1.5. Bayesian Inference from a Posterior Random Sample
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-approaches-to-regression">
   1.3.2. Bayesian Approaches to Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-logistic-regression">
     1.3.2.1. Bayesian Logistic Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-linear-regression">
     1.3.2.2. Bayesian Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-machine-learning">
   1.3.3. Bayesian Machine Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-references">
   1.3.4. Further References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Bayesian methods</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-inference">
   1.3.1. Bayesian Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo-integration">
     1.3.1.1. Monte Carlo Integration
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-point-estimation">
     1.3.1.2. Bayesian Point Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-interval-estimation">
     1.3.1.3. Bayesian Interval Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predictive-distribution-of-a-new-observation">
     1.3.1.4. Predictive Distribution of a New Observation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-inference-from-a-posterior-random-sample">
     1.3.1.5. Bayesian Inference from a Posterior Random Sample
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-approaches-to-regression">
   1.3.2. Bayesian Approaches to Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-logistic-regression">
     1.3.2.1. Bayesian Logistic Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-linear-regression">
     1.3.2.2. Bayesian Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-machine-learning">
   1.3.3. Bayesian Machine Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-references">
   1.3.4. Further References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-methods">
<h1><span class="section-number">1.3. </span>Bayesian methods<a class="headerlink" href="#bayesian-methods" title="Permalink to this headline">#</a></h1>
<p>We present Bayesian Inference and its applications to regression and classification.</p>
<section id="bayesian-inference">
<h2><span class="section-number">1.3.1. </span>Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Permalink to this headline">#</a></h2>
<p>The main ideals upon which Bayesian statistics is founded are</p>
<ul class="simple">
<li><p>Uncertainty over parameters -&gt; treatment as random variables</p></li>
<li><p>A probability over a parameter essentially expresses a degree of belief</p></li>
<li><p>Inference over parameters using rules of probability</p></li>
<li><p>We combine the prior knowledge and the observed data with Bayes’ theorem</p></li>
</ul>
<blockquote>
<div><p>Refresher - Bayes’ theorem is given by <span class="math notranslate nohighlight">\(\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}\)</span></p>
</div></blockquote>
<p>So what we are interested in is the <strong>posterior</strong> distribution over our parameters, which can be found using Bayes’ theorem. While this may at first glance look straightforward, and holds for the <strong>unscaled posterior</strong> i.e the distribution which has not been normalized by dividing by <span class="math notranslate nohighlight">\(\mathbb{P}(B)\)</span>, obtaining the scaled posterior is much much harder due to the difficulty in computing the divisor <span class="math notranslate nohighlight">\(\mathbb{P}(B)\)</span>. To evaluate this divisor we draw on Monte Carlo sampling.</p>
<blockquote>
<div><p>What is a <em>scaled posterior</em>? A scaled posterior is a distribution whose integral over the entire distribution evaluates to 1.</p>
</div></blockquote>
<p>Taking Bayes’ theorem, and using the probability theorems in their conditional form we then obtain the following formula for the posterior density</p>
<div class="math notranslate nohighlight" id="equation-posterior-density">
<span class="eqno">(1.63)<a class="headerlink" href="#equation-posterior-density" title="Permalink to this equation">#</a></span>\[g(\theta | y) = \frac{g(\theta) \times f(y | \theta)}{\int g(\theta) \times f(y | \theta) d\theta}\]</div>
<p>If we now seek to compute the denominator, then we have to integrate</p>
<div class="math notranslate nohighlight" id="equation-evidence">
<span class="eqno">(1.64)<a class="headerlink" href="#equation-evidence" title="Permalink to this equation">#</a></span>\[\int f(y|\theta) g(\theta) d\theta.\]</div>
<section id="monte-carlo-integration">
<h3><span class="section-number">1.3.1.1. </span>Monte Carlo Integration<a class="headerlink" href="#monte-carlo-integration" title="Permalink to this headline">#</a></h3>
<p>To approximate this quantity with Monte Carlo sampling techniques we then need to draw from the posterior <span class="math notranslate nohighlight">\(\int f(y|\theta) g(\theta) d\theta\)</span>. A process which given enough samples always converges to the true value of the denominator according to the <a class="reference external" href="http://www-star.st-and.ac.uk/~kw25/teaching/mcrt/MC_history_3.pdf">Monte Carlo theorem</a>.</p>
<p>Monte Carlo integration is a fundamental tool first developed by Physicists dealing with the solution of high-dimensional integrals. The main objective is solving integrals like</p>
<div class="math notranslate nohighlight" id="equation-mc-integral">
<span class="eqno">(1.65)<a class="headerlink" href="#equation-mc-integral" title="Permalink to this equation">#</a></span>\[E[h(\mathbf{x})]=\int h(\mathbf{x}) p(\mathbf{x}) d\mathbf{x}, \quad \mathbf{x}\in \mathbb{R}^d\]</div>
<p>with some function of interest <span class="math notranslate nohighlight">\(h\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> being a r.v.</p>
<p>The approach consists of the following three steps:</p>
<ol class="simple">
<li><p>Generate i.i.d. random samples <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\in \mathbb{R}^d, \; i=1,2,...,N\)</span> from the density <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span>.</p></li>
<li><p>Evaluate <span class="math notranslate nohighlight">\(h^{(i)}=h(\mathbf{x}^{(i)}), \; \forall i\)</span>.</p></li>
<li><p>Approximate</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-mc-sum">
<span class="eqno">(1.66)<a class="headerlink" href="#equation-mc-sum" title="Permalink to this equation">#</a></span>\[E[h(\mathbf{x})]\approx \frac{1}{N}\sum_{i=1}^{N}h^{(i)}\]</div>
<hr class="docutils" />
<p>Bayesian approaches based on random Monte Carlo sampling from the posterior have a number of advantages for us:</p>
<ul class="simple">
<li><p>Given a large enough number of samples, we are not working with an approximation, but with an estimate which can be made as precise as desired (given the requisite computational budget)</p></li>
<li><p>Sensitivity analysis of the model becomes easier.</p></li>
</ul>
<hr class="docutils" />
<p>In the Bayesian framework, everything centers around the posterior distribution and our ability to relate our previous knowledge with newly gained evidence to the next stage of our belief (of a probability distribution). With the posterior being our entire inference about the parameters given the data there exist multiple inference approaches with their roots in frequentist statistics.</p>
</section>
<section id="bayesian-point-estimation">
<h3><span class="section-number">1.3.1.2. </span>Bayesian Point Estimation<a class="headerlink" href="#bayesian-point-estimation" title="Permalink to this headline">#</a></h3>
<p>Bayesian point estimation chooses a single value to represent the entire posterior distribution. Potential choices here are locations like the posterior mean, and posterior median. For the posterior mean squared error, the posterior mean is then the first moment of the posterior distribution</p>
<div class="math notranslate nohighlight">
\[PMS(\hat{\theta}) = \int (\theta - \hat{\theta})^{2} g(\theta | y_{1}, \ldots, y_{n})d\theta\]</div>
<div class="math notranslate nohighlight" id="equation-mps-error">
<span class="eqno">(1.67)<a class="headerlink" href="#equation-mps-error" title="Permalink to this equation">#</a></span>\[\hat{\theta} = \int_{-\infty}^{\infty} \theta g(\theta | y_{1}, \ldots, y_{n})d \theta\]</div>
<figure class="align-center" id="posterior-mean">
<a class="reference internal image-reference" href="../_images/posterior_mean.png"><img alt="../_images/posterior_mean.png" src="../_images/posterior_mean.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.9 </span><span class="caption-text">Posterior mean (Source: <span id="id1">[<a class="reference internal" href="../references.html#id8" title="William M. Bolstad. Understanding Computational Bayesian Statistics. Wiley, 2009. ISBN 978-0-470-04609-8. URL: https://onlinelibrary.wiley.com/doi/book/10.1002/9780470567371.">Bolstad, 2009</a>]</span>, Chapter 3).</span><a class="headerlink" href="#posterior-mean" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>and for the posterior median <span class="math notranslate nohighlight">\(\tilde{\theta}\)</span></p>
<div class="math notranslate nohighlight">
\[PMAD(\hat{\theta}) = \int |\theta - \hat{\theta}| g(\theta| y_{1}, \ldots, y_{n})d\theta\]</div>
<div class="math notranslate nohighlight" id="equation-pmad">
<span class="eqno">(1.68)<a class="headerlink" href="#equation-pmad" title="Permalink to this equation">#</a></span>\[.5 = \int_{-\infty}^{\tilde{\theta}} g(\theta | y_{1}, \ldots, y_{n}) d\theta\]</div>
<figure class="align-center" id="posterior-median">
<a class="reference internal image-reference" href="../_images/posterior_median.png"><img alt="../_images/posterior_median.png" src="../_images/posterior_median.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.10 </span><span class="caption-text">Posterior median (Source: <span id="id2">[<a class="reference internal" href="../references.html#id8" title="William M. Bolstad. Understanding Computational Bayesian Statistics. Wiley, 2009. ISBN 978-0-470-04609-8. URL: https://onlinelibrary.wiley.com/doi/book/10.1002/9780470567371.">Bolstad, 2009</a>]</span>, Chapter 3).</span><a class="headerlink" href="#posterior-median" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="bayesian-interval-estimation">
<h3><span class="section-number">1.3.1.3. </span>Bayesian Interval Estimation<a class="headerlink" href="#bayesian-interval-estimation" title="Permalink to this headline">#</a></h3>
<p>Another type of Bayesian inference is the one in which we seek to find an interval that, with a pre-determined probability, contains the true value. In Bayesian statistics, these are called credible intervals. Finding the interval with equal tail areas to both sides <span class="math notranslate nohighlight">\((\theta_{l}, \theta_{u})\)</span>, and which has the probability to contain the true value of the parameter, i.e.</p>
<div class="math notranslate nohighlight" id="equation-confidence-intervals">
<span class="eqno">(1.69)<a class="headerlink" href="#equation-confidence-intervals" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\int_{-\infty}^{\theta_{l}} g(\theta | y_{1}, \ldots, y_{n}) d\theta &amp;= \frac{\alpha}{2} \\
\int_{\theta_{u}}^{\infty} g(\theta | y_{1}, \ldots, y_{n}) d\theta &amp;= \frac{\alpha}{2}
\end{aligned}
\end{split}\]</div>
<p>which we then only need to solve. A visual example of such a scenario is in the following picture:</p>
<figure class="align-center" id="credible-intervals">
<a class="reference internal image-reference" href="../_images/credible_interval.png"><img alt="../_images/credible_interval.png" src="../_images/credible_interval.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.11 </span><span class="caption-text">The 95% credible interval (Source: <span id="id3">[<a class="reference internal" href="../references.html#id8" title="William M. Bolstad. Understanding Computational Bayesian Statistics. Wiley, 2009. ISBN 978-0-470-04609-8. URL: https://onlinelibrary.wiley.com/doi/book/10.1002/9780470567371.">Bolstad, 2009</a>]</span>, Chapter 3).</span><a class="headerlink" href="#credible-intervals" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!---
#### Maximum A Posteriori Estimation (MAP)

Blubba blub
--->
</section>
<section id="predictive-distribution-of-a-new-observation">
<h3><span class="section-number">1.3.1.4. </span>Predictive Distribution of a New Observation<a class="headerlink" href="#predictive-distribution-of-a-new-observation" title="Permalink to this headline">#</a></h3>
<p>If we obtain a new observation, then we can compute the updated predictive distribution by combining the conditional distribution of the new observation, and conditioning it on the previous observations. Then we only need to integrate the parameter out of the joint posterior</p>
<div class="math notranslate nohighlight">
\[f(y_{n+1}|y_{1}, \ldots, y_{n}) \propto \int g(\theta) \times f(y_{n+1}| \theta) \times \ldots \times f(y_{1}|\theta) d\theta\]</div>
<div class="math notranslate nohighlight" id="equation-predictive-posterior-verbose">
<span class="eqno">(1.70)<a class="headerlink" href="#equation-predictive-posterior-verbose" title="Permalink to this equation">#</a></span>\[\propto \int f(y_{n+1}|\theta)g(\theta|y_{1}, \ldots, y_{n}) d\theta\]</div>
<p>and marginalize it out.</p>
</section>
<section id="bayesian-inference-from-a-posterior-random-sample">
<h3><span class="section-number">1.3.1.5. </span>Bayesian Inference from a Posterior Random Sample<a class="headerlink" href="#bayesian-inference-from-a-posterior-random-sample" title="Permalink to this headline">#</a></h3>
<p>When we only have a random sample from the posterior instead of the approximation, we are still able to apply the same techniques, but just apply them to the posterior sample.</p>
<p>With our rudimentary approximations of the denominator by sampling from the posterior. This only constitutes an approximation, but given the sampling budget, this approximation can be made as accurate as desired. In summary Bayesian inference can be condensed to the following main take-home knowledge:</p>
<ul class="simple">
<li><p>The posterior distribution is the current summary of beliefs about the parameter in the Bayesian framework.</p></li>
<li><p>Bayesian inference is then performed using the probabilities calculated from the posterior distribution of the parameter.</p>
<ul>
<li><p>To get an approximation of the scaling factor for the posterior we have to utilize sampling-based Monte Carlo techniques to approximate the requisite integral.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="bayesian-approaches-to-regression">
<h2><span class="section-number">1.3.2. </span>Bayesian Approaches to Regression<a class="headerlink" href="#bayesian-approaches-to-regression" title="Permalink to this headline">#</a></h2>
<p>If we are faced with the scenario of having very little data, then we ideally seek to quantify the uncertainty of our model and preserve the predictive utility of our machine learning model. The right approach to this is to extend Linear Regression, and Logistic Regression with the just presented Bayesian Approach utilizing Bayesian Inference.</p>
<section id="bayesian-logistic-regression">
<h3><span class="section-number">1.3.2.1. </span>Bayesian Logistic Regression<a class="headerlink" href="#bayesian-logistic-regression" title="Permalink to this headline">#</a></h3>
<p>If we now want to capture the uncertainty over our predictions in our logistic regression, then we have to resort to the Bayesian approach. To make the Bayesian approach work for logistic regression, we have to apply something called the <em>Laplace Approximation</em> in which we approximate the posterior using a Gaussian</p>
<div class="math notranslate nohighlight" id="equation-laplace-approx">
<span class="eqno">(1.71)<a class="headerlink" href="#equation-laplace-approx" title="Permalink to this equation">#</a></span>\[p(\omega | \mathcal{D}) \approx \mathcal{N}({\bf{\omega}}| {\bf{\hat{\omega}}}, {\bf{H}}^{-1})\]</div>
<p>where <span class="math notranslate nohighlight">\(H^{-1}\)</span> is the inverse of the Hessian, <span class="math notranslate nohighlight">\(\omega\)</span> corresponds to the learned parameters <span class="math notranslate nohighlight">\(\vartheta\)</span>, and <span class="math notranslate nohighlight">\(\hat{\omega}\)</span> is the MLE of <span class="math notranslate nohighlight">\(\vartheta\)</span>. There exist many different modes representing viable solutions for this problem when we seek to optimize it.</p>
<figure class="align-center" id="bayesian-log-reg-data">
<a class="reference internal image-reference" href="../_images/bayesian_log_reg_data.png"><img alt="../_images/bayesian_log_reg_data.png" src="../_images/bayesian_log_reg_data.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.12 </span><span class="caption-text">Bayesian logistic regression data (Source: <span id="id4">[<a class="reference internal" href="../references.html#id4" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: http://probml.github.io/book1.">Murphy, 2022</a>]</span>, Chapter 10).</span><a class="headerlink" href="#bayesian-log-reg-data" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Using a Gaussian prior centered at the origin, we can then multiply our prior with the likelihood to obtain the unnormalized posterior. Which yields us the posterior predictive distribution</p>
<div class="math notranslate nohighlight" id="equation-predictive-posterior">
<span class="eqno">(1.72)<a class="headerlink" href="#equation-predictive-posterior" title="Permalink to this equation">#</a></span>\[p(y|x, \mathcal{D}) = \int p(y | x, \omega) p(\omega | \mathcal{D})) d\omega\]</div>
<p>To now compute the uncertainty in our predictions we use a Gaussian prior, and then perform a <em>Monte Carlo Approximation</em> of the integral using <span class="math notranslate nohighlight">\(S\)</span> samples from the posterior <span class="math notranslate nohighlight">\(\omega_s \sim p(\omega|\mathcal{D})\)</span></p>
<div class="math notranslate nohighlight" id="equation-blr-mc">
<span class="eqno">(1.73)<a class="headerlink" href="#equation-blr-mc" title="Permalink to this equation">#</a></span>\[p(y=1 | x, \mathcal{D}) = \frac{1}{S} \sum_{s=1}^{S} \sigma \left( \omega_{s}^{\top} x \right)\]</div>
<p>Looking at a larger visual example of Bayesian Logistic Regression applied.</p>
<figure class="align-center" id="bayesian-log-reg">
<a class="reference internal image-reference" href="../_images/bayesian_log_reg.jpg"><img alt="../_images/bayesian_log_reg.jpg" src="../_images/bayesian_log_reg.jpg" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.13 </span><span class="caption-text">Bayesian logistic regression (Source: <span id="id5">[<a class="reference internal" href="../references.html#id4" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: http://probml.github.io/book1.">Murphy, 2022</a>]</span>, Chapter 10).</span><a class="headerlink" href="#bayesian-log-reg" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="bayesian-linear-regression">
<h3><span class="section-number">1.3.2.2. </span>Bayesian Linear Regression<a class="headerlink" href="#bayesian-linear-regression" title="Permalink to this headline">#</a></h3>
<p>To now introduce the Bayesian approach to linear regression we have to assume that we already know the variance <span class="math notranslate nohighlight">\(\sigma^{2}\)</span>, so the posterior which we actually compute at that point is</p>
<div class="math notranslate nohighlight" id="equation-blr-formulation">
<span class="eqno">(1.74)<a class="headerlink" href="#equation-blr-formulation" title="Permalink to this equation">#</a></span>\[p(\omega | \mathcal{D}, \sigma^{2})\]</div>
<p>If we then take a Gaussian distribution as our prior distribution <span class="math notranslate nohighlight">\(p(\omega)\)</span></p>
<div class="math notranslate nohighlight" id="equation-blr-normal-prior">
<span class="eqno">(1.75)<a class="headerlink" href="#equation-blr-normal-prior" title="Permalink to this equation">#</a></span>\[p(\omega) = \mathcal{N}(\omega | \breve{\omega}, \breve{\Sigma})\]</div>
<p>Then we can write down the likelihood as a Multivariate-Normal distribution.</p>
<div class="math notranslate nohighlight" id="equation-blr-likelihood">
<span class="eqno">(1.76)<a class="headerlink" href="#equation-blr-likelihood" title="Permalink to this equation">#</a></span>\[p(\mathcal{D} | \omega, \sigma^{2}) = \prod_{n=1}^{N}p(y_{n}|{\bf{\omega^{\top}}}{\bf{x}}, \sigma^{2}) = \mathcal{N}({\bf{y}} | {\bf{X} \bf{\omega}}, \sigma^{2} {\bf{I}}_{N})\]</div>
<p>The posterior can then be analytically derived using Bayes’ rule for Gaussians (see <span id="id6">[<a class="reference internal" href="../references.html#id4" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: http://probml.github.io/book1.">Murphy, 2022</a>]</span>, Eq. 3.37)</p>
<div class="math notranslate nohighlight" id="equation-blr-posterior">
<span class="eqno">(1.77)<a class="headerlink" href="#equation-blr-posterior" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
p({\bf{\omega}} | {\bf{X}}, {\bf{y}}, \sigma^{2}) &amp;\propto \mathcal{N}(\omega | \breve{\omega}, \breve{\Sigma}) \mathcal{N}({\bf{y}} | {\bf{X} \bf{\omega}}, \sigma^{2} {\bf{I}}_{N}) = \mathcal{N}({\bf{\omega}} | {\bf{\hat{\omega}}}, {\bf{\hat{\Sigma}}}) \\
{\bf{\hat{\omega}}} &amp;\equiv {\bf{\hat{\Sigma}}} \left( {\bf{\breve{\Sigma}}}^{-1} {\bf{\breve{\omega}}} + \frac{1}{\sigma^{2}} {\bf{X^{\top} y}}  \right) \\
{\bf{\hat{\Sigma}}} &amp;\equiv \left( {\bf{\breve{\Sigma}}}^{-1} + \frac{1}{\sigma^{2}} {\bf{X^{\top} X}} \right)^{-1}
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\omega}\)</span> is the posterior mean, and <span class="math notranslate nohighlight">\(\hat{\Sigma}\)</span> is the posterior covariance. A good visual example of this is the sequential Bayesian inference on a linear regression model:</p>
<figure class="align-center" id="bayesian-lin-reg">
<a class="reference internal image-reference" href="../_images/bayesian_lin_reg.png"><img alt="../_images/bayesian_lin_reg.png" src="../_images/bayesian_lin_reg.png" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.14 </span><span class="caption-text">Bayesian linear regression (Source: <span id="id7">[<a class="reference internal" href="../references.html#id4" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: http://probml.github.io/book1.">Murphy, 2022</a>]</span>, Chapter 11).</span><a class="headerlink" href="#bayesian-lin-reg" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="bayesian-machine-learning">
<h2><span class="section-number">1.3.3. </span>Bayesian Machine Learning<a class="headerlink" href="#bayesian-machine-learning" title="Permalink to this headline">#</a></h2>
<p>Let’s consider the setup we have encountered so far in which we have labels <span class="math notranslate nohighlight">\(x\)</span>, hyperparameters <span class="math notranslate nohighlight">\(\theta\)</span>, and seek to predict labels <span class="math notranslate nohighlight">\(y\)</span>. Probabilistically expressed this amounts to <span class="math notranslate nohighlight">\(p(y|x, \theta)\)</span>. Then the posterior is defined as <span class="math notranslate nohighlight">\(p(\theta| \mathcal{D})\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is our labeled dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \left\{ (x_{n}, y_{n}) \right\}_{n=1:N}\)</span>.</p>
<p>Applying the previously discussed Bayesian approaches to these problems, and the respective model parameters, are called <strong>Bayesian Machine Learning</strong>.</p>
<p>While we lose computational efficiency at first glance, as we have to perform a sampling-based inference procedure, what we gain is a principled approach to discuss uncertainties within our model. This can help us most especially when we move in the <em>small-data limit</em>, where we can not realistically expect our model to converge. See e.g. below a Bayesian logistic regression example in which the posterior distribution is visualized.</p>
<figure class="align-center" id="bayesian-nn">
<a class="reference internal image-reference" href="../_images/bayesian_nn.png"><img alt="../_images/bayesian_nn.png" src="../_images/bayesian_nn.png" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.15 </span><span class="caption-text">Bayesian machine learning (Source: <span id="id8">[<a class="reference internal" href="../references.html#id4" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: http://probml.github.io/book1.">Murphy, 2022</a>]</span>, Chapter 4).</span><a class="headerlink" href="#bayesian-nn" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="further-references">
<h2><span class="section-number">1.3.4. </span>Further References<a class="headerlink" href="#further-references" title="Permalink to this headline">#</a></h2>
<p><strong>Bayesian Methods</strong>
There exist a wide number of references to the herein presented Bayesian approach, most famously introductory treatment of Probabilistic Programming frameworks, which utilize the herein presented modeling approach to obtain posteriors over programs.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://pyro.ai/examples/intro_long.html">Introduction to Pyro</a></p></li>
<li><p><a class="reference external" href="https://m-clark.github.io/bayesian-basics/example.html#posterior-predictive">A Practical Example with Stan</a></p></li>
</ul>
<p>In addition, there exists highly curated didactic material from Michael Betancourt:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://betanalpha.github.io/assets/case_studies/sampling.html">Sampling</a>: Section 3, 4, and 5</p></li>
<li><p><a class="reference external" href="https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html">Towards a Principled Bayesian Workflow</a></p></li>
<li><p><a class="reference external" href="https://betanalpha.github.io/assets/case_studies/markov_chain_monte_carlo.html">Markov Chain Monte Carlo</a>: Section 1, 2, and 3</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="cc-1-2-gmm-mcmc.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">1.2. </span>GMM and Sampling</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="cc-2-0-optim.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2. </span><strong>Optimization</strong></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By N. Adams, L. Paehler, A. Toshev<br/>
  
      &copy; Copyright 2022,2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>