

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>7. Gaussian Processes &#8212; Introduction to Scientific Machine Learning for Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture/gp';</script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8. Gradients" href="gradients.html" />
    <link rel="prev" title="6. Support Vector Machines" href="svm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../about.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Scientific Machine Learning for Engineers - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Scientific Machine Learning for Engineers - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="linear.html">1. Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="gmm.html">2. Gaussian Mixture Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes.html">3. Bayesian methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">4. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tricks.html">5. Tricks of Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svm.html">6. Support Vector Machines</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">7. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients.html">8. Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlp.html">9. Multilayer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">10. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html">11. Recurrent Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="ae.html">12. Encoder-Decoder Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercise</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/linear.html">1. Linear and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/bayes.html">2. Bayesian Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/optimization.html">3. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/svm.html">4. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/gp.html">5. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/cnn.html">6. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/rnn.html">7. Recurrent Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../admin.html">Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preliminary_knowledge.html">Preliminary Knowledge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software.html">Software Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../practical_exam.html">Practical Exam WS22/23</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tumaer/SciML" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tumaer/SciML/issues/new?title=Issue%20on%20page%20%2Flecture/gp.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lecture/gp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gaussian Processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian-distribution">7.1. Multivariate Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#st-and-2nd-moment">7.1.1. 1st and 2nd Moment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-gaussian-pdf">7.1.2. Conditional Gaussian PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-gaussian-pdf">7.1.3. Marginal Gaussian PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-for-gaussian-variables">7.1.4. Bayes Theorem for Gaussian Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-for-gaussians">7.1.5. Maximum Likelihood for Gaussians</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-process-gp">7.2. The Gaussian Process (GP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gp-for-regression">7.3. GP for Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-kernel-configurations">7.3.1. Further Kernel Configurations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-prediction">7.3.2. Multivariate Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notes-on-gaussian-process-regression">7.3.3. Notes on Gaussian Process Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-the-hyperparameters">7.3.4. Learning the Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gp-for-classification">7.4. GP for Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relation-of-gp-to-neural-networks">7.5. Relation of GP to Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">7.6. Further References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gaussian-processes">
<h1><span class="section-number">7. </span>Gaussian Processes<a class="headerlink" href="#gaussian-processes" title="Permalink to this heading">#</a></h1>
<p>As the main mathematical construct behind Gaussian Processes, we first introduce the Multivariate Gaussian distribution. We analyze this distribution in some more detail to provide reference results. For a more detailed derivation of the results, refer to <span id="id1">[<a class="reference internal" href="../references.html#id2" title="Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. ISBN 978-0-387-31073-2. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span>, Section 2.3. In the rest of this lecture we define the Gaussian Process and how in can be applied to regression and classification.</p>
<section id="multivariate-gaussian-distribution">
<h2><span class="section-number">7.1. </span>Multivariate Gaussian Distribution<a class="headerlink" href="#multivariate-gaussian-distribution" title="Permalink to this heading">#</a></h2>
<p>The <strong>univariate</strong> (for a scalar random variable) Gaussian distribution has the form</p>
<div class="math notranslate nohighlight" id="equation-univariate-gaussian">
<span class="eqno">(7.1)<a class="headerlink" href="#equation-univariate-gaussian" title="Permalink to this equation">#</a></span>\[\mathcal{N}(x; \underbrace{\mu, \sigma^2}_{\text{parameters}}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{ - \frac{(x-\mu)^2}{2 \sigma^2}\right\}.\]</div>
<p>The two parameters are the mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>The <strong>multivariate</strong> Gaussian distribution then has the following form</p>
<div class="math notranslate nohighlight" id="equation-multivariate-gaussian-duplicate">
<span class="eqno">(7.2)<a class="headerlink" href="#equation-multivariate-gaussian-duplicate" title="Permalink to this equation">#</a></span>\[\mathcal{N}(x; \mu, \Sigma)= \frac{1}{(2\pi)^{d/2}\sqrt{\det (\Sigma)}} \exp \left(-\frac{1}{2}(x-\mu)^{\top}\Sigma^{-1}(x-\mu)\right),\]</div>
<p>with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x \in \mathbb{R}^d \quad \)</span> - feature / sample / random vector</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu \in \mathbb{R}^d \quad \)</span> - mean vector</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma \in \mathbb{R}^{d \times d} \quad\)</span> - covariance matrix</p></li>
</ul>
<p>Properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Delta^2=(x-\mu)^{\top}\Sigma^{-1}(x-\mu)\)</span> is a quadratic form.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta\)</span> is the Mahalanobis distance from <span class="math notranslate nohighlight">\(\mu\)</span> to <span class="math notranslate nohighlight">\(x\)</span>. It collapses to the Euclidean distance for <span class="math notranslate nohighlight">\(\Sigma = I\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> is symmetric positive semi-definite and its diagonal elements contain the variance, i.e. covariance with itself.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma\)</span> can be diagonalized with real eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-sigma-eigendecomposition">
<span class="eqno">(7.3)<a class="headerlink" href="#equation-sigma-eigendecomposition" title="Permalink to this equation">#</a></span>\[\Sigma u_i = \lambda_i u_i \quad i=1,...,d\]</div>
<p>and eigenvectors <span class="math notranslate nohighlight">\(u_i\)</span> forming a unitary matrix</p>
<div class="math notranslate nohighlight" id="equation-sigma-eigenvectors-matrix">
<span class="eqno">(7.4)<a class="headerlink" href="#equation-sigma-eigenvectors-matrix" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
    &amp; U= \left[\begin{array}{l} u_1^{\top} \\ ... \\ u_d^{\top}\\ \end{array}\right], \\
    &amp; U^{\top}U = U U^{\top}=I, \\
\end{aligned}
\end{split}\]</div>
<p>which fulfils</p>
<div class="math notranslate nohighlight" id="equation-sigma-eigendecomposition-matrix">
<span class="eqno">(7.5)<a class="headerlink" href="#equation-sigma-eigendecomposition-matrix" title="Permalink to this equation">#</a></span>\[U\Sigma U^{\top} = \lambda,\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> denotes a diagonal matrix of eigenvalues. If we apply the variable transformation <span class="math notranslate nohighlight">\(y=U(x-\mu)\)</span>, we can transform the Gaussian PDF to the <span class="math notranslate nohighlight">\(y\)</span> coordinates according to the change of variables rule (see beginning of <a class="reference internal" href="gmm.html"><span class="doc std std-doc">Gaussian Mixture Models</span></a>).</p>
<div class="math notranslate nohighlight" id="equation-change-of-vars-duplicate">
<span class="eqno">(7.6)<a class="headerlink" href="#equation-change-of-vars-duplicate" title="Permalink to this equation">#</a></span>\[p_Y(y)=p_X(h^{-1}(y)) \underbrace{\left|\frac{\text{d}h^{-1}(y)}{\text{d}y}\right|}_{(det|U_{ij}|)^{-1}=1}\]</div>
<p>which leads to</p>
<div class="math notranslate nohighlight">
\[p(y) = \Pi_{i=1}^d \frac{1}{\sqrt{2 \pi \lambda_i}} \exp \left\{ - \frac{y_i^2}{2\lambda_i} \right\}.\]</div>
<p>Note that the diagonalization of <span class="math notranslate nohighlight">\(\Delta\)</span> leads to a factorization of the PDF into <span class="math notranslate nohighlight">\(d\)</span> 1D PDFs.</p>
<section id="st-and-2nd-moment">
<h3><span class="section-number">7.1.1. </span>1st and 2nd Moment<a class="headerlink" href="#st-and-2nd-moment" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight" id="equation-def-first-moment">
<span class="eqno">(7.7)<a class="headerlink" href="#equation-def-first-moment" title="Permalink to this equation">#</a></span>\[E[x] = \mu\]</div>
<div class="math notranslate nohighlight" id="equation-def-second-moment">
<span class="eqno">(7.8)<a class="headerlink" href="#equation-def-second-moment" title="Permalink to this equation">#</a></span>\[E[xx^{\top}] = \mu \mu^{\top} + \Sigma\]</div>
<div class="math notranslate nohighlight" id="equation-def-cov">
<span class="eqno">(7.9)<a class="headerlink" href="#equation-def-cov" title="Permalink to this equation">#</a></span>\[Cov(x) = \Sigma\]</div>
</section>
<section id="conditional-gaussian-pdf">
<h3><span class="section-number">7.1.2. </span>Conditional Gaussian PDF<a class="headerlink" href="#conditional-gaussian-pdf" title="Permalink to this heading">#</a></h3>
<p>Consider the case <span class="math notranslate nohighlight">\(x \sim \mathcal{N}(x; \mu, \Sigma)\)</span> being a <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian random vector. We can partition <span class="math notranslate nohighlight">\(x\)</span> into two disjoint subsets <span class="math notranslate nohighlight">\(x_a\)</span> and <span class="math notranslate nohighlight">\(x_b\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-rv-partition">
<span class="eqno">(7.10)<a class="headerlink" href="#equation-rv-partition" title="Permalink to this equation">#</a></span>\[\begin{split}
x=\left(\begin{array}{c}
x_a \\
x_b
\end{array}\right) .
\end{split}\]</div>
<p>The corresponding partitions of the mean vector <span class="math notranslate nohighlight">\(\mu\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> become</p>
<div class="math notranslate nohighlight" id="equation-mean-partition">
<span class="eqno">(7.11)<a class="headerlink" href="#equation-mean-partition" title="Permalink to this equation">#</a></span>\[\begin{split}
\mu=\left(\begin{array}{l}
\mu_a \\
\mu_b
\end{array}\right)
\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-cov-partition">
<span class="eqno">(7.12)<a class="headerlink" href="#equation-cov-partition" title="Permalink to this equation">#</a></span>\[\begin{split}
\Sigma=\left(\begin{array}{ll}
\Sigma_{a a} &amp; \Sigma_{a b} \\
\Sigma_{b a} &amp; \Sigma_{b b}
\end{array}\right) .
\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\Sigma^T=\Sigma\)</span> implies that <span class="math notranslate nohighlight">\(\Sigma_{a a}\)</span> and <span class="math notranslate nohighlight">\(\Sigma_{b b}\)</span> are symmetric, while <span class="math notranslate nohighlight">\(\Sigma_{b a}=\Sigma_{a b}^{\mathrm{T}}\)</span>.</p>
<p>We also define the precision matrix <span class="math notranslate nohighlight">\(\Lambda = \Sigma^{-1}\)</span>, being the inverse of the covariance matrix, where</p>
<div class="math notranslate nohighlight" id="equation-precision-partition">
<span class="eqno">(7.13)<a class="headerlink" href="#equation-precision-partition" title="Permalink to this equation">#</a></span>\[\begin{split}
\Lambda=\left(\begin{array}{ll}
\Lambda_{a a} &amp; \Lambda_{a b} \\
\Lambda_{b a} &amp; \Lambda_{b b}
\end{array}\right)
\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(\Lambda_{a a}\)</span> and <span class="math notranslate nohighlight">\(\Lambda_{b b}\)</span> are symmetric, while <span class="math notranslate nohighlight">\(\Lambda_{a b}^{\mathrm{T}}=\Lambda_{b a}\)</span>. Note, <span class="math notranslate nohighlight">\(\Lambda_{a a}\)</span> is not simply the inverse of <span class="math notranslate nohighlight">\(\Sigma_{a a}\)</span>.</p>
<p>Now, we want to evaluate <span class="math notranslate nohighlight">\(\mathcal{N}(x_a| x_b; \mu, \Sigma)\)</span> and use <span class="math notranslate nohighlight">\(p(x_a, x_b) = p(x_a|x_b)p(x_b)\)</span>. We expand all terms of the pdf given the split, and consider all terms that do not involve <span class="math notranslate nohighlight">\(x_a\)</span> as constant and then we compare with the generic form of a Gaussian for <span class="math notranslate nohighlight">\(p(x_a| x_b)\)</span>. We can decompose the equation into quadratic, linear and constant terms in <span class="math notranslate nohighlight">\(x_a\)</span> and find an expression for <span class="math notranslate nohighlight">\(p(x_a|x_b)\)</span>.</p>
<blockquote>
<div><p>For all intermediate steps refer to <span id="id2">[<a class="reference internal" href="../references.html#id2" title="Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. ISBN 978-0-387-31073-2. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span>.</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-cond-pdf-stats">
<span class="eqno">(7.14)<a class="headerlink" href="#equation-cond-pdf-stats" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mu_{a \mid b} &amp; =\mu_a+\Sigma_{a b} \Sigma_{b b}^{-1}\left(x_b-\mu_b\right) \\
\Sigma_{a \mid b} &amp; =\Sigma_{a a}-\Sigma_{a b} \Sigma_{b b}^{-1} \Sigma_{b a}
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-gaussian-cond-pdf">
<span class="eqno">(7.15)<a class="headerlink" href="#equation-gaussian-cond-pdf" title="Permalink to this equation">#</a></span>\[p(x_a| x_b) = \mathcal{N(x; \mu_{a|b}, \Sigma_{a|b})}\]</div>
</section>
<section id="marginal-gaussian-pdf">
<h3><span class="section-number">7.1.3. </span>Marginal Gaussian PDF<a class="headerlink" href="#marginal-gaussian-pdf" title="Permalink to this heading">#</a></h3>
<p>For the marginal PDF we integrate out the dependence on <span class="math notranslate nohighlight">\(x_b\)</span> of the joint PDF:</p>
<div class="math notranslate nohighlight" id="equation-def-marginal-pdf">
<span class="eqno">(7.16)<a class="headerlink" href="#equation-def-marginal-pdf" title="Permalink to this equation">#</a></span>\[p(x_a) = \int p(x_a, x_b) dx_b.\]</div>
<p>We can follow similar steps as above for separating terms that involve <span class="math notranslate nohighlight">\(x_a\)</span> and <span class="math notranslate nohighlight">\(x_b\)</span>. After integrating out the Gaussian with a quadratic term depending on <span class="math notranslate nohighlight">\(x_b\)</span> we are left with a lengthy term involving <span class="math notranslate nohighlight">\(x_a\)</span> only. By comparison with a Gaussian PDF and re-using the block relation between <span class="math notranslate nohighlight">\(\Lambda\)</span> and <span class="math notranslate nohighlight">\(\Sigma\)</span> as above we obtain for the marginal</p>
<div class="math notranslate nohighlight" id="equation-gaussian-marginal-pdf">
<span class="eqno">(7.17)<a class="headerlink" href="#equation-gaussian-marginal-pdf" title="Permalink to this equation">#</a></span>\[p(x_a) = \mathcal{N}(x_a; \mu_a, \Sigma_{a a}).\]</div>
</section>
<section id="bayes-theorem-for-gaussian-variables">
<h3><span class="section-number">7.1.4. </span>Bayes Theorem for Gaussian Variables<a class="headerlink" href="#bayes-theorem-for-gaussian-variables" title="Permalink to this heading">#</a></h3>
<p>Generative learning addresses the problem of finding a posterior PDF from a likelihood and prior. The basis is Bayes rule for conditional probabilities</p>
<div class="math notranslate nohighlight" id="equation-def-bayes-rule">
<span class="eqno">(7.18)<a class="headerlink" href="#equation-def-bayes-rule" title="Permalink to this equation">#</a></span>\[p(x|y) = \frac{p(y|x)p(x)}{p(y)}.\]</div>
<p>We want to find the posterior <span class="math notranslate nohighlight">\(p(x|y)\)</span> and the evidence <span class="math notranslate nohighlight">\(p(y)\)</span> under the assumption that the likelihood <span class="math notranslate nohighlight">\(p(y|x)\)</span> and the prior are <strong>linear Gaussian models</strong>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(y|x)\)</span> is Gaussian and has a mean that depends at most linearly on <span class="math notranslate nohighlight">\(x\)</span> and a variance that is independent of <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(p(x)\)</span> is Gaussian.</p></li>
</ul>
<p>These requirements correspond to the following structure of <span class="math notranslate nohighlight">\(p(x)\)</span> and <span class="math notranslate nohighlight">\(p(y|x)\)</span></p>
<div class="math notranslate nohighlight" id="equation-gaussian-prior-and-likelihood">
<span class="eqno">(7.19)<a class="headerlink" href="#equation-gaussian-prior-and-likelihood" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
p(x) &amp; =\mathcal{N}\left(x \mid \mu, \Lambda^{-1}\right) \\
p(y \mid x) &amp; =\mathcal{N}\left(y \mid A x+b, L^{-1}\right).
\end{aligned}
\end{split}\]</div>
<p>From that we can derive an analytical evidence (marginal) and posterior (conditional) distributions (for more details see <span id="id3">[<a class="reference internal" href="../references.html#id2" title="Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. ISBN 978-0-387-31073-2. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span>):</p>
<div class="math notranslate nohighlight" id="equation-gaussian-evidence-and-posterior">
<span class="eqno">(7.20)<a class="headerlink" href="#equation-gaussian-evidence-and-posterior" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
p(y) &amp; =\mathcal{N}\left(y \mid A \mu+b, L^{-1}+A \Lambda^{-1} A^{\top}\right) \\
p(x \mid y) &amp; =\mathcal{N}\left(x \mid \Sigma\left\{A^{\top} L(y-b)+\Lambda \mu\right\}, \Sigma\right),
\end{aligned}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-gaussian-posterior-sigma">
<span class="eqno">(7.21)<a class="headerlink" href="#equation-gaussian-posterior-sigma" title="Permalink to this equation">#</a></span>\[
\Sigma=\left(\Lambda+A^{\top} L A\right)^{-1} .
\]</div>
</section>
<section id="maximum-likelihood-for-gaussians">
<h3><span class="section-number">7.1.5. </span>Maximum Likelihood for Gaussians<a class="headerlink" href="#maximum-likelihood-for-gaussians" title="Permalink to this heading">#</a></h3>
<p>In generative learning, we need to infer PDFs from data. Given a dataset <span class="math notranslate nohighlight">\(X=(x_1, ..., x_N)\)</span>, where <span class="math notranslate nohighlight">\(x_i\)</span> are i.i.d. random variables drawn from a multivariate Gaussian, we can estimate <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\Sigma\)</span> from the maximum likelihood (ML) (for more details see <a class="reference external" href="https://link.springer.com/book/9780387310732">Bishop, 2006</a>):</p>
<div class="math notranslate nohighlight" id="equation-ml-mean">
<span class="eqno">(7.22)<a class="headerlink" href="#equation-ml-mean" title="Permalink to this equation">#</a></span>\[\mu_{ML} = \frac{1}{N} \sum_{n=1}^N x_n\]</div>
<div class="math notranslate nohighlight" id="equation-ml-cov">
<span class="eqno">(7.23)<a class="headerlink" href="#equation-ml-cov" title="Permalink to this equation">#</a></span>\[\Sigma_{ML} = \frac{1}{N} \sum_{n=1}^N (x-\mu)^{\top}(x-\mu)\]</div>
<p><span class="math notranslate nohighlight">\(\mu_{ML}\)</span> and <span class="math notranslate nohighlight">\(\Sigma_{ML}\)</span> correspond to the so-called sample or empirical estimates. However, <span class="math notranslate nohighlight">\(\Sigma_{ML}\)</span> does not deliver an unbiased estimate of the covariance. The difference decreases with <span class="math notranslate nohighlight">\(N \to \infty\)</span>, but for a certain <span class="math notranslate nohighlight">\(N\)</span> we have <span class="math notranslate nohighlight">\(\Sigma_{ML}\neq \Sigma\)</span>. The practical reason used in the above derivation is that the <span class="math notranslate nohighlight">\(\mu_{ML}\)</span> estimate may occur as <span class="math notranslate nohighlight">\(\bar{x}\)</span> within the sampling of <span class="math notranslate nohighlight">\(\Sigma\)</span> <span class="math notranslate nohighlight">\(\Rightarrow\)</span> miscount by one. An unbiased sample variance can be defined as</p>
<div class="math notranslate nohighlight" id="equation-ml-cov-unbiased">
<span class="eqno">(7.24)<a class="headerlink" href="#equation-ml-cov-unbiased" title="Permalink to this equation">#</a></span>\[\widetilde{\Sigma} = \frac{1}{N-1} \sum_{n=1}^N (x_n-\mu_{ML})^{\top}(x_n-\mu_{ML})\]</div>
</section>
</section>
<section id="the-gaussian-process-gp">
<h2><span class="section-number">7.2. </span>The Gaussian Process (GP)<a class="headerlink" href="#the-gaussian-process-gp" title="Permalink to this heading">#</a></h2>
<p>Gaussian Processes are a generalization of the generative learning concept based on Bayes rule and Gaussian distributions as models derived from <a class="reference external" href="https://eecs189.org/docs/notes/n18.pdf">Gaussian Discriminant Analysis</a>. We explain Gaussian Processes (GPs) on the example of regression and discuss an adaptation to discrimination in the spirit of Gaussian Discriminant Analysis (GDA), however not being identical to GDA at the same time. Consider linear regression:</p>
<div class="math notranslate nohighlight" id="equation-lin-reg-with-phi">
<span class="eqno">(7.25)<a class="headerlink" href="#equation-lin-reg-with-phi" title="Permalink to this equation">#</a></span>\[
h(x) = \omega^{\top} \varphi(x),
\]</div>
<p>with <span class="math notranslate nohighlight">\(\omega \in \mathbb{R}^{m}\)</span> are the weights, <span class="math notranslate nohighlight">\(\varphi \in \mathbb{R}^{m}\)</span> is the feature map, and <span class="math notranslate nohighlight">\(h(x)\)</span> is the hypothesis giving the probability of <span class="math notranslate nohighlight">\(y\)</span>. We now introduce an isotropic Gaussian prior on <span class="math notranslate nohighlight">\(\omega\)</span>, where isotropic is defined as having a diagonal covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> with some scalar <span class="math notranslate nohighlight">\(\alpha^{-1}\)</span></p>
<div class="math notranslate nohighlight" id="equation-gp-prior">
<span class="eqno">(7.26)<a class="headerlink" href="#equation-gp-prior" title="Permalink to this equation">#</a></span>\[
p(\omega) = \mathcal{N}(\omega; 0, \alpha^{-1} I),
\]</div>
<p>where <span class="math notranslate nohighlight">\(0\)</span> is the zero-mean nature of the Gaussian prior, with covariance <span class="math notranslate nohighlight">\(\alpha^{-1} I\)</span>.</p>
<blockquote>
<div><p>Note that <span class="math notranslate nohighlight">\(\alpha\)</span> is a hyperparameter, i.e. responsible for the model properties, it is called hyper <span class="math notranslate nohighlight">\(\ldots\)</span> to differentiate from the weight parameters <span class="math notranslate nohighlight">\(\omega\)</span>.</p>
</div></blockquote>
<p>Now let’s consider the data samples <span class="math notranslate nohighlight">\(\left( y^{(i)} \in \mathbb{R}, x^{(i)} \in \mathbb{R}^{m} \right)_{i=1, \ldots, n}\)</span>. We are interested in leveraging the information contained in the data samples, i.e. in probabilistic lingo in the joint distribution of <span class="math notranslate nohighlight">\(y^{(1)}, \ldots, y^{(n)}\)</span>. We can then write</p>
<div class="math notranslate nohighlight" id="equation-ling-reg-with-phi">
<span class="eqno">(7.27)<a class="headerlink" href="#equation-ling-reg-with-phi" title="Permalink to this equation">#</a></span>\[
y = \Phi \hspace{2pt} \omega,
\]</div>
<p>where <span class="math notranslate nohighlight">\(y \in \mathbb{R}^{n}\)</span>, <span class="math notranslate nohighlight">\(\Phi \in \mathbb{R}^{n \times m}\)</span>, and <span class="math notranslate nohighlight">\(\omega \in \mathbb{R}^{m}\)</span>. <span class="math notranslate nohighlight">\(\Phi\)</span> is the well-known design matrix from the lecture on <a class="reference internal" href="svm.html"><span class="doc std std-doc">Support Vector Machines</span></a>, Eq. <a class="reference internal" href="svm.html#equation-ridge-reg-design-matrix">(6.58)</a>. As we know that <span class="math notranslate nohighlight">\(\omega\)</span> is Gaussian the probability density function (pdf) of <span class="math notranslate nohighlight">\(y\)</span> is also Gaussian:</p>
<div class="math notranslate nohighlight" id="equation-gp-y-stats">
<span class="eqno">(7.28)<a class="headerlink" href="#equation-gp-y-stats" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \mathbb{E}[y] &amp;= \mathbb{E}[\Phi \omega] = \Phi \mathbb{E}[\omega] = 0 \\
    \text{Cov}[y] &amp;= \mathbb{E}[y y^{\top}] = \Phi \mathbb{E}[\omega \omega^{\top}] \Phi^{\top}= \frac{1}{\alpha} \Phi \Phi^{\top} = K \\
    \Longrightarrow p(y) &amp;= \mathcal{N}(y; 0,K)
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(K\)</span> is the Grammian matrix which we already encountered in the SVM lecture.</p>
<p>A Gaussian Process is now defined as:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y^{(i)} = h(x^{(i)}), \quad i=1, \ldots, m\)</span> have a joint Gaussian PDF, i.e. are fully determined by their 2nd-order statistics (mean and covariance).</p></li>
</ul>
</section>
<section id="gp-for-regression">
<h2><span class="section-number">7.3. </span>GP for Regression<a class="headerlink" href="#gp-for-regression" title="Permalink to this heading">#</a></h2>
<p>Take into account Gaussian noise on sample data as a modeling assumption:</p>
<div class="math notranslate nohighlight" id="equation-gp-model">
<span class="eqno">(7.29)<a class="headerlink" href="#equation-gp-model" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    y^{(i)} &amp;= h(x^{(i)}) + \varepsilon^{(i)}, \quad i=1, \ldots, m \\
    p(y^{(i)} | h(x^{(i)})) &amp;= \mathcal{N} \left( y^{(i)}; h(x^{(i)}), \frac{1}{\beta} \right),
\end{align}
\end{split}\]</div>
<p>for an isotropic Gaussian with precision parameter <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<figure class="align-center" id="gp-regression-data">
<a class="reference internal image-reference" href="../_images/gp_regression_data.png"><img alt="../_images/gp_regression_data.png" src="../_images/gp_regression_data.png" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.1 </span><span class="caption-text">Regression example.</span><a class="headerlink" href="#gp-regression-data" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Here, <span class="math notranslate nohighlight">\(h(x^{(i)}) = h^{(i)}\)</span> becomes a <em>latent variable</em>. For the latent variables (these correspond to the noise-free regression data we considered earlier) we then obtain a Gaussian marginal PDF with some Gram matrix <span class="math notranslate nohighlight">\(K\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-gp-h-prior">
<span class="eqno">(7.30)<a class="headerlink" href="#equation-gp-h-prior" title="Permalink to this equation">#</a></span>\[
p(h) = \mathcal{N}(h; 0, K).
\]</div>
<p>The kernel function underlying <span class="math notranslate nohighlight">\(K\)</span> defines a similarity such that if <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_j\)</span> are similar, then <span class="math notranslate nohighlight">\(h(x_i)\)</span> and <span class="math notranslate nohighlight">\(h(x_j)\)</span> should also be similar. The requisite posterior PDF then becomes</p>
<div class="math notranslate nohighlight" id="equation-gp-y-conditional">
<span class="eqno">(7.31)<a class="headerlink" href="#equation-gp-y-conditional" title="Permalink to this equation">#</a></span>\[
p(y|x) = \int p(y | h, x) p(h|x) dh,
\]</div>
<p>i.e. the posterior pdf is defined by the marginalization of <span class="math notranslate nohighlight">\(p(y, h| x)\)</span> over the latent variable <span class="math notranslate nohighlight">\(h\)</span>. For the computation of <span class="math notranslate nohighlight">\(p(y|x)\)</span> we follow previous computations with adjustments of the notation where appropriate. We define the joint random variable <span class="math notranslate nohighlight">\(z\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-gp-joint-z">
<span class="eqno">(7.32)<a class="headerlink" href="#equation-gp-joint-z" title="Permalink to this equation">#</a></span>\[\begin{split}
z = \left[\begin{matrix}
    h \\
    y
\end{matrix}\right], \quad h \in \mathbb{R}^{n}, y \in \mathbb{R}^{n}.
\end{split}\]</div>
<p>From Eq. <a class="reference internal" href="#equation-gaussian-evidence-and-posterior">(7.20)</a> with prior <span class="math notranslate nohighlight">\(p(h)\)</span> and likelihood <span class="math notranslate nohighlight">\(p(y|h)\)</span> we recall that the evidence follows</p>
<div class="math notranslate nohighlight" id="equation-gaussian-evidence-and-posterior-duplicate">
<span class="eqno">(7.33)<a class="headerlink" href="#equation-gaussian-evidence-and-posterior-duplicate" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \mathbb{E}[y] &amp;= A \mu + b \\
    \text{Cov}[y] &amp;= L^{-1} + A  \Lambda A^{\top},
\end{align}
\end{split}\]</div>
<p>into which we now substitute</p>
<div class="math notranslate nohighlight" id="equation-gp-assumptions">
<span class="eqno">(7.34)<a class="headerlink" href="#equation-gp-assumptions" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \mu &amp;= 0, \text{ by modeling assumption} \\
    b &amp;= 0, \text{ by modeling assumption} \\
    A &amp;= I \\
    L^{-1} &amp;= \frac{1}{\beta} I \\
    \Lambda^{-1} &amp;= \text{Cov}[h] = K
\end{align}
\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-gp-y-conditional-substituted">
<span class="eqno">(7.35)<a class="headerlink" href="#equation-gp-y-conditional-substituted" title="Permalink to this equation">#</a></span>\[
\Longrightarrow p(y|x) = \mathcal{N}(y; 0, \frac{1}{\beta}I + K).
\]</div>
<p>Note that the kernel <span class="math notranslate nohighlight">\(K\)</span> can be presumed and is responsible for the accuracy of the prediction by the posterior. There exists a whole plethora of possible choices of K, the most common of which are:</p>
<figure class="align-center" id="gp-kernels">
<a class="reference internal image-reference" href="../_images/gp_kernels.png"><img alt="../_images/gp_kernels.png" src="../_images/gp_kernels.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.2 </span><span class="caption-text">Common GP kernels (Source: <span id="id4">[<a class="reference internal" href="../references.html#id17" title="David Duvenaud. Automatic model construction with gaussian processes. 2014. URL: https://www.cs.toronto.edu/~duvenaud/thesis.pdf.">Duvenaud, 2014</a>]</span>).</span><a class="headerlink" href="#gp-kernels" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Looking at a practical example of a possible kernel:</p>
<div class="math notranslate nohighlight" id="equation-gp-kernel-example">
<span class="eqno">(7.36)<a class="headerlink" href="#equation-gp-kernel-example" title="Permalink to this equation">#</a></span>\[
k(x^{(i)}, x^{(j)}) = \theta_{0} \exp \{- \frac{\theta_{1}}{2} || x^{(i)} - x^{(j)}||^{2}\} + \theta_{2} + \theta_{3} x^{(i) \top} x^{(j)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta_{0}\)</span>, <span class="math notranslate nohighlight">\(\theta_{1}\)</span>, <span class="math notranslate nohighlight">\(\theta_{2}\)</span>, <span class="math notranslate nohighlight">\(\theta_{3}\)</span> are model hyperparameters. With the above <span class="math notranslate nohighlight">\(p(y|x)\)</span> we have identified which Gaussian is inferred from the data. Moreover we have not yet formulated a predictive posterior for <em>unseen data</em>. For this purporse we extend the data set</p>
<div class="math notranslate nohighlight" id="equation-gp-data-split">
<span class="eqno">(7.37)<a class="headerlink" href="#equation-gp-data-split" title="Permalink to this equation">#</a></span>\[
\underbrace{\left( y^{(n+1)}, x^{(n+1)} \right)}_{\text{unseen data}}, \quad \underbrace{\left( y^{(n)}, x^{(n)} \right), \ldots, \left( y^{(1)}, x^{(1)} \right)}_{\text{seen data}}.
\]</div>
<p>We reformulate the prediction problem as that of determining a conditional PDF. Given</p>
<div class="math notranslate nohighlight" id="equation-gp-joint-rv">
<span class="eqno">(7.38)<a class="headerlink" href="#equation-gp-joint-rv" title="Permalink to this equation">#</a></span>\[\begin{split}
y = \left[
    \begin{matrix}
        y^{(1)}\\
        \vdots \\
        y^{(n)}
    \end{matrix}
\right],
\end{split}\]</div>
<p>we search for the conditional PDF <span class="math notranslate nohighlight">\(p(y^{(n+1)}| y)\)</span>. As intermediate we need the joint PDF</p>
<div class="math notranslate nohighlight" id="equation-gp-joint-pdf">
<span class="eqno">(7.39)<a class="headerlink" href="#equation-gp-joint-pdf" title="Permalink to this equation">#</a></span>\[\begin{split}
p(\tilde{y}), \quad \tilde{y} = \left[
    \begin{matrix}
        y^{(1)}\\
        \vdots \\
        y^{(n+1)}
    \end{matrix}
\right].
\end{split}\]</div>
<p>We assume that it also follows a Gaussian PDF. However, as <span class="math notranslate nohighlight">\(x^{(n+1)}\)</span> is unknown before prediction we have to make the dependence of the covariance matrix of <span class="math notranslate nohighlight">\(p(\tilde{y})\)</span> explicit. We first decompose the full covariance matrix into</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Cov[\tilde{y}] = \left[
    \begin{matrix}
        \frac{1}{\beta}I + K &amp;k \\
        k^{\top} &amp;c
    \end{matrix}
\right]= \left[
    \begin{matrix}
        \Sigma_{bb} &amp;\Sigma_{ba} \\
        \Sigma_{ab} &amp;\Sigma_{aa} \\
    \end{matrix}
\right],
\end{split}\]</div>
<p>with the vector</p>
<div class="math notranslate nohighlight" id="equation-gp-kernel-similarity">
<span class="eqno">(7.40)<a class="headerlink" href="#equation-gp-kernel-similarity" title="Permalink to this equation">#</a></span>\[\begin{split}
k = \left[
    \begin{matrix}
        k(x^{(1)}, x^{(n+1)}) \\
        \vdots \\
        k(x^{(n)}, x^{(n+1)})
    \end{matrix}
\right].
\end{split}\]</div>
<p>I.e. the corresponding entries in the extended <span class="math notranslate nohighlight">\(\tilde{K} = \frac{1}{\alpha} \tilde{\Phi} \tilde{\Phi}^{\top}\)</span> i.e. <span class="math notranslate nohighlight">\(\tilde{K}_{1, \ldots, n+1}\)</span>, where <span class="math notranslate nohighlight">\(\tilde{\Phi}_{n+1} = \varphi(x^{n+1})\)</span>. The <span class="math notranslate nohighlight">\(c\)</span>-entry above is then given by</p>
<div class="math notranslate nohighlight" id="equation-gp-query-c">
<span class="eqno">(7.41)<a class="headerlink" href="#equation-gp-query-c" title="Permalink to this equation">#</a></span>\[
c = \tilde{K}_{n+1, n+1} + \frac{1}{\beta}.
\]</div>
<p>Using the same approach as before we can then calculate the mean and covariance of the predictive posterior for unseen data <span class="math notranslate nohighlight">\(p(y^{(n+1)}|y)\)</span>. Recalling from before (Eq. <a class="reference internal" href="#equation-cond-pdf-stats">(7.14)</a>):</p>
<div class="math notranslate nohighlight" id="equation-gp-predictive-mean">
<span class="eqno">(7.42)<a class="headerlink" href="#equation-gp-predictive-mean" title="Permalink to this equation">#</a></span>\[
\mu_{a | b} = \mu_a + \Sigma_{ab} \Sigma^{-1}_{bb}(x_{b} - \mu_{b}),
\]</div>
<p>where we now adjust the entries for our present case with</p>
<div class="math notranslate nohighlight" id="equation-gp-predictive-mean-values">
<span class="eqno">(7.43)<a class="headerlink" href="#equation-gp-predictive-mean-values" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \mu_{a|b} &amp;= \mu_{y^{n+1}|y} \in \mathbb{R} \\
    \mu_{a} &amp;= 0 \\
    \Sigma_{ab} &amp;= k^{\top} \\
    \Sigma^{-1}_{bb} &amp;=  (K + \frac{1}{\beta} I)^{-1} \\
    x_{b} &amp;= y \\
    \mu_{b} &amp;= 0.
\end{align}
\end{split}\]</div>
<p>And for the covariance <span class="math notranslate nohighlight">\(\Sigma\)</span> we recall</p>
<div class="math notranslate nohighlight" id="equation-gp-predicitve-cov">
<span class="eqno">(7.44)<a class="headerlink" href="#equation-gp-predicitve-cov" title="Permalink to this equation">#</a></span>\[
\Sigma_{a|b} = \Sigma_{aa} - \Sigma_{ab} \Sigma^{-1}_{bb} \Sigma_{ba},
\]</div>
<p>adjusting for the entries in our present case</p>
<div class="math notranslate nohighlight" id="equation-gp-predicitve-cov-values">
<span class="eqno">(7.45)<a class="headerlink" href="#equation-gp-predicitve-cov-values" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \Sigma_{aa} &amp;= c \\
    \Sigma_{ab} &amp;= k^{\top} \\
    \Sigma_{bb}^{-1} &amp;= (K + \frac{1}{\beta} I)^{-1} \\
    \Sigma_{ba} &amp;= k \\
    \Sigma_{a|b} &amp;= \Sigma_{y^{(n+1)}|y} \in \mathbb{R}.
\end{align}
\end{split}\]</div>
<p>From which follows</p>
<div class="math notranslate nohighlight" id="equation-gp-predictive-distribution">
<span class="eqno">(7.46)<a class="headerlink" href="#equation-gp-predictive-distribution" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \mu_{y^{(n+1)}|y} &amp;= k^{\top}(K + \frac{1}{\beta}I)^{-1} y \\
    \Sigma_{y^{(n+1)}|y} &amp;= c - k^{\top} (K + \frac{1}{\beta}I)^{-1} k,
\end{align}
\end{split}\]</div>
<p>which fully defines the Gaussian posterior PDF of Gaussian-process regression. Looking at some sketches of GP regression, beginning with the data sample <span class="math notranslate nohighlight">\(y_{1}\)</span>, and predicting the data <span class="math notranslate nohighlight">\(y_{2}\)</span></p>
<figure class="align-center" id="gp-2d-marginalization">
<a class="reference internal image-reference" href="../_images/gp_2d_marginalization.png"><img alt="../_images/gp_2d_marginalization.png" src="../_images/gp_2d_marginalization.png" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.3 </span><span class="caption-text">Marginalizing a 2D Gaussian distribution.</span><a class="headerlink" href="#gp-2d-marginalization" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- TODO: bad figure. Maximum incorrect. -->
<figure class="align-center" id="gp-regression-example1d">
<a class="reference internal image-reference" href="../_images/gp_regression_example1d.png"><img alt="../_images/gp_regression_example1d.png" src="../_images/gp_regression_example1d.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.4 </span><span class="caption-text">1D Gaussian process.</span><a class="headerlink" href="#gp-regression-example1d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>And in practice for a 1-dimensional function, which is being approximated with a GP, and we are monitoring the change in mean, variance and synonymously the GP-behaviour (recall, the GP is defined by the behaviour of its mean and covariance) with each successive data point.</p>
<figure class="align-center" id="gp-regression-examples1d">
<a class="reference internal image-reference" href="../_images/gp_regression_examples1d_.png"><img alt="../_images/gp_regression_examples1d_.png" src="../_images/gp_regression_examples1d_.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.5 </span><span class="caption-text">Probability distribution of a 1D Gaussian process (Source: <span id="id5">[<a class="reference internal" href="../references.html#id17" title="David Duvenaud. Automatic model construction with gaussian processes. 2014. URL: https://www.cs.toronto.edu/~duvenaud/thesis.pdf.">Duvenaud, 2014</a>]</span>).</span><a class="headerlink" href="#gp-regression-examples1d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<blockquote>
<div><p>This immediately shows the utility of GP-regression for engineering applications where we often have few data points but yet need to be able to provide guarantees for our predictions which GPs offer at a reasonable computational cost.</p>
</div></blockquote>
<section id="further-kernel-configurations">
<h3><span class="section-number">7.3.1. </span>Further Kernel Configurations<a class="headerlink" href="#further-kernel-configurations" title="Permalink to this heading">#</a></h3>
<p>There exist many ways in which we can extend beyond just individual kernels by multiplication and addition of simple “basis” kernels to construct better informed kernels. Just taking a quick glance at some of these possible combinations, where the following explores this combinatorial space for the following three kernels (see <a class="reference internal" href="#gp-kernels"><span class="std std-numref">Fig. 7.2</span></a> for definitions):</p>
<ul class="simple">
<li><p>Squared Exponential (SE) kernel</p></li>
<li><p>Linear (Lin) kernel</p></li>
<li><p>Periodic (Per) kernel</p></li>
</ul>
<figure class="align-center" id="gp-adding-kernels">
<a class="reference internal image-reference" href="../_images/gp_adding_kernels.png"><img alt="../_images/gp_adding_kernels.png" src="../_images/gp_adding_kernels.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.6 </span><span class="caption-text">Adding 1D kernels (Source: <span id="id6">[<a class="reference internal" href="../references.html#id17" title="David Duvenaud. Automatic model construction with gaussian processes. 2014. URL: https://www.cs.toronto.edu/~duvenaud/thesis.pdf.">Duvenaud, 2014</a>]</span>).</span><a class="headerlink" href="#gp-adding-kernels" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="gp-multiplying-kernels">
<a class="reference internal image-reference" href="../_images/gp_multiplying_kernels.png"><img alt="../_images/gp_multiplying_kernels.png" src="../_images/gp_multiplying_kernels.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.7 </span><span class="caption-text">Multiplying 1D kernels (Source: <span id="id7">[<a class="reference internal" href="../references.html#id17" title="David Duvenaud. Automatic model construction with gaussian processes. 2014. URL: https://www.cs.toronto.edu/~duvenaud/thesis.pdf.">Duvenaud, 2014</a>]</span>).</span><a class="headerlink" href="#gp-multiplying-kernels" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="gp-multiplying-kernels-to-2d">
<a class="reference internal image-reference" href="../_images/gp_multiplying_kernels_to_2d.png"><img alt="../_images/gp_multiplying_kernels_to_2d.png" src="../_images/gp_multiplying_kernels_to_2d.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.8 </span><span class="caption-text">Multiplying 1D kernels, 2D visualization (Source: <span id="id8">[<a class="reference internal" href="../references.html#id17" title="David Duvenaud. Automatic model construction with gaussian processes. 2014. URL: https://www.cs.toronto.edu/~duvenaud/thesis.pdf.">Duvenaud, 2014</a>]</span>).</span><a class="headerlink" href="#gp-multiplying-kernels-to-2d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In higher dimensions this then takes the following shape:</p>
<figure class="align-center" id="gp-kernels-2d">
<a class="reference internal image-reference" href="../_images/gp_kernels_2d.png"><img alt="../_images/gp_kernels_2d.png" src="../_images/gp_kernels_2d.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.9 </span><span class="caption-text">Examples of 2D kernels (Source: <span id="id9">[<a class="reference internal" href="../references.html#id17" title="David Duvenaud. Automatic model construction with gaussian processes. 2014. URL: https://www.cs.toronto.edu/~duvenaud/thesis.pdf.">Duvenaud, 2014</a>]</span>).</span><a class="headerlink" href="#gp-kernels-2d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>With all these possible combinations one almost begs the question if these kernels cannot be constructed automatically. The answer is, partially yes, partially no. In a sense the right kernel construction is almost like feature engineering, and while it can be automated in parts it remains a craft for the domain scientists to understand the nature of their problem to then construct the right prior distribution.</p>
<figure class="align-center" id="gp-kernel-tree-search">
<a class="reference internal image-reference" href="../_images/gp_kernel_tree_search.png"><img alt="../_images/gp_kernel_tree_search.png" src="../_images/gp_kernel_tree_search.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7.10 </span><span class="caption-text">Search tree over kernels (Source: <span id="id10">[<a class="reference internal" href="../references.html#id17" title="David Duvenaud. Automatic model construction with gaussian processes. 2014. URL: https://www.cs.toronto.edu/~duvenaud/thesis.pdf.">Duvenaud, 2014</a>]</span>).</span><a class="headerlink" href="#gp-kernel-tree-search" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="multivariate-prediction">
<h3><span class="section-number">7.3.2. </span>Multivariate Prediction<a class="headerlink" href="#multivariate-prediction" title="Permalink to this heading">#</a></h3>
<p>The extension to multivariate prediction is then straightforward. For the unseen data</p>
<div class="math notranslate nohighlight" id="equation-gp-multiple-targets">
<span class="eqno">(7.47)<a class="headerlink" href="#equation-gp-multiple-targets" title="Permalink to this equation">#</a></span>\[\begin{split}
y' = \left[
    \begin{matrix}
        y^{(n+1)} \\
        \vdots \\
        y^{(n+l)}
    \end{matrix}
\right] \in \mathbb{R}^{l}, \quad x' = \left[
    \begin{matrix}
        x^{(n+1)}\\
        \vdots \\
        x^{(n+l)}
    \end{matrix}
\right] \in \mathbb{R}^{l \times m},
\end{split}\]</div>
<p>and for the sample data</p>
<div class="math notranslate nohighlight" id="equation-gp-multiple-inputs">
<span class="eqno">(7.48)<a class="headerlink" href="#equation-gp-multiple-inputs" title="Permalink to this equation">#</a></span>\[\begin{split}
y = \left[
    \begin{matrix}
        y^{(1)} \\
        \vdots \\
        y^{(n)}
    \end{matrix}
\right] \in \mathbb{R}^{n}, \quad x = \left[
    \begin{matrix}
        x^{(1)}\\
        \vdots \\
        x^{(n)}
    \end{matrix}
\right] \in \mathbb{R}^{n \times m},
\end{split}\]</div>
<p>for ever larger <span class="math notranslate nohighlight">\(l\)</span>’s the process can then just be repeated. The mean and covariance matrix are then given by</p>
<div class="math notranslate nohighlight" id="equation-gp-posterior-multiple-targets">
<span class="eqno">(7.49)<a class="headerlink" href="#equation-gp-posterior-multiple-targets" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \mu_{y'|y} &amp;= k(x', x) \left( k(x, x) + \frac{1}{\beta} I \right)^{-1} y \\
    \Sigma_{y'|y} &amp;= k(x', x') + \frac{1}{\beta}I \\
    &amp;- k(x', x) \left( k(x, x) + \frac{1}{\beta} I \right)^{-1} k(x, x')
\end{align}
\end{split}\]</div>
</section>
<section id="notes-on-gaussian-process-regression">
<h3><span class="section-number">7.3.3. </span>Notes on Gaussian Process Regression<a class="headerlink" href="#notes-on-gaussian-process-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K + \frac{1}{\beta}I\)</span> needs to be symmetric and positive definite, where positive definite implies that the matrix is symmetric. <span class="math notranslate nohighlight">\(K\)</span> can be constructed from valid kernel functions involving some hyperparameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p>GP regression involves a <span class="math notranslate nohighlight">\(n \times n\)</span> matrix inversion, which requires <span class="math notranslate nohighlight">\(\mathcal{O}(n^{3})\)</span> operations for learning.</p></li>
<li><p>GP prediction involves a <span class="math notranslate nohighlight">\(n \times n\)</span> matrix multiplication which requires <span class="math notranslate nohighlight">\(\mathcal{O}(n^{2})\)</span> operations.</p></li>
<li><p>The operation count can be reduced significantly when lower-dimensional projections of the kernel function on basis functions can be employed, or we are able to exploit sparse matrix computations.</p></li>
</ul>
</section>
<section id="learning-the-hyperparameters">
<h3><span class="section-number">7.3.4. </span>Learning the Hyperparameters<a class="headerlink" href="#learning-the-hyperparameters" title="Permalink to this heading">#</a></h3>
<p>To infer the kernel hyperparameters from data we need to:</p>
<ol class="arabic simple">
<li><p>Introduce an appropriate likelihood function <span class="math notranslate nohighlight">\(p(y | \theta)\)</span></p></li>
<li><p>Determine the optimum <span class="math notranslate nohighlight">\(\theta\)</span> via maximum likelihood estimation (MLE) <span class="math notranslate nohighlight">\(\theta^{\star} = \arg \max \ln p(y | \theta)\)</span>, which corresponds to linear regression</p></li>
<li><p><span class="math notranslate nohighlight">\(\ln p(y|\theta) = -\frac{1}{2} \ln |K + \frac{1}{\beta}I| - \frac{1}{2} y^{\top} \left( K + \frac{1}{\beta}I \right)^{-1} y - \frac{n}{2} \ln 2 \pi\)</span></p></li>
<li><p>Use iterative gradient descent or Newton’s method to find the optimum, where you need to beware of the fact that <span class="math notranslate nohighlight">\(p(y | \theta)\)</span> may be non-convex in <span class="math notranslate nohighlight">\(\theta\)</span>, and hence have multiple maxima</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial}{\partial \theta_{i}} \ln p(y | \theta) &amp;= - \frac{1}{2} \text{trace} \left[ \left(K + \frac{1}{\beta}I\right)^{-1} \frac{\partial(K + \frac{1}{\beta}I)}{\partial \theta_{i}} \right] \\
 &amp;+ \frac{1}{2} y^{\top} \left( K + \frac{1}{\beta} I \right)^{-1} \frac{\partial(K + \frac{1}{\beta}I)}{\partial \theta_{i}}\left(K + \frac{1}{\beta}\right)^{-1}y
\end{align*}
\end{split}\]</div>
</section>
</section>
<section id="gp-for-classification">
<h2><span class="section-number">7.4. </span>GP for Classification<a class="headerlink" href="#gp-for-classification" title="Permalink to this heading">#</a></h2>
<p>Without discussing all the details, we will now provide a brief sketch how Gaussian Processes can be adapted to the task of classification. Consider for simplicity the 2-class problem:</p>
<div class="math notranslate nohighlight">
\[
0 &lt; y &lt; 1, \quad h(x) = \text{sigmoid}(\varphi(x))
\]</div>
<p>the PDF for <span class="math notranslate nohighlight">\(y\)</span> conditioned on the feature <span class="math notranslate nohighlight">\(\varphi(x)\)</span> then follows a Bernoulli-distribution:</p>
<div class="math notranslate nohighlight">
\[
p(y | \varphi) = \text{sigmoid}(\varphi)^{y} \left( 1 - \text{sigmoid}(\varphi) \right)^{1-y}, \quad i=1, \ldots, n
\]</div>
<p>Finding the predictive PDF for unseen data <span class="math notranslate nohighlight">\(p(y^{(n+1)}|y)\)</span>, given the training data</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y = \left[
    \begin{matrix}
        y^{(1)} \\
        \vdots \\
        y^{(n)}
    \end{matrix}
\right]
\end{split}\]</div>
<p>we then introduce a GP prior on</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\tilde{\varphi} = \left[
    \begin{matrix}
        \varphi^{(1)} \\
        \vdots \\
        \varphi^{(n+1)}
    \end{matrix}
\right]
\end{split}\]</div>
<p>hence giving us</p>
<div class="math notranslate nohighlight">
\[
p(\tilde{\varphi}) = \mathcal{N}( \tilde{\varphi}; 0, K + \nu I)
\]</div>
<p>where <span class="math notranslate nohighlight">\(K_{ij} = k(x^{(i)}, x^{(j)})\)</span>, i.e. a Grammian matrix generated by the kernel functions from the feature map <span class="math notranslate nohighlight">\(\varphi(x)\)</span>.</p>
<blockquote>
<div><ul class="simple">
<li><p>Note that we do <strong>NOT</strong> include an explicit noise term in the data covariance as we assume that all sample data have been correctly classified.</p></li>
<li><p>For numerical reasons we introduce a noise-like form which improves conditioning of <span class="math notranslate nohighlight">\(K + \mu I\)</span></p></li>
<li><p>For two-class classification it is sufficient to predict <span class="math notranslate nohighlight">\(p(y^{(n+1)} = 1 | y)\)</span> as <span class="math notranslate nohighlight">\(p(y^{(n+1)} = 0 | y) = 1 - p(y^{(n+1)} = 1 | y)\)</span></p></li>
</ul>
</div></blockquote>
<p>Using the PDF <span class="math notranslate nohighlight">\(p(y=1|\varphi) = \text{sigmoid}(\varphi(x))\)</span> we obtain the predictive PDF:</p>
<div class="math notranslate nohighlight">
\[
p(y^{(n+1)} = 1 | y) = \int p(y^{(n+1)} = 1 | \varphi^{(n+1)})  p(\varphi^{(n+1)}| y) d\varphi^{(n+1)}
\]</div>
<p>The integration of this PDF is analytically intractable, we are hence faced with a number of choices to integrate the PDF:</p>
<ul class="simple">
<li><p>Use sampling-based methods, and Monte-Carlo approximation for the integral</p></li>
<li><p>Assume a Gaussian approximation for the posterior and evaluate the resulting convolution with the sigmoid in an approximating fashion</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(p(\varphi^{(n+1)}|y)\)</span> is the posterior PDF, which is computed from the conditional PDF <span class="math notranslate nohighlight">\(p(y| \varphi)\)</span> with a Gaussian prior <span class="math notranslate nohighlight">\(p(\varphi)\)</span>.</p>
</section>
<section id="relation-of-gp-to-neural-networks">
<h2><span class="section-number">7.5. </span>Relation of GP to Neural Networks<a class="headerlink" href="#relation-of-gp-to-neural-networks" title="Permalink to this heading">#</a></h2>
<p>We have not presented a definition a neural network yet, but we have already met the most primitive definition of a neural network, the perceptron, for classification. The perceptron can be thought of as a neural network with just one layer of “neurons”. On the other hand, the number of hidden units should be limited for perceptrons to limit overfitting. The essential power of neural networks  i.e. the outputs sharing the hidden units tends to get lost when the number of hidden units becomes very large. This is the core connection between neural networks and GPs, <strong>a neural network with infinite width recovers a Gaussian process</strong>.</p>
</section>
<section id="further-references">
<h2><span class="section-number">7.6. </span>Further References<a class="headerlink" href="#further-references" title="Permalink to this heading">#</a></h2>
<p><strong>The Gaussian Distribution</strong></p>
<ul class="simple">
<li><p><span id="id11">[<a class="reference internal" href="../references.html#id2" title="Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. ISBN 978-0-387-31073-2. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span>, Section 2.3</p></li>
</ul>
<p><strong>Gaussian Processes</strong></p>
<ul class="simple">
<li><p><span id="id12">[<a class="reference internal" href="../references.html#id2" title="Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. ISBN 978-0-387-31073-2. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span>, Section 6.4</p></li>
<li><p><span id="id13">[<a class="reference internal" href="../references.html#id17" title="David Duvenaud. Automatic model construction with gaussian processes. 2014. URL: https://www.cs.toronto.edu/~duvenaud/thesis.pdf.">Duvenaud, 2014</a>]</span>, up to page 22</p></li>
<li><p><span id="id14">[<a class="reference internal" href="../references.html#id16" title="Carl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning. MIT Press, 2006. URL: https://gaussianprocess.org/gpml/.">Rasmussen and Williams, 2006</a>]</span>, Sections 2.1-2.6</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="svm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Support Vector Machines</p>
      </div>
    </a>
    <a class="right-next"
       href="gradients.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Gradients</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-gaussian-distribution">7.1. Multivariate Gaussian Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#st-and-2nd-moment">7.1.1. 1st and 2nd Moment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-gaussian-pdf">7.1.2. Conditional Gaussian PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-gaussian-pdf">7.1.3. Marginal Gaussian PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-for-gaussian-variables">7.1.4. Bayes Theorem for Gaussian Variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-for-gaussians">7.1.5. Maximum Likelihood for Gaussians</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-process-gp">7.2. The Gaussian Process (GP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gp-for-regression">7.3. GP for Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-kernel-configurations">7.3.1. Further Kernel Configurations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-prediction">7.3.2. Multivariate Prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notes-on-gaussian-process-regression">7.3.3. Notes on Gaussian Process Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-the-hyperparameters">7.3.4. Learning the Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gp-for-classification">7.4. GP for Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relation-of-gp-to-neural-networks">7.5. Relation of GP to Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">7.6. Further References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By N. Adams, L. Paehler, A. Toshev
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022,2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>