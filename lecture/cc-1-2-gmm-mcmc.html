

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>1.2. GMM and MCMC &#8212; Introduction to Scientific Machine Learning for Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture/cc-1-2-gmm-mcmc';</script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.3. Bayesian methods" href="cc-1-3-bayes.html" />
    <link rel="prev" title="1.1. Linear Models" href="cc-1-1-linear.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../about.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="cc-1-0-basics.html">1. <strong>Basics</strong></a><input checked class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="cc-1-1-linear.html">1.1. Linear Models</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">1.2. GMM and MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-1-3-bayes.html">1.3. Bayesian methods</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cc-2-0-optim.html">2. <strong>Optimization</strong></a><input checked class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cc-2-1-algorithms.html">2.1. Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-2-2-tricks.html">2.2. Tricks of Optimization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cc-3-0-ml.html">3. <strong>Machine Learning</strong></a><input checked class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cc-3-1-svm.html">3.1. Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-3-2-gp.html">3.2. Gaussian Processes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="cc-4-0-dl.html">4. <strong>Deep Learning</strong></a><input checked class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="cc-4-1-gradients.html">4.1. Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-2-mlp.html">4.2. Multilayer Perceptron</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-3-cnn.html">4.3. Convolutional Neural Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-4-rnn.html">4.4. Recurrent Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="cc-4-5-ae.html">4.5. Encoder-Decoder Models</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercise</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/1_linReg_logReg.html">1. Linear Regression and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/2_BayesianInference.html">2. Bayesian Linear and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/3_optimization.html">3. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/4_SVM.html">4. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/5_GPs.html">5. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/6_CNNs.html">6. CNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/7_RNNs.html">7. RNNs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../admin.html">Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preliminary_knowledge.html">Preliminary Knowledge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software.html">Software Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../practical_exam.html">Practical Exam WS22/23</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tumaer/SciML" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tumaer/SciML/issues/new?title=Issue%20on%20page%20%2Flecture/cc-1-2-gmm-mcmc.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lecture/cc-1-2-gmm-mcmc.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>GMM and MCMC</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-theory">1.2.1. Probability Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-building-blocks">1.2.1.1. Basic Building Blocks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables-and-their-properties">1.2.1.2. Random Variables and Their Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#catalogue-of-important-distributions">1.2.1.3. Catalogue of Important Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-family">1.2.1.4. Exponential Family</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-models">1.2.2. Gaussian Mixture Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization">1.2.2.1. Expectation-Maximization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-limitations">1.2.2.2. Applications and Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-methods">1.2.3. Sampling Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#acceptance-rejection-sampling">1.2.3.1. Acceptance-Rejection Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-importance-resampling-bayesian-bootstrap">1.2.3.2. Sampling-Importance-Resampling / Bayesian Bootstrap</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-rejection-sampling">1.2.3.3. Adaptive Rejection Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo">1.2.3.4. Markov Chain Monte Carlo</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">1.2.4. Further References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gmm-and-mcmc">
<h1><span class="section-number">1.2. </span>GMM and MCMC<a class="headerlink" href="#gmm-and-mcmc" title="Permalink to this heading">#</a></h1>
<p>This lesson first recaps on Probability Theory and then introduces Gaussian Mixture Models (GMM) and some popular sampling methods like Markov Chain Monte Carlo (MCMC).</p>
<p>On a high level, GMMs and MCMC are two complementary approaches:</p>
<ul class="simple">
<li><p>GMMs estimate the density of a given set of samples</p></li>
<li><p>MCMC generates samples from a given density</p></li>
</ul>
<figure class="align-center" id="density-estim-vs-sampling">
<a class="reference internal image-reference" href="../_images/density_estimation_vs_sampling.png"><img alt="../_images/density_estimation_vs_sampling.png" src="../_images/density_estimation_vs_sampling.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.3 </span><span class="caption-text">Density estimation vs sampling.</span><a class="headerlink" href="#density-estim-vs-sampling" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>But first, we revise Probability Theory.</p>
<section id="probability-theory">
<h2><span class="section-number">1.2.1. </span>Probability Theory<a class="headerlink" href="#probability-theory" title="Permalink to this heading">#</a></h2>
<section id="basic-building-blocks">
<h3><span class="section-number">1.2.1.1. </span>Basic Building Blocks<a class="headerlink" href="#basic-building-blocks" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><span class="math notranslate nohighlight">\(\Omega\)</span> - <em>sample space</em>; the set of all outcomes of a random experiment.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(E)\)</span> - <em>probability measure of an event <span class="math notranslate nohighlight">\(E \in \Omega\)</span></em>; a function <span class="math notranslate nohighlight">\(\mathbb{P}: \Omega \rightarrow \mathbb{R}\)</span>  satisfies the following three properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(0 \le \mathbb{P}(E) \le 1 \quad \forall E \in \Omega\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(\Omega)=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(\cup_{i=1}^n E_i) = \sum_{i=1}^n \mathbb{P}(E_i) \;\)</span> for disjoint events <span class="math notranslate nohighlight">\({E_1, ..., E_n}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(A, B)\)</span> - <em>joint probability</em>; probability that both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> occur simultaneously.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(A | B)\)</span> - <em>conditional probability</em>; probability that <span class="math notranslate nohighlight">\(A\)</span> occurs, if <span class="math notranslate nohighlight">\(B\)</span> has occured.</p></li>
<li><p>Product rule of probabilities:</p>
<ul class="simple">
<li><p>general case: <br></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-product-rule-general">
<span class="eqno">(1.33)<a class="headerlink" href="#equation-product-rule-general" title="Permalink to this equation">#</a></span>\[\mathbb{P}(A, B) = \mathbb{P}(A | B)\cdot  \mathbb{P}(B) = \mathbb{P}(B | A) \cdot \mathbb{P}(A)\]</div>
<ul class="simple">
<li><p>independent events: <br></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-product-rule-indep">
<span class="eqno">(1.34)<a class="headerlink" href="#equation-product-rule-indep" title="Permalink to this equation">#</a></span>\[\mathbb{P}(A, B) = \mathbb{P}(A) \cdot \mathbb{P}(B)\]</div>
</li>
<li><p>Sum rule of probabilities:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-sum-rule">
<span class="eqno">(1.35)<a class="headerlink" href="#equation-sum-rule" title="Permalink to this equation">#</a></span>\[\mathbb{P}(A)=\sum_{B}\mathbb{P}(A, B)\]</div>
<ul>
<li><p>Bayes rule: solving the general case of the product rule for <span class="math notranslate nohighlight">\(\mathbb{P}(A)\)</span> results in:</p>
<div class="math notranslate nohighlight" id="equation-bayes-rule">
<span class="eqno">(1.36)<a class="headerlink" href="#equation-bayes-rule" title="Permalink to this equation">#</a></span>\[ \mathbb{P}(B|A) = \frac{\mathbb{P}(A|B) \mathbb{P}(B)}{\mathbb{P}(A)} = \frac{\mathbb{P}(A|B) \mathbb{P}(B)}{\sum_{i=1}^n \mathbb{P}(A|B_i)\mathbb{P}(B_i)}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(B|A)\)</span> - <em>posterior</em></p></li>
<li><p><span class="math notranslate nohighlight">\(p(A|B)\)</span> - <em>likelihood</em></p></li>
<li><p><span class="math notranslate nohighlight">\(p(B)\)</span> - <em>prior</em></p></li>
<li><p><span class="math notranslate nohighlight">\(p(A)\)</span> - <em>evidence</em></p></li>
</ul>
</li>
</ul>
</section>
<section id="random-variables-and-their-properties">
<h3><span class="section-number">1.2.1.2. </span>Random Variables and Their Properties<a class="headerlink" href="#random-variables-and-their-properties" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><em>Random variable</em> (r.v.) <span class="math notranslate nohighlight">\(X\)</span> is a function <span class="math notranslate nohighlight">\(X:\Omega \rightarrow \mathbb{R}\)</span>. This is the formal way by which we move from abstract events to real-valued numbers. <span class="math notranslate nohighlight">\(X\)</span> is essentially a variable that does not have a fixed value, but can have different values with certain probabilities.</p></li>
<li><p>Continuous r.v.:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F_X(x)\)</span> - <em>Cumulative distribution function</em> (CDF); probability that the r.v. <span class="math notranslate nohighlight">\(X\)</span> is smaller than some value <span class="math notranslate nohighlight">\(x\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-cdf">
<span class="eqno">(1.37)<a class="headerlink" href="#equation-cdf" title="Permalink to this equation">#</a></span>\[F_X(x) = \mathbb{P}(X\le x)\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p_X(x)\)</span> - <em>Probability density function</em> (PDF):</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-pdf">
<span class="eqno">(1.38)<a class="headerlink" href="#equation-pdf" title="Permalink to this equation">#</a></span>\[p_X(x)=\frac{dF_X(x)}{dx}\ge 0 \;\text{ and } \; \int_{-\infty}^{+\infty}p_X(x) dx =1\]</div>
</li>
</ul>
<figure class="align-center" id="pdf-cdf">
<a class="reference internal image-reference" href="../_images/pdf_cdf.png"><img alt="../_images/pdf_cdf.png" src="../_images/pdf_cdf.png" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.4 </span><span class="caption-text">PDF and CDF functions.</span><a class="headerlink" href="#pdf-cdf" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>discrete r.v.:</p>
<ul>
<li><p><em>Probability mass function</em> (PMF) - same as the pdf but for a discrete r.v. <span class="math notranslate nohighlight">\(X\)</span>. Integrals become sums.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mu = E[X]\)</span> - <em>mean value</em> or <em>expected value</em></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-mean">
<span class="eqno">(1.39)<a class="headerlink" href="#equation-mean" title="Permalink to this equation">#</a></span>\[E[X] = \int_{-\infty}^{+\infty}x \, p_X(x) \, dx\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma^2 = Var[X]\)</span> - <em>variance</em></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-variance">
<span class="eqno">(1.40)<a class="headerlink" href="#equation-variance" title="Permalink to this equation">#</a></span>\[Var[X] = \int_{-\infty}^{+\infty}x^2 \, p_X(x) \, dx = E[(X-\mu)^2]\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Cov[X,Y]=E[(X-\mu_X)(Y-\mu_Y)]\)</span> - <em>covariance</em></p></li>
<li><p><em>Change of variables</em> - if <span class="math notranslate nohighlight">\(X \sim p_X\)</span> and <span class="math notranslate nohighlight">\(Y=h(X)\)</span>, then the distribution of <span class="math notranslate nohighlight">\(Y\)</span> becomes:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-change-of-vars">
<span class="eqno">(1.41)<a class="headerlink" href="#equation-change-of-vars" title="Permalink to this equation">#</a></span>\[p_Y(y)=\frac{p_X(h^{-1}(y))}{\left|\frac{dh}{dx}\right|}\]</div>
</section>
<section id="catalogue-of-important-distributions">
<h3><span class="section-number">1.2.1.3. </span>Catalogue of Important Distributions<a class="headerlink" href="#catalogue-of-important-distributions" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><em>Binomial</em>, <span class="math notranslate nohighlight">\(X\in\{0,1,...,n\}\)</span>. Describes how often we get <span class="math notranslate nohighlight">\(k\)</span> positive outcomes out of <span class="math notranslate nohighlight">\(n\)</span> independent experiments. Parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is the success probability of each trial.</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-binomial">
<span class="eqno">(1.42)<a class="headerlink" href="#equation-binomial" title="Permalink to this equation">#</a></span>\[\mathbb{P}(X=k|\lambda)=\binom{n}{k}\lambda^k(1-\lambda)^{n-k}, \quad \text{ with } k\in(1,2,..., n).\]</div>
<ul class="simple">
<li><p><em>Bernoulli</em> - special case of Binomial with <span class="math notranslate nohighlight">\(n=1\)</span>.</p></li>
<li><p><em>Normal</em> (aka <em>Gaussian</em>), <span class="math notranslate nohighlight">\(X \in \mathbb{R}\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-gaussian">
<span class="eqno">(1.43)<a class="headerlink" href="#equation-gaussian" title="Permalink to this equation">#</a></span>\[p(x| \mu, \sigma)=\mathcal{N}(x|\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</div>
<ul class="simple">
<li><p><em>Multivariate Gaussian</em> <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{\mu}, \mathbf{\Sigma})\)</span> of <span class="math notranslate nohighlight">\(\mathbf{X}\in \mathbb{R}^n\)</span> with mean <span class="math notranslate nohighlight">\(\mathbf{\mu}\in \mathbb{R}^n \)</span> and covariance <span class="math notranslate nohighlight">\(\mathbb{\Sigma} \in \mathbb{R}_{+}^{n\times n}\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-multivariate-gaussian">
<span class="eqno">(1.44)<a class="headerlink" href="#equation-multivariate-gaussian" title="Permalink to this equation">#</a></span>\[p_X(x)= \frac{1}{(2\pi)^{n/2}\sqrt{\det (\mathbf{\Sigma})}} \exp \left(-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^{\top}\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right).\]</div>
</section>
<section id="exponential-family">
<h3><span class="section-number">1.2.1.4. </span>Exponential Family<a class="headerlink" href="#exponential-family" title="Permalink to this heading">#</a></h3>
<p>The exponential family of distributions is a large family of distributions with shared properties, some of which we have already encountered in other courses before. Prominent members of the exponential family include:</p>
<ul class="simple">
<li><p>Bernoulli</p></li>
<li><p>Gaussian</p></li>
<li><p>Dirichlet</p></li>
<li><p>Gamma</p></li>
<li><p>Poisson</p></li>
<li><p>Beta</p></li>
</ul>
<p>At their core, members of the exponential family all fit the same general probability distribution form</p>
<div class="math notranslate nohighlight" id="equation-exponential-pdfs">
<span class="eqno">(1.45)<a class="headerlink" href="#equation-exponential-pdfs" title="Permalink to this equation">#</a></span>\[p(x|\eta) = h(x) \exp \left\{ \eta^{\top} t(x) - a(\eta) \right\},\]</div>
<p>where the individual components are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\eta\)</span> - <em>natural parameter</em></p></li>
<li><p><span class="math notranslate nohighlight">\(t(x)\)</span> - <em>sufficient statistic</em></p></li>
<li><p><span class="math notranslate nohighlight">\(h(x)\)</span> - <em>probability support measure</em></p></li>
<li><p><span class="math notranslate nohighlight">\(a(\eta)\)</span> - <em>log normalizer</em>; guarantees that the probability density integrates to 1.</p></li>
</ul>
<blockquote>
<div><p>If you are unfamiliar with the concept of probability measures, then <span class="math notranslate nohighlight">\(h(x)\)</span> can safely be disregarded. Conceptually it describes the area in the probability space over which the probability distribution is defined.</p>
</div></blockquote>
<p><strong>Why is this family of distributions relevant to this course?</strong></p>
<blockquote>
<div><p>The exponential family has a direct connection to graphical models, which are a formalism favored by many people to visualize machine learning models, and the way individual components interact with each other. As such they are highly instructive, and at the same time foundational to many probabilistic approaches covered in this course.</p>
</div></blockquote>
<p>Let’s inspect the practical example of the Gaussian distribution to see how the theory translates into practice. Taking the probability density function which we have also previously worked with</p>
<div class="math notranslate nohighlight" id="equation-gaussian-2">
<span class="eqno">(1.46)<a class="headerlink" href="#equation-gaussian-2" title="Permalink to this equation">#</a></span>\[p(x|\mu, \sigma^{2}) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left\{ \frac{(x - \mu)^{2}}{2 \sigma^{2}} \right\}.\]</div>
<p>We can then expand the square in the exponent of the Gaussian to isolate the individual components of the exponential family</p>
<div class="math notranslate nohighlight" id="equation-gaussian-expanded">
<span class="eqno">(1.47)<a class="headerlink" href="#equation-gaussian-expanded" title="Permalink to this equation">#</a></span>\[p(x|\mu, \sigma^{2}) = \frac{1}{\sqrt{2 \pi}} \exp \left\{ \frac{\mu}{\sigma^{2}}x - \frac{1}{2 \sigma^{2}}x^{2} - \frac{1}{2 \sigma^{2}} \mu^{2} - \log \sigma \right\}.\]</div>
<p>Then the individual components of the Gaussian are</p>
<div class="math notranslate nohighlight">
\[\eta = \langle \frac{\mu}{\sigma^{2}}, - \frac{1}{2 \sigma^{2}} \rangle\]</div>
<div class="math notranslate nohighlight">
\[t(x) = \langle x, x^{2} \rangle\]</div>
<div class="math notranslate nohighlight">
\[a(\eta) = \frac{\mu^{2}}{2 \sigma^{2}} + \log \sigma\]</div>
<div class="math notranslate nohighlight" id="equation-gaussian-as-exponential">
<span class="eqno">(1.48)<a class="headerlink" href="#equation-gaussian-as-exponential" title="Permalink to this equation">#</a></span>\[h(x) = \frac{1}{\sqrt{2 \pi}}\]</div>
<p>For the sufficient statistics, we then need to derive the derivative of the log normalizer, i.e</p>
<div class="math notranslate nohighlight" id="equation-gaussian-as-exponential-suff-stats">
<span class="eqno">(1.49)<a class="headerlink" href="#equation-gaussian-as-exponential-suff-stats" title="Permalink to this equation">#</a></span>\[\frac{d}{d\eta}a(\eta) = \mathbb{E}\left[ t(X) \right]\]</div>
<p>Which yields</p>
<div class="math notranslate nohighlight">
\[\frac{da(\eta)}{d\eta_{1}} = \mu = \mathbb{E}[X] \]</div>
<div class="math notranslate nohighlight" id="equation-gaussian-as-exponential-suff-stats-2">
<span class="eqno">(1.50)<a class="headerlink" href="#equation-gaussian-as-exponential-suff-stats-2" title="Permalink to this equation">#</a></span>\[\frac{da(\eta)}{d\eta_{2}} = \sigma^2 - \mu^2 = \mathbb{E}[X^{2}] \]</div>
<p><strong>Exercise: Exponential Family 1</strong></p>
<p>Show that the Dirichlet distribution is a member of the exponential family.</p>
<p><strong>Exercise: Exponential Family 2</strong></p>
<p>Show that the Bernoulli distribution is a member of the exponential family</p>
</section>
</section>
<section id="gaussian-mixture-models">
<h2><span class="section-number">1.2.2. </span>Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Permalink to this heading">#</a></h2>
<p>Assume that we have a set of measurements <span class="math notranslate nohighlight">\(\{x^{(1)}, \dots x^{(m)}\}\)</span>. This is one of the few unsupervised learning examples in this lecture, thus, we do not know the true labels <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Gaussian Mixture Models (GMMs) assume that the data comes from a mixture of <span class="math notranslate nohighlight">\(K\)</span> Gaussian distributions in the form</p>
<div class="math notranslate nohighlight" id="equation-gmm-model">
<span class="eqno">(1.51)<a class="headerlink" href="#equation-gmm-model" title="Permalink to this equation">#</a></span>\[p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k),\]</div>
<p>with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\pi = (\pi_1,...,\pi_k)\)</span> called mixing coefficients, or cluster probabilities,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu = (\mu_1,...,\mu_k)\)</span> the cluster means, and</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma = (\Sigma_1,...,\Sigma_k)\)</span> the cluster covariance matrices.</p></li>
</ul>
<p>We define a K-dimensional r.v. <span class="math notranslate nohighlight">\(z\)</span> which satisfies <span class="math notranslate nohighlight">\(z\in \{0,1\}\)</span> and <span class="math notranslate nohighlight">\(\sum_k z_k=1\)</span> (i.e. with only one of its dimensions being 1, while all others are 0), such that <span class="math notranslate nohighlight">\(z_k~\sim \text{Multinomial}(\pi_k)\)</span> and <span class="math notranslate nohighlight">\(p(z_k=1) = \pi_k\)</span> . For Eq. <a class="reference internal" href="#equation-gmm-model">(1.51)</a> to be a valid probability density, the parameters <span class="math notranslate nohighlight">\(\{\pi_k\}\)</span> must satisfy <span class="math notranslate nohighlight">\(0\le\pi_k\le 1\)</span> and <span class="math notranslate nohighlight">\(\sum_k \pi_k=1\)</span>.</p>
<p>The marginal distribution of <span class="math notranslate nohighlight">\(z\)</span> can be equivalently written as</p>
<div class="math notranslate nohighlight" id="equation-gmm-marginal-z">
<span class="eqno">(1.52)<a class="headerlink" href="#equation-gmm-marginal-z" title="Permalink to this equation">#</a></span>\[p(z)=\prod_{k=1}^{K} \pi_k^{z_k},\]</div>
<p>whereas the conditional <span class="math notranslate nohighlight">\(p(x|z_k=1) = \mathcal{N}(x|\mu_k, \Sigma_k)\)</span> becomes</p>
<div class="math notranslate nohighlight" id="equation-gmm-conditional">
<span class="eqno">(1.53)<a class="headerlink" href="#equation-gmm-conditional" title="Permalink to this equation">#</a></span>\[p(x|z) = \prod_{k=1}^{K}\mathcal{N}(x|\mu_k, \Sigma_k)^{z_k}.\]</div>
<p>If we then express the distribution of interest <span class="math notranslate nohighlight">\(p(x)\)</span> as the marginalized joint distribution, we obtain</p>
<div class="math notranslate nohighlight" id="equation-gmm-marginalization">
<span class="eqno">(1.54)<a class="headerlink" href="#equation-gmm-marginalization" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
p(x) &amp;= \sum_z p(x,z) \\
&amp; = \sum_z p(x|z) p(z) \\
&amp; = \sum_{k=1}^K \pi_k\mathcal{N}(x| \mu_k, \Sigma_k)
\end{aligned}
\end{split}\]</div>
<p>Thus, the unknown parameters are <span class="math notranslate nohighlight">\(\{\pi_k, \mu_k, \Sigma_k\}_{k=1:K}\)</span>. We can write the log likelihood of the data as</p>
<div class="math notranslate nohighlight" id="equation-gmm-mle">
<span class="eqno">(1.55)<a class="headerlink" href="#equation-gmm-mle" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
l(x | \pi,\mu,\Sigma) &amp;= \sum_{i=1}^{m}\log p(x^{(i)}|\pi,\mu,\Sigma) \\
&amp;= \sum_{i=1}^{m}\log \left\{ \sum_{k=1}^K \pi_k \mathcal{N}(x^{(i)}|\mu_k,\Sigma_k) \right\}
\end{aligned}\end{split}\]</div>
<p>However, if we try to analytically solve this problem, we will see that there is no closed form solution. The problem is that we do not know which <span class="math notranslate nohighlight">\(z_k\)</span> each of the measurements comes from.</p>
<section id="expectation-maximization">
<h3><span class="section-number">1.2.2.1. </span>Expectation-Maximization<a class="headerlink" href="#expectation-maximization" title="Permalink to this heading">#</a></h3>
<figure class="align-center" id="em-algorithm">
<a class="reference internal image-reference" href="../_images/em_algorithm.png"><img alt="../_images/em_algorithm.png" src="../_images/em_algorithm.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.5 </span><span class="caption-text">EM algorithm for a GMM with <span class="math notranslate nohighlight">\(k=2\)</span> (Source: <a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Bishop, 2006</a>, Section 9.2).</span><a class="headerlink" href="#em-algorithm" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>There is an iterative algorithms that can solve the maximum likelihood problem by alternating between two steps. The algorithm goes as follows:</p>
<ol class="arabic simple" start="0">
<li><p>Guess the number of modes <span class="math notranslate nohighlight">\(K\)</span></p></li>
<li><p>Randomly initialize the means <span class="math notranslate nohighlight">\(\mu_k\)</span>, covariances <span class="math notranslate nohighlight">\(\Sigma_k\)</span>, and mixing coefficients <span class="math notranslate nohighlight">\(\pi_k\)</span>, and evaluate the likelihood</p></li>
<li><p><strong>(E-step)</strong>. Evaluate <span class="math notranslate nohighlight">\(\omega_k^{(i)}\)</span> assuming constant <span class="math notranslate nohighlight">\(\pi, \mu, \Sigma\)</span> (see expression after the algorithm)
$<span class="math notranslate nohighlight">\(w_k^{(i)} := p(z^{(i)}=k| x^{(i)}, \pi, \mu, \Sigma).\)</span>$ (gmm_e_step)</p></li>
<li><p><strong>(M-step)</strong>. Update the parameters by solving the maximum likelihood probelms for fixed <span class="math notranslate nohighlight">\(z_k\)</span> values.
$<span class="math notranslate nohighlight">\(\begin{aligned}
 \pi_k &amp;:= \frac{1}{m}\sum_{i=1}^m w_k^{(i)} \\
 \mu_k &amp;:= \frac{\sum_{i=1}^{m} w_k^{(i)}x^{(i)}}{\sum_{i=1}^{m} w_k^{(i)}} \\
 \Sigma_k &amp;:= \frac{\sum_{i=1}^{m} w_k^{(i)}(x^{(i)}-\mu_k)(x^{(i)}-\mu_k)^{\top}}{\sum_{i=1}^{m} w_k^{(i)}}
 \end{aligned}
 \)</span>$ (gmm_m_step)</p></li>
<li><p>Evaluate the log likelihood
$<span class="math notranslate nohighlight">\(l(x | \pi,\mu,\Sigma) = \sum_{i=1}^{m}\log \left\{ \sum_{k=1}^K \pi_k \mathcal{N}(x^{(i)}|\mu_k,\Sigma_k) \right\}\)</span>$ (gmm_lig_likelihood)
and check for convergence. If not converged, return to step 2.</p></li>
</ol>
<p>In the E-step, we compute the posterior probability of <span class="math notranslate nohighlight">\(z^{(i)}_k\)</span> given the data point <span class="math notranslate nohighlight">\(x^{(i)}\)</span> and the current <span class="math notranslate nohighlight">\(\pi\)</span>, <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span> values as</p>
<div class="math notranslate nohighlight" id="equation-gmm-responsibilities">
<span class="eqno">(1.56)<a class="headerlink" href="#equation-gmm-responsibilities" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
p(z^{(i)}=k| x^{(i)},\pi,\mu,\Sigma) &amp;= \frac{p(x^{(i)}|z^{(i)}=k, \mu, \Sigma)p(z^{(i)}=k,\pi)}{\sum_{l=1}^K p(x^{(i)}|z^{(i)}=l, \mu, \Sigma)p(z^{(i)}=l,\pi)} \\
 &amp;= \frac{\pi_k \mathcal{N}(x^{(i)}|\mu_K, \Sigma_k)}{\sum_{l=1}^K \pi_l \mathcal{N}(x^{(i)}|\mu_l, \Sigma_l)}
\end{aligned}\end{split}\]</div>
<p>The values of <span class="math notranslate nohighlight">\(p(x^{(i)}|z^{(i)}=k, \mu, \Sigma)\)</span> can be computed by evaluating the <span class="math notranslate nohighlight">\(k\)</span>th Gaussian with parameters <span class="math notranslate nohighlight">\(\mu_k\)</span> and <span class="math notranslate nohighlight">\(\Sigma_k\)</span>. And <span class="math notranslate nohighlight">\(p(z^{(i)}=k,\pi)\)</span> is just <span class="math notranslate nohighlight">\(\pi_k\)</span>.</p>
<p><strong>Exercise: derive the M-step update equations following the maximum likelihood approach.</strong></p>
<blockquote>
<div><p>Hint: look at <a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Bishop, 2006</a>, Section 9.2.</p>
</div></blockquote>
</section>
<section id="applications-and-limitations">
<h3><span class="section-number">1.2.2.2. </span>Applications and Limitations<a class="headerlink" href="#applications-and-limitations" title="Permalink to this heading">#</a></h3>
<p>Once we have fitted a GMM on <span class="math notranslate nohighlight">\(p(x)\)</span>, we can use it for:</p>
<ol class="arabic simple">
<li><p>Density estimation: by evaluate the probability <span class="math notranslate nohighlight">\(p(\tilde{x})\)</span> of any new point <span class="math notranslate nohighlight">\(\tilde{x}\)</span>, we can say how probable it is that this point comes from the same distribution as the training data.</p></li>
<li><p>Clustering: so far we have talked about density estimation, but GMMs are typically used for clustering. Given a new query point <span class="math notranslate nohighlight">\(\tilde{x}\)</span>, we can evaluate each of the <span class="math notranslate nohighlight">\(K\)</span> Gaussians and scale their probability by the respective <span class="math notranslate nohighlight">\(\pi_k\)</span>. These will be the probabilities of <span class="math notranslate nohighlight">\(\tilde{x}\)</span> to be part of cluster <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
</ol>
<p>Most limitations of this approach arive from the assumption that the indivudual clusters follow the Gaussian distribution:</p>
<ul class="simple">
<li><p>If the data does not follow a Gaussian distribution, e.g. heavy-tailed ditribution with outliers, then too much weight will be given to the outliers</p></li>
<li><p>If there is an outlier, eventually one mode will focus only on this one data point. But if a Gaussian describes only one data point, then its variance will be zero and we recover a singularity/Dirac function.</p></li>
<li><p>The choice of <span class="math notranslate nohighlight">\(K\)</span> is crucial and this parameters need to be optimized in a outer loop.</p></li>
<li><p>GMMs do now scale well to high dimensions.</p></li>
</ul>
</section>
</section>
<section id="sampling-methods">
<h2><span class="section-number">1.2.3. </span>Sampling Methods<a class="headerlink" href="#sampling-methods" title="Permalink to this heading">#</a></h2>
<p>The opposite process to density estimation is sampling. Here, we will look at the general case of an unnormalized (<span class="math notranslate nohighlight">\(\int_{\Omega}p(\cdot)\ne 1\)</span>) <em>target distribution</em></p>
<div class="math notranslate nohighlight" id="equation-target-density">
<span class="eqno">(1.57)<a class="headerlink" href="#equation-target-density" title="Permalink to this equation">#</a></span>\[p(y,\theta) = g(\theta)f(y|\theta).\]</div>
<p>The task we will be trying to solve here is how to generate samples from this distribution. How these samples are useful and why we choose this particular form of a target distribution will be the topic of next week’s lecture.</p>
<section id="acceptance-rejection-sampling">
<h3><span class="section-number">1.2.3.1. </span>Acceptance-Rejection Sampling<a class="headerlink" href="#acceptance-rejection-sampling" title="Permalink to this heading">#</a></h3>
<p>Acceptance-rejection sampling draws its random samples directly from the target posterior distribution, as we only have access to the unscaled target distribution initially we will have to draw from the unscaled target. <em>The acceptance-rejection algorithm is specially made for this scenario.</em> The acceptance-rejection algorithm draws random samples from an easier-to-sample starting distribution and then successively reshapes its distribution by only selectively accepting candidate values into the final sample. For this approach to work the <em>candidate distribution</em> <span class="math notranslate nohighlight">\(g_{0}(\theta)\)</span> has to dominate the posterior distribution, i.e. there must exist an <span class="math notranslate nohighlight">\(M\)</span> s.t.</p>
<div class="math notranslate nohighlight" id="equation-acceptance-rejection">
<span class="eqno">(1.58)<a class="headerlink" href="#equation-acceptance-rejection" title="Permalink to this equation">#</a></span>\[M \times g_{0}(\theta) \geq g(\theta) f(y|\theta), \quad \forall \theta\]</div>
<p>Taking an example candidate density for an unscaled target as an example to show the “dominance” of the candidate distribution over the posterior distribution.</p>
<figure class="align-center" id="acceptance-rejection">
<a class="reference internal image-reference" href="../_images/acceptance_rejection.png"><img alt="../_images/acceptance_rejection.png" src="../_images/acceptance_rejection.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.6 </span><span class="caption-text">Acceptance-rejection candidate and target distributions (Source: Bolstad, 2009).</span><a class="headerlink" href="#acceptance-rejection" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>To then apply acceptance-rejection sampling to the posterior distribution we can write out the algorithm as follows:</p>
<ol class="arabic simple">
<li><p>Draw a random sample of size <span class="math notranslate nohighlight">\(N\)</span> from the candidate distribution <span class="math notranslate nohighlight">\(g_{0}(\theta)\)</span>.</p></li>
<li><p>Calculate the value of the unscaled target density at each random sample.</p></li>
<li><p>Calculate the candidate density at each random sample, and multiply by <span class="math notranslate nohighlight">\(M\)</span>.</p></li>
<li><p>Compute the weights for each random sample</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-acceptance-rejection-weights">
<span class="eqno">(1.59)<a class="headerlink" href="#equation-acceptance-rejection-weights" title="Permalink to this equation">#</a></span>\[ w_{i} = \frac{g(\theta_{i}) \times f(y_{1}, \ldots, y_{n}| \theta_{i})}{M \times g_{0}(\theta_{i})}\]</div>
<ol class="arabic simple" start="5">
<li><p>Draw <span class="math notranslate nohighlight">\(N\)</span> samples from the <span class="math notranslate nohighlight">\(U(0, 1)\)</span> uniform distribution.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(u_{i} &lt; w_{i}\)</span> accept <span class="math notranslate nohighlight">\(\theta_{i}\)</span></p></li>
</ol>
</section>
<section id="sampling-importance-resampling-bayesian-bootstrap">
<h3><span class="section-number">1.2.3.2. </span>Sampling-Importance-Resampling / Bayesian Bootstrap<a class="headerlink" href="#sampling-importance-resampling-bayesian-bootstrap" title="Permalink to this heading">#</a></h3>
<p>The sampling-importance-resampling algorithm is a two-stage extension of the acceptance-rejection sampling which has an improved weight-calculation, but most importantly employs a <em>resampling</em> step. This resampling step resamples from the space of parameters. The weight is then calculated as</p>
<div class="math notranslate nohighlight" id="equation-sampling-importance-resampling-weights">
<span class="eqno">(1.60)<a class="headerlink" href="#equation-sampling-importance-resampling-weights" title="Permalink to this equation">#</a></span>\[w_{i} = \frac{\frac{g(\theta_{i})f(y_{1}, \ldots\ y_{n} | \theta_{i})}{g_{0}(\theta_{i})}}{\left( \sum_{i=1}^{N} \frac{g(\theta_{i})f(y_{1}, \ldots, y_{n}| \theta_{i})}{g_{0}(\theta_{i})} \right)} \]</div>
<p>The algorithm to sample from the posterior distribution is then:</p>
<ol class="arabic simple">
<li><p>Draw <span class="math notranslate nohighlight">\(N\)</span> random samples <span class="math notranslate nohighlight">\(\theta_{i}\)</span> from the starting density <span class="math notranslate nohighlight">\(g_{0}(\theta)\)</span></p></li>
<li><p>Calculate the value of the unscaled target density at each random sample.</p></li>
<li><p>Calculate the starting density at each random sample, and multiply by <span class="math notranslate nohighlight">\(M\)</span>.</p></li>
<li><p>Calculate the ratio of the unscaled posterior to the starting distribution</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-sampling-importance-resampling-weights-r">
<span class="eqno">(1.61)<a class="headerlink" href="#equation-sampling-importance-resampling-weights-r" title="Permalink to this equation">#</a></span>\[r_{i} = \frac{g(\theta_{i})f(y_{1}, \ldots, y_{n}| \theta_{i})}{g_{0}(\theta_{i})}\]</div>
<ol class="arabic simple" start="5">
<li><p>Calculate the importance weights</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-sampling-importance-resampling-weights-w">
<span class="eqno">(1.62)<a class="headerlink" href="#equation-sampling-importance-resampling-weights-w" title="Permalink to this equation">#</a></span>\[w_{i} = \frac{r_{i}}{\sum r_{i}}\]</div>
<ol class="arabic simple" start="6">
<li><p>Draw <span class="math notranslate nohighlight">\(n \leq 0.1 \times N\)</span> random samples with the sampling probabilities given by the importance weights.</p></li>
</ol>
</section>
<section id="adaptive-rejection-sampling">
<h3><span class="section-number">1.2.3.3. </span>Adaptive Rejection Sampling<a class="headerlink" href="#adaptive-rejection-sampling" title="Permalink to this heading">#</a></h3>
<p>If we are unable to find a candidate/starting distribution, which dominates the unscaled posterior distribution immediately, then we have to rely on <em>adaptive rejection sampling</em>.</p>
<blockquote>
<div><p>This approach only works for a log-concave posterior!</p>
</div></blockquote>
<p>See below for an example of a log-concave distribution.</p>
<figure class="align-center" id="adaptive-rejection-sapling">
<a class="reference internal image-reference" href="../_images/adaptive_rejection_sampling.png"><img alt="../_images/adaptive_rejection_sampling.png" src="../_images/adaptive_rejection_sampling.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.7 </span><span class="caption-text">(Not) log-concave function (Source: Bolstad, 2009).</span><a class="headerlink" href="#adaptive-rejection-sapling" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Using the tangent method, our algorithm then takes the following form:</p>
<ol class="arabic simple">
<li><p>Construct an upper bound from piecewise exponential functions, which dominate the log-concave unscaled posterior</p></li>
<li><p>With the envelope giving us the initial candidate density we draw <span class="math notranslate nohighlight">\(N\)</span> random samples</p></li>
<li><p>Rejection sampling, see the preceding two subsections for details.</p></li>
<li><p>If rejected, add another exponential piece which is tangent to the target density.</p></li>
</ol>
<blockquote>
<div><p>As all three presented sampling approaches have their limitations, practitioners often rely more on Markov chain Monte Carlo methods such as Gibbs sampling, and Metropolis-Hastings.</p>
</div></blockquote>
</section>
<section id="markov-chain-monte-carlo">
<h3><span class="section-number">1.2.3.4. </span>Markov Chain Monte Carlo<a class="headerlink" href="#markov-chain-monte-carlo" title="Permalink to this heading">#</a></h3>
<p>The idea of Markov Chain Monte Carlo (MCMC) is to construct an ergodic Markov chain of samples <span class="math notranslate nohighlight">\(\{\theta^0, \theta^1, ...,\theta^N\}\)</span> distributed according to the posterior distribution <span class="math notranslate nohighlight">\(g(\theta|y) \propto g(\theta)f(y|\theta)\)</span>. This chain evolves according to a transition kernel given by <span class="math notranslate nohighlight">\(q(x_{next}|x_{current})\)</span>. Let’s look at one of the most popular MCMC algorithms: Metropolis Hastings</p>
<p><strong>Metropolis-Hastings</strong></p>
<p>The general Metropolis-Hastings prescribes a rule which guarantees that the constructed chain is representative of the target distribution <span class="math notranslate nohighlight">\(g(\theta|y)\)</span>. This is done by following the algorithm:</p>
<ol class="arabic simple" start="0">
<li><p>Start at an initial point <span class="math notranslate nohighlight">\(\theta_{current} = \theta^0\)</span>.</p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(\theta' \sim q(\theta_{next}|\theta_{current})\)</span></p></li>
<li><p>Compute the ecceptance probability
$<span class="math notranslate nohighlight">\(\alpha = min \left\{ 1, \frac{g(\theta'|y) q(\theta_{current}|\theta')}{g(\theta_{current}|y) q(\theta'|\theta_{current})} \right\}\)</span>$</p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(u\sim \text{Uniform}(0,1)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(\alpha &gt; u\)</span>, then <span class="math notranslate nohighlight">\(\theta_{current} = \theta'\)</span>, else <span class="math notranslate nohighlight">\(\theta_{current} = \theta_{current}\)</span></p></li>
<li><p>Repeat <span class="math notranslate nohighlight">\(N\)</span> times from step 1.</p></li>
</ol>
<p>A special choice of <span class="math notranslate nohighlight">\(q(\cdot | \cdot)\)</span> is for example the normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\cdot | \theta_{current}, \sigma^2)\)</span>, which results in the so-called Random Walk Metropolis algorithm. Other special cases include the Metropolis-Adjusted Langevin Algorithm (MALA), as well as the Hamiltonian Monte Carlo (HMC) algorithm.</p>
<figure class="align-center" id="metropolis-hastings">
<a class="reference internal image-reference" href="../_images/metropolis_hastings.png"><img alt="../_images/metropolis_hastings.png" src="../_images/metropolis_hastings.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1.8 </span><span class="caption-text">Metropolis-Hastings trajectory (Source: <a class="reference external" href="https://relguzman.blogspot.com/2018/04/sampling-metropolis-hastings.html">relguzman.blogpost.com</a>).</span><a class="headerlink" href="#metropolis-hastings" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<hr class="docutils" />
<p>In summary:</p>
<ul class="simple">
<li><p>The unscaled posterior <span class="math notranslate nohighlight">\(g(\theta|y) \propto g(\theta)f(y|\theta)\)</span> contains the <em>shape information</em> of the posterior</p></li>
<li><p>For the true posterior, the unscaled posterior needs to be divided by an integral over the whole parameter space.</p></li>
<li><p>Integral has to be evaluated numerically for which we rely on the just presented Monte Carlo sampling techniques.</p></li>
</ul>
</section>
</section>
<section id="further-references">
<h2><span class="section-number">1.2.4. </span>Further References<a class="headerlink" href="#further-references" title="Permalink to this heading">#</a></h2>
<p><strong>Probability Theory</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://sgfin.github.io/files/notes/CS229_Lecture_Notes.pdf">CS229 Lecture notes</a>, Andrew Ng, Parts III, “The exponential family”</p></li>
</ul>
<p><strong>Gaussian Mixture Models</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://sgfin.github.io/files/notes/CS229_Lecture_Notes.pdf">CS229 Lecture notes</a>, Andrew Ng, Parts VIII, “Mixtures of Gaussians and the EM algorithm”</p></li>
<li><p><a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>, Bishop, 2006,
Section 9.2</p></li>
</ul>
<p><strong>Markov Chain Monte Carlo</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH&amp;target=banana">Interactive MCMC visualizations</a></p></li>
<li><p><a class="reference external" href="https://link.springer.com/book/10.1007/978-1-4757-4145-2">Monte Carlo Statistical Methods</a>, Rober &amp; Casella, 2004</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="cc-1-1-linear.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1.1. </span>Linear Models</p>
      </div>
    </a>
    <a class="right-next"
       href="cc-1-3-bayes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.3. </span>Bayesian methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-theory">1.2.1. Probability Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-building-blocks">1.2.1.1. Basic Building Blocks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables-and-their-properties">1.2.1.2. Random Variables and Their Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#catalogue-of-important-distributions">1.2.1.3. Catalogue of Important Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-family">1.2.1.4. Exponential Family</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-models">1.2.2. Gaussian Mixture Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization">1.2.2.1. Expectation-Maximization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-limitations">1.2.2.2. Applications and Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-methods">1.2.3. Sampling Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#acceptance-rejection-sampling">1.2.3.1. Acceptance-Rejection Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-importance-resampling-bayesian-bootstrap">1.2.3.2. Sampling-Importance-Resampling / Bayesian Bootstrap</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-rejection-sampling">1.2.3.3. Adaptive Rejection Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo">1.2.3.4. Markov Chain Monte Carlo</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">1.2.4. Further References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By N. Adams, L. Paehler, A. Toshev
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022,2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>