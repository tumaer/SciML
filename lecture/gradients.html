
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>8. Gradients &#8212; Introduction to Scientific Machine Learning for Engineers</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. Multilayer Perceptron" href="mlp.html" />
    <link rel="prev" title="7. Gaussian Processes" href="gp.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Scientific Machine Learning for Engineers</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction.html">
   Introduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lecture
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="linear.html">
   1. Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gmm.html">
   2. Gaussian Mixture Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bayes.html">
   3. Bayesian methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optimization.html">
   4. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tricks.html">
   5. Tricks of Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svm.html">
   6. Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gp.html">
   7. Gaussian Processes
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   8. Gradients
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mlp.html">
   9. Multilayer Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnn.html">
   10. Convolutional Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rnn.html">
   11. Recurrent Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ae.html">
   12. Encoder-Decoder Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Exercise
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/linear.html">
   1. Linear and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/bayes.html">
   2. Bayesian Linear and Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/optimization.html">
   3. Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/svm.html">
   4. Support Vector Machines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/gp.html">
   5. Gaussian Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/cnn.html">
   6. CNNs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../exercise/rnn.html">
   7. RNNs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Miscellaneous
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../admin.html">
   Admin
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../books.html">
   Books
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preliminary_knowledge.html">
   Preliminary Knowledge
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../software.html">
   Software Infrastructure
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../practical_exam.html">
   Practical Exam WS22/23
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../faq.html">
   Frequently Asked Questions
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/tumaer/SciML"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/tumaer/SciML/issues/new?title=Issue%20on%20page%20%2Flecture/gradients.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/lecture/gradients.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-brief-incomplete-history">
   8.1. A Brief Incomplete History
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-tl-dr-of-gradients">
   8.2. The tl;dr of Gradients
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forward-mode-differentiation">
     8.2.1. Forward-Mode Differentiation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reverse-mode-differentiation-backpropagation">
     8.2.2. Reverse-Mode Differentiation (Backpropagation)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forward-vs-reverse-mode">
     8.2.3. Forward- vs. Reverse-Mode
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-depth-look">
   8.3. In-Depth Look
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-practical-example">
   8.4. A Practical Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-are-the-core-levers-of-the-alternative-approaches">
   8.5. What are the Core-Levers of the Alternative Approaches
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-references">
   8.6. Further References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Gradients</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-brief-incomplete-history">
   8.1. A Brief Incomplete History
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-tl-dr-of-gradients">
   8.2. The tl;dr of Gradients
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forward-mode-differentiation">
     8.2.1. Forward-Mode Differentiation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reverse-mode-differentiation-backpropagation">
     8.2.2. Reverse-Mode Differentiation (Backpropagation)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forward-vs-reverse-mode">
     8.2.3. Forward- vs. Reverse-Mode
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-depth-look">
   8.3. In-Depth Look
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-practical-example">
   8.4. A Practical Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-are-the-core-levers-of-the-alternative-approaches">
   8.5. What are the Core-Levers of the Alternative Approaches
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-references">
   8.6. Further References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="gradients">
<h1><span class="section-number">8. </span>Gradients<a class="headerlink" href="#gradients" title="Permalink to this headline">#</a></h1>
<p>Gradients are a general tool of utility across many scientific domains and keep reappearing across areas. Machine learning is just one of a much larger group of examples which utilizes gradients to accelerate its optimization processes. Breaking the uses down into a few rough areas</p>
<ul class="simple">
<li><p>Machine learning (Backpropagation, Bayesian Inference, Uncertainty Quantification, Optimization)</p></li>
<li><p>Scientific Computing (Modeling, Simulation)</p></li>
</ul>
<p>But what are the general trends driving the continued use of automatic differentiation as compared to finite differences, or manual adjoints?</p>
<ul class="simple">
<li><p>The writing of manual derivative functions becomes intractable for large codebases or dynamically-generated programs</p></li>
<li><p>We want to be able to automatically generate our derivatives</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\Longrightarrow \text{ Automatic Differentiation}
\]</div>
<section id="a-brief-incomplete-history">
<h2><span class="section-number">8.1. </span>A Brief Incomplete History<a class="headerlink" href="#a-brief-incomplete-history" title="Permalink to this headline">#</a></h2>
<ol class="simple">
<li><p>1980s/1990s: Automatic Differentiation in Scientific Computing mostly spearheaded by Griewank, Walther, and Pearlmutter</p>
<ul class="simple">
<li><p>Adifor</p></li>
<li><p>Adol-C</p></li>
<li><p>…</p></li>
</ul>
</li>
<li><p>2000s: Rise of Python begins</p></li>
<li><p>2015: Autograd for the automatic differentiation of Python &amp; NumPy is released</p></li>
<li><p>2016/2017: PyTorch &amp; Tensorflow are introduced with automatic differentiation at their core</p></li>
<li><p>2018: JAX is introduced with its very thin Python layer on top of Tensorflow’s compilation stack, where it performs automatic differentiation on the highest representation level</p></li>
<li><p>2020-2022: Forward-mode estimators to replace the costly and difficult-to-implement backpropagation are being introduced</p></li>
</ol>
<p>with the cost of machine learning training dominating datacenter-bills for many companies and startups alike there exist many alternative approaches out there to replace gradients, but none of them have gained significant traction so far. <strong>But it is definitely an area to keep an eye out for.</strong></p>
</section>
<section id="the-tl-dr-of-gradients">
<h2><span class="section-number">8.2. </span>The tl;dr of Gradients<a class="headerlink" href="#the-tl-dr-of-gradients" title="Permalink to this headline">#</a></h2>
<p>Giving a brief overview of the two modes, with derivations of the properties as well as examples following later.</p>
<section id="forward-mode-differentiation">
<h3><span class="section-number">8.2.1. </span>Forward-Mode Differentiation<a class="headerlink" href="#forward-mode-differentiation" title="Permalink to this headline">#</a></h3>
<p>Examining a classical derivative computation</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial c} \left( \frac{\partial c}{\partial b} \left( \frac{\partial b}{\partial a} \frac{\partial a}{\partial x} \right) \right)
\]</div>
<p>then in the case of the forward-mode derivative the evaluation of the gradient is performed from the right to the left. The Jacobian of the intermediate values is then accumulated with respect to the input x</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial a}{\partial x}, \quad \frac{\partial b}{\partial x}
\]</div>
<p>and the information flows in the same direction as the computation. This means that we do not require any elaborate caching system to hold values in-memory for later use in the computation of a gradient, and hence require <strong>much less memory</strong> and are left with a <strong>much simpler algorithm</strong>.</p>
<div>
    <center>
    <img = src="https://i.imgur.com/0ExLlWa.png" width="450">
</div>
</section>
<section id="reverse-mode-differentiation-backpropagation">
<h3><span class="section-number">8.2.2. </span>Reverse-Mode Differentiation (Backpropagation)<a class="headerlink" href="#reverse-mode-differentiation-backpropagation" title="Permalink to this headline">#</a></h3>
<p>Taking a typical case of reverse-mode differentiation, or as it is called in machine learning “backpropagation”</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial y}{\partial x} = \left( \left( \frac{\partial y}{\partial c} \frac{\partial c}{\partial b} \right) \frac{\partial b}{\partial a} \right) \frac{\partial a}{\partial x}
\]</div>
<p>in the case of reverse-mode differentiation the evaluation of the gradient is then performed from the left to the right. The Jacobians of the output <span class="math notranslate nohighlight">\(y\)</span> are then accumulated with respect to each of the intermediate variables</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial y}{\partial a}, \quad \frac{\partial y}{\partial b}
\]</div>
<p>and the information flows in the opposite direction of the function evaluation, which points to the main difficulty of reverse-mode differentiation. We require an elaborate caching system to hold values in-memory for when they are needed for the gradient computation, and hence require <strong>much more memory</strong> and are left with a <strong>much more difficult algorithm</strong>.</p>
<div>
    <center>
    <img = src="https://i.imgur.com/3H2gZDl.png" width="450">
</div>
</section>
<section id="forward-vs-reverse-mode">
<h3><span class="section-number">8.2.3. </span>Forward- vs. Reverse-Mode<a class="headerlink" href="#forward-vs-reverse-mode" title="Permalink to this headline">#</a></h3>
<p>The performance comparison between forward-mode, and reverse-mode gradient can be broken down depending on the size of our input vector, and our output vector. So for the case of abstracting our neural network as a function with takes an input vector of a certain size <span class="math notranslate nohighlight">\(n\)</span>, and generates and output vector of a certain size <span class="math notranslate nohighlight">\(m\)</span></p>
<div class="math notranslate nohighlight">
\[
f: \mathbb{R}^{n} \longrightarrow \mathbb{R}^{m}
\]</div>
<ul class="simple">
<li><p>Forward-mode: More efficient for gradients of scalar-to-vector functions, i.e. <span class="math notranslate nohighlight">\(m &gt;&gt; n\)</span></p></li>
<li><p>Reverse-mode: More efficient for gradients of vector-to-scalar functions, i.e. <span class="math notranslate nohighlight">\(m &lt;&lt; n\)</span></p></li>
</ul>
<p>As most loss functions in machine learning output a scalar value, reverse-mode differentiation is a very natural choice for these computations. A way to circumvent these issues of forward-mode differentiation, and simplify the technical infrastructure in the background is to <strong>compose</strong> forward-mode with vectorization, or only compute an estimator of the gradient where multiple forward-mode samples are used.</p>
</section>
</section>
<section id="in-depth-look">
<h2><span class="section-number">8.3. </span>In-Depth Look<a class="headerlink" href="#in-depth-look" title="Permalink to this headline">#</a></h2>
<div class="math notranslate nohighlight">
\[
f = f_{4} \circ f_{3} \circ f_{2} \circ f_{1}
\]</div>
<p>where we are essentially converting from space-to-space with each function. Each function is an abstraction for an individual neural network layer as we will see in the new year in much more depth when constructing neural networks, or in the practical example later on.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    f_{1}: \mathbb{R}^{n} &amp;\longrightarrow \mathbb{R}^{m_{1}} \\
    f_{2}: \mathbb{R}^{m_{1}} &amp;\longrightarrow \mathbb{R}^{m_{2}} \\
    f_{3}: \mathbb{R}^{m_{2}} &amp;\longrightarrow \mathbb{R}^{m_{3}} \\
    f_{4}: \mathbb{R}^{m_{3}} &amp;\longrightarrow \mathbb{R}^{m} \\
\end{align}
\end{split}\]</div>
<p>where our overall network <span class="math notranslate nohighlight">\(o = f(x)\)</span> is broken down as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    x_{2} &amp;= f_{1}(x) \\
    x_{3} &amp;= f_{2}(x_{2}) \\
    x_{4} &amp;= f_{3}(x_{3}) \\
    o     &amp;= f_{4}(x_{4})
\end{align}
\end{split}\]</div>
<p>using the chain rule we can then compute the Jacobian <span class="math notranslate nohighlight">\(J_{f}(x) = \frac{\partial o}{\partial x}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\frac{\partial o}{\partial x} &amp;= \frac{\partial o}{\partial x_{4}} \frac{\partial x_{4}}{\partial x_{3}} \frac{\partial x_{3}}{\partial x_{2}} \frac{\partial x_{2}}{\partial x} \\
&amp;= \frac{\partial f_{4}(x_{4})}{\partial x_{4}} \frac{\partial f_{3}(x_{3})}{\partial x_{3}} \frac{\partial f_{2}(x_{2})}{\partial x_{2}} \frac{\partial f_{1}(x)}{\partial x} \\
&amp;= J_{f_{4}}(x_{4}) J_{f_{3}}(x_{3}) J_{f_{2}}(x_{2}) J_{f_{1}}(x)
\end{align}
\end{split}\]</div>
<p>this approach to this computation would be highly inefficient, as such we rely on matrix computation for more efficiency, i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{f}(x) = \frac{\partial f(x)}{\partial x} = \left(\begin{matrix}
    \frac{\partial f_{1}}{\partial x_{1}} &amp; \ldots &amp; \frac{\partial f_{1}}{\partial x_{n}} \\
    \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial f_{m}}{\partial x_{1}} &amp; \ldots &amp; \frac{\partial f_{m}}{\partial x_{n}}
\end{matrix} \right) = \left( \begin{matrix}
    \nabla f_{1}(x)^{\top} \\
    \vdots \\
    \nabla f_{m}(x)^{\top}
\end{matrix}\right) = \left( \begin{matrix}
    \frac{\partial f}{\partial x_{1}}, &amp; \ldots &amp; , \frac{\partial f}{\partial x_{n}}
\end{matrix}\right)
\end{split}\]</div>
<p>in practice we would <strong>love to</strong> have access to this Jacobian, but the reality is that in 99.99999% of the cases it is too expensive to compute and as such we have to make do with snippets from this Jacobian, namely the <strong>Jacobian Vector Product (JVP)</strong>, and the <strong>Vector Jacobian Product (VJP)</strong>.</p>
<ul class="simple">
<li><p>The i-th row of <span class="math notranslate nohighlight">\(J_{f}(x)\)</span> gives us the vector Jacobian product (reverse-mode differentiation)</p></li>
<li><p>The j-th column of <span class="math notranslate nohighlight">\(J_{f}(x)\)</span> gives us the Jacobian vector product (forward-mode differentiation)</p></li>
</ul>
<p>Examining the case for when <span class="math notranslate nohighlight">\(n&lt;m\)</span>, then it is more efficient to compute each column using the jacobian vector product in a right-to-left manner, i.e. the right multiplication a column vector gives us</p>
<div class="math notranslate nohighlight">
\[
J_{f}(x) v = \underbrace{J_{f_{4}}(x_{4})}_{m \times m_{3}} \underbrace{J_{f_{3}}(x_{3})}_{m_{3} \times m_{2}} \underbrace{J_{f_{2}}(x_{2})}_{m_{2} \times m_{1}} \underbrace{J_{f_{1}}(x_{1})}_{m_{1} \times n} \underbrace{v}_{n \times 1}
\]</div>
<p>which is then computed with forward-mode differentiation. The pseudoalgorithm is given below.</p>
<div>
    <center>
    <img = src="https://i.imgur.com/zun2BoB.png" width="450">
</div>
<p>Returning to the cost-advantage of forward-mode differentiation for this specific case the cost of computation in this case if <span class="math notranslate nohighlight">\(\mathcal{O}(n^{3})\)</span>. If we now have the case where <span class="math notranslate nohighlight">\(n&gt;m\)</span>, then it is more efficient to compute <span class="math notranslate nohighlight">\(J_{f}(x)\)</span> for each row using the vector Jacobian product (VJP) in a left-to-right manner, i.e.</p>
<div class="math notranslate nohighlight">
\[
u^{\top} J_{f}(x) = \underbrace{u^{\top}}_{1 \times m} \underbrace{J_{f_{4}}(x_{4})}_{m \times m_{3}} \underbrace{J_{f_{3}}(x_{3})}_{m_{3} \times m_{2}} \underbrace{J_{f_{2}}(x_{2})}_{m_{2} \times m_{1}} \underbrace{J_{f_{1}}(x_{1})}_{m_{1} \times n}
\]</div>
<p>for the solving of which reverse-mode differentiation is the most well-suited. The pseudoalgorithm for which can be found below</p>
<div>
    <center>
    <img = src="https://i.imgur.com/W4rgJyk.png" width="450">
</div>
<p>The cost of computation in this case is <span class="math notranslate nohighlight">\(\mathcal{O}(n^{2})\)</span>.</p>
</section>
<section id="a-practical-example">
<h2><span class="section-number">8.4. </span>A Practical Example<a class="headerlink" href="#a-practical-example" title="Permalink to this headline">#</a></h2>
<p>Considering a simple feed-forward model with 4 layers / an MLP with one hidden layer, we now have the following computation setup represented as a directed acyclic graph:</p>
<div>
    <center>
    <img src="https://i.imgur.com/aMbZOqb.png" width="450">
</div>
<p>The MLP with one hidden layer is written down as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\left( (x, y), \theta \right) = \frac{1}{2} || y - W_{2} \varphi(W_{1}x) ||^{2}_{2}
\]</div>
<p>which is then represented as the following feedforward model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \mathcal{L} &amp;= f_{4} \circ f_{3} \circ f_{2} \circ f_{1} \\
    x_{2} &amp;= f_{1}(x, \theta_{1}) = W_{1}x \\
    x_{3} &amp;= f_{2}(x_{2}, \emptyset) = \varphi(x_{2}) \\
    x_{4} &amp;= f_{3}(x_{3}, \theta_{3}) = W_{2}x_{3} \\
    \mathcal{L} &amp;= f_{4}(x_{4}, y) = \frac{1}{2} || x_{4} - y ||^{2}
\end{align}
\end{split}\]</div>
<p>the <span class="math notranslate nohighlight">\(\theta_{k}\)</span> are the optional parameters for each layer. As, by construction, the final layer returns a scalar, it is much more efficient to use reverse-mode differentiation to compute the gradient vectors in this case. We begin by computing the gradients of the loss with respect to the parameters in the earlier layers</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \theta_{3}} &amp;= \frac{\partial \mathcal{L}}{\partial x_{4}} \frac{\partial x_{4}}{\partial \theta_{3}} \\
    \frac{\partial \mathcal{L}}{\partial \theta_{2}} &amp;= \frac{\partial \mathcal{L}}{\partial x_{4}} \frac{\partial x_{4}}{\partial x_{3}} \frac{\partial x_{3}}{\partial \theta_{2}} \\
    \frac{\partial \mathcal{L}}{\partial \theta_{1}} &amp;= \frac{\partial \mathcal{L}}{\partial x_{4}} \frac{\partial x_{4}}{\partial x_{3}} \frac{\partial x_{3}}{\partial x_{2}} \frac{\partial x_{2}}{\partial \theta_{1}}
\end{align}
\end{split}\]</div>
<p>this recursive computation procedure can subsequently be condensed down to a pseudoalgorithm:</p>
<div>
    <center>
    <img src="https://i.imgur.com/4jWskvp.png" width="450">
</div>
<p>what is missing from this pseudoalgorithm is the definition of the vector Jacobian product of each layer, which depends on the type and function of each layer. Or in a slightly more intricate case, please see the example below for what this computation looks like in the case of backpropagation.</p>
<div>
    <center>
    <img src="https://i.imgur.com/meT1rHZ.png" width="450">
</div>
</section>
<section id="what-are-the-core-levers-of-the-alternative-approaches">
<h2><span class="section-number">8.5. </span>What are the Core-Levers of the Alternative Approaches<a class="headerlink" href="#what-are-the-core-levers-of-the-alternative-approaches" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Do we actually need accurate gradients for the training, or can we actually get away with much much coarser gradients to power our training?</p></li>
<li><p>Approximate the reverse-mode gradients with a construction of cheap forward-mode gradients</p>
<ul>
<li><p>By construction of a Monte-Carlo estimator for the reverse-mode gradient using forward-mode gradient samples</p></li>
<li><p>Randomizing the forward-mode gradients and then constructing an estimator</p></li>
</ul>
</li>
<li><p>Taking gradients at different program abstraction levels, taking the example of JAX we have access to the following main program abstraction levels at which gradients can be computed</p>
<ul>
<li><p>Python frontend</p></li>
<li><p>Jaxpr</p></li>
<li><p>MHLO</p></li>
<li><p>XLA</p></li>
</ul>
</li>
</ul>
</section>
<section id="further-references">
<h2><span class="section-number">8.6. </span>Further References<a class="headerlink" href="#further-references" title="Permalink to this headline">#</a></h2>
<ol class="simple">
<li><p><a class="reference external" href="https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf">Autograd: Effortless Gradients in Numpy</a></p></li>
<li><p><a class="reference external" href="https://openreview.net/pdf?id=BJJsrmfCZ">Automatic Differentiation in PyTorch</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1711.02712.pdf">Tangent: Automatic Differentiation Using Source Code Transformation in Python</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1502.05767.pdf">Automatic Differentiation in Machine Learning: A Survey</a></p></li>
<li><p>Chapter 4 Dougal MacLaurin’s <a class="reference external" href="https://dougalmaclaurin.com/phd-thesis.pdf">PhD Thesis</a></p></li>
<li><p>A Playful Introduction to Gradients: <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html">Jax’s Autodiff Cookbook</a></p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="gp.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7. </span>Gaussian Processes</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="mlp.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Multilayer Perceptron</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By N. Adams, L. Paehler, A. Toshev<br/>
  
      &copy; Copyright 2022,2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>