

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>8. Gradients &#8212; Introduction to Scientific Machine Learning for Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture/gradients';</script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. Multilayer Perceptron" href="mlp.html" />
    <link rel="prev" title="7. Gaussian Processes" href="gp.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../about.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Scientific Machine Learning for Engineers - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Scientific Machine Learning for Engineers - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="linear.html">1. Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="gmm.html">2. Gaussian Mixture Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes.html">3. Bayesian methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">4. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tricks.html">5. Tricks of Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svm.html">6. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">7. Gaussian Processes</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlp.html">9. Multilayer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">10. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html">11. Recurrent Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="ae.html">12. Encoder-Decoder Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercise</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/linear.html">1. Linear and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/bayes.html">2. Bayesian Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/optimization.html">3. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/svm.html">4. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/gp.html">5. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/cnn.html">6. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/rnn.html">7. Recurrent Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../admin.html">Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preliminary_knowledge.html">Preliminary Knowledge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software.html">Software Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../practical_exam.html">Practical Exam WS22/23</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tumaer/SciML" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tumaer/SciML/issues/new?title=Issue%20on%20page%20%2Flecture/gradients.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lecture/gradients.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gradients</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-brief-incomplete-history">8.1. A Brief Incomplete History</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-tl-dr-of-gradients">8.2. The tl;dr of Gradients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-mode-differentiation">8.2.1. Forward-Mode Differentiation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reverse-mode-differentiation-backpropagation">8.2.2. Reverse-Mode Differentiation (Backpropagation)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-vs-reverse-mode">8.2.3. Forward- vs. Reverse-Mode</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#in-depth-look">8.3. In-Depth Look</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-practical-example">8.4. A Practical Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-core-levers-of-the-alternative-approaches">8.5. What are the Core-Levers of the Alternative Approaches</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">8.6. Further References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gradients">
<h1><span class="section-number">8. </span>Gradients<a class="headerlink" href="#gradients" title="Permalink to this heading">#</a></h1>
<p>Gradients are a general tool of utility across many scientific domains and keep reappearing across areas. Machine learning is just one of a much larger group of examples which utilizes gradients to accelerate its optimization processes. Breaking the uses down into a few rough areas, we have:</p>
<ul class="simple">
<li><p>Machine learning (Backpropagation, Bayesian Inference, Uncertainty Quantification, Optimization)</p></li>
<li><p>Scientific Computing (Modeling, Simulation)</p></li>
</ul>
<p>But what are the general trends driving the continued use of automatic differentiation as compared to finite differences, or manual adjoints?</p>
<ul class="simple">
<li><p>The writing of manual derivative functions becomes intractable for large codebases or dynamically-generated programs</p></li>
<li><p>We want to be able to automatically generate our derivatives</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\Longrightarrow \text{ Automatic Differentiation}
\]</div>
<figure class="align-center" id="mlp-2">
<a class="reference internal image-reference" href="../_images/mlp.png"><img alt="../_images/mlp.png" src="../_images/mlp.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">Multilayer perceptron (Source: <a class="reference external" href="https://towardsdatascience.com/techniques-for-handling-underfitting-and-overfitting-in-machine-learning-348daa2380b9">Techniques for handling underfitting and overfitting in Machine Learning</a>)</span><a class="headerlink" href="#mlp-2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="a-brief-incomplete-history">
<h2><span class="section-number">8.1. </span>A Brief Incomplete History<a class="headerlink" href="#a-brief-incomplete-history" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>1980s/1990s: Automatic Differentiation in Scientific Computing mostly spearheaded by Griewank, Walther, and Pearlmutter</p>
<ul class="simple">
<li><p>Adifor</p></li>
<li><p>Adol-C</p></li>
<li><p>…</p></li>
</ul>
</li>
<li><p>2000s: Rise of Python begins</p></li>
<li><p>2015: Autograd for the automatic differentiation of Python &amp; NumPy is released</p></li>
<li><p>2016/2017: PyTorch &amp; Tensorflow are introduced with automatic differentiation at their core</p></li>
<li><p>2018: JAX is introduced with its very thin Python layer on top of Tensorflow’s compilation stack, where it performs automatic differentiation on the highest representation level</p></li>
<li><p>2020-2022: Forward-mode estimators to replace the costly and difficult-to-implement backpropagation are being introduced</p></li>
</ol>
<p>With the cost of machine learning training dominating datacenter-bills for many companies and startups alike there exist many alternative approaches out there to replace gradients, but none of them have gained significant traction so far. <strong>But it is definitely an area to keep an eye out for.</strong></p>
</section>
<section id="the-tl-dr-of-gradients">
<h2><span class="section-number">8.2. </span>The tl;dr of Gradients<a class="headerlink" href="#the-tl-dr-of-gradients" title="Permalink to this heading">#</a></h2>
<p>Giving a brief overview of the two modes, with derivations of the properties as well as examples following later.</p>
<section id="forward-mode-differentiation">
<h3><span class="section-number">8.2.1. </span>Forward-Mode Differentiation<a class="headerlink" href="#forward-mode-differentiation" title="Permalink to this heading">#</a></h3>
<p>Examining a classical derivative computation</p>
<div class="math notranslate nohighlight" id="equation-ad-forward-chain">
<span class="eqno">(8.1)<a class="headerlink" href="#equation-ad-forward-chain" title="Permalink to this equation">#</a></span>\[
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial c} \left( \frac{\partial c}{\partial b} \left( \frac{\partial b}{\partial a} \frac{\partial a}{\partial x} \right) \right)
\]</div>
<p>then in the case of the forward-mode derivative the evaluation of the gradient is performed from the right to the left. The Jacobian of the intermediate values is then accumulated with respect to the input <span class="math notranslate nohighlight">\(x\)</span></p>
<div class="math notranslate nohighlight" id="equation-ad-forward-order">
<span class="eqno">(8.2)<a class="headerlink" href="#equation-ad-forward-order" title="Permalink to this equation">#</a></span>\[
\frac{\partial a}{\partial x}, \quad \frac{\partial b}{\partial x}
\]</div>
<p>and the information flows in the same direction as the computation. This means that we do not require any elaborate caching system to hold values in-memory for later use in the computation of a gradient, and hence require <strong>much less memory</strong> and are left with a <strong>much simpler algorithm</strong>.</p>
<figure class="align-center" id="gradients-forward">
<a class="reference internal image-reference" href="../_images/gradients_forward.png"><img alt="../_images/gradients_forward.png" src="../_images/gradients_forward.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text">Forward-mode differentiation. (Source: <span id="id1">[<a class="reference internal" href="../references.html#id19" title="Dougal Maclaurin. Modeling, inference and optimization with composable differentiable procedures. 2016. URL: https://dougalmaclaurin.com/phd-thesis.pdf.">Maclaurin, 2016</a>]</span>, Section 2)</span><a class="headerlink" href="#gradients-forward" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="reverse-mode-differentiation-backpropagation">
<h3><span class="section-number">8.2.2. </span>Reverse-Mode Differentiation (Backpropagation)<a class="headerlink" href="#reverse-mode-differentiation-backpropagation" title="Permalink to this heading">#</a></h3>
<p>Taking a typical case of reverse-mode differentiation, or as it is called in machine learning “backpropagation”</p>
<div class="math notranslate nohighlight" id="equation-ad-reverse-chain">
<span class="eqno">(8.3)<a class="headerlink" href="#equation-ad-reverse-chain" title="Permalink to this equation">#</a></span>\[
\frac{\partial y}{\partial x} = \left( \left( \frac{\partial y}{\partial c} \frac{\partial c}{\partial b} \right) \frac{\partial b}{\partial a} \right) \frac{\partial a}{\partial x}.
\]</div>
<p>In the case of reverse-mode differentiation the evaluation of the gradient is then performed from the left to the right. The Jacobians of the output <span class="math notranslate nohighlight">\(y\)</span> are then accumulated with respect to each of the intermediate variables</p>
<div class="math notranslate nohighlight" id="equation-ad-reveerse-order">
<span class="eqno">(8.4)<a class="headerlink" href="#equation-ad-reveerse-order" title="Permalink to this equation">#</a></span>\[
\frac{\partial y}{\partial a}, \quad \frac{\partial y}{\partial b}
\]</div>
<p>and the information flows in the opposite direction of the function evaluation, which points to the main difficulty of reverse-mode differentiation. We require an elaborate caching system to hold values in-memory for when they are needed for the gradient computation, and hence require <strong>much more memory</strong> and are left with a <strong>much more difficult algorithm</strong>.</p>
<figure class="align-center" id="gradients-reverse">
<a class="reference internal image-reference" href="../_images/gradients_reverse.png"><img alt="../_images/gradients_reverse.png" src="../_images/gradients_reverse.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">Reverse-mode differentiation. (Source: <span id="id2">[<a class="reference internal" href="../references.html#id19" title="Dougal Maclaurin. Modeling, inference and optimization with composable differentiable procedures. 2016. URL: https://dougalmaclaurin.com/phd-thesis.pdf.">Maclaurin, 2016</a>]</span>, Section 2)</span><a class="headerlink" href="#gradients-reverse" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="forward-vs-reverse-mode">
<h3><span class="section-number">8.2.3. </span>Forward- vs. Reverse-Mode<a class="headerlink" href="#forward-vs-reverse-mode" title="Permalink to this heading">#</a></h3>
<p>The performance comparison between forward-mode and reverse-mode gradient can be broken down depending on the size of our input vector and our output vector. So for the case of abstracting our neural network as a function which takes an input vector of a certain size <span class="math notranslate nohighlight">\(n\)</span>, and generates and output vector of a certain size <span class="math notranslate nohighlight">\(m\)</span></p>
<div class="math notranslate nohighlight" id="equation-ad-f-map">
<span class="eqno">(8.5)<a class="headerlink" href="#equation-ad-f-map" title="Permalink to this equation">#</a></span>\[
f: \mathbb{R}^{n} \longrightarrow \mathbb{R}^{m}
\]</div>
<ul class="simple">
<li><p>Forward-mode: More efficient for gradients of scalar-to-vector functions, i.e. <span class="math notranslate nohighlight">\(m &gt;&gt; n\)</span></p></li>
<li><p>Reverse-mode: More efficient for gradients of vector-to-scalar functions, i.e. <span class="math notranslate nohighlight">\(m &lt;&lt; n\)</span></p></li>
</ul>
<p>As most loss functions in machine learning output a scalar value, reverse-mode differentiation is a very natural choice for these computations. A way to circumvent these issues of forward-mode differentiation, and simplify the technical infrastructure in the background, is to <strong>compose</strong> forward-mode with vectorization, or only compute an estimator of the gradient where multiple forward-mode samples are used.</p>
</section>
</section>
<section id="in-depth-look">
<h2><span class="section-number">8.3. </span>In-Depth Look<a class="headerlink" href="#in-depth-look" title="Permalink to this heading">#</a></h2>
<p>We decompose the function <span class="math notranslate nohighlight">\(f\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-ad-f-decomp">
<span class="eqno">(8.6)<a class="headerlink" href="#equation-ad-f-decomp" title="Permalink to this equation">#</a></span>\[
f = f_{4} \circ f_{3} \circ f_{2} \circ f_{1},
\]</div>
<p>where we are essentially converting from space-to-space with each function. Each function is an abstraction for an individual neural network layer as we will see in much more depth when constructing neural networks in the upcoming lectures, or in the exercises later on.</p>
<div class="math notranslate nohighlight" id="equation-ad-f-submaps">
<span class="eqno">(8.7)<a class="headerlink" href="#equation-ad-f-submaps" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    f_{1}: \mathbb{R}^{n} &amp;\longrightarrow \mathbb{R}^{m_{1}} \\
    f_{2}: \mathbb{R}^{m_{1}} &amp;\longrightarrow \mathbb{R}^{m_{2}} \\
    f_{3}: \mathbb{R}^{m_{2}} &amp;\longrightarrow \mathbb{R}^{m_{3}} \\
    f_{4}: \mathbb{R}^{m_{3}} &amp;\longrightarrow \mathbb{R}^{m} \\
\end{align}
\end{split}\]</div>
<p>where our overall network <span class="math notranslate nohighlight">\(o = f(x)\)</span> is broken down as</p>
<div class="math notranslate nohighlight" id="equation-ad-f-intermediates">
<span class="eqno">(8.8)<a class="headerlink" href="#equation-ad-f-intermediates" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    x_{2} &amp;= f_{1}(x) \\
    x_{3} &amp;= f_{2}(x_{2}) \\
    x_{4} &amp;= f_{3}(x_{3}) \\
    o     &amp;= f_{4}(x_{4}).
\end{align}
\end{split}\]</div>
<p>Using the chain rule we can then compute the Jacobian <span class="math notranslate nohighlight">\(J_{f}(x) = \frac{\partial o}{\partial x}\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-ad-grad-chain">
<span class="eqno">(8.9)<a class="headerlink" href="#equation-ad-grad-chain" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
\frac{\partial o}{\partial x} &amp;= \frac{\partial o}{\partial x_{4}} \frac{\partial x_{4}}{\partial x_{3}} \frac{\partial x_{3}}{\partial x_{2}} \frac{\partial x_{2}}{\partial x} \\
 &amp;= \frac{\partial f_{4}(x_{4})}{\partial x_{4}} \frac{\partial f_{3}(x_{3})}{\partial x_{3}} \frac{\partial f_{2}(x_{2})}{\partial x_{2}} \frac{\partial f_{1}(x)}{\partial x} \\
 &amp;= J_{f_{4}}(x_{4}) J_{f_{3}}(x_{3}) J_{f_{2}}(x_{2}) J_{f_{1}}(x).
\end{align}
\end{split}\]</div>
<p>This approach to the computation would be highly inefficient. As such we rely on matrix computation for more efficiency, i.e.</p>
<div class="math notranslate nohighlight" id="equation-jacobian-splits">
<span class="eqno">(8.10)<a class="headerlink" href="#equation-jacobian-splits" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
J_{f}(x) = \frac{\partial f(x)}{\partial x} &amp;= \left(\begin{matrix}
    \frac{\partial f_{1}}{\partial x_{1}} &amp; \ldots &amp; \frac{\partial f_{1}}{\partial x_{n}} \\
    \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial f_{m}}{\partial x_{1}} &amp; \ldots &amp; \frac{\partial f_{m}}{\partial x_{n}}
\end{matrix} \right) \\ 
 &amp;= \left( \begin{matrix}
    \nabla f_{1}(x)^{\top} \\
    \vdots \\
    \nabla f_{m}(x)^{\top}
\end{matrix}\right) \\
 &amp;= \left( \begin{matrix}
    \frac{\partial f}{\partial x_{1}}, &amp; \ldots &amp; , \frac{\partial f}{\partial x_{n}}
\end{matrix}\right).
\end{align}
\end{split}\]</div>
<p>In practice we would <strong>love to</strong> have access to this Jacobian, but the reality is that in 99.99% of the cases it is too expensive to compute and as such we have to make do with snippets from this Jacobian, namely the <strong>Jacobian Vector Product (JVP)</strong>, and the <strong>Vector Jacobian Product (VJP)</strong>.</p>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(J_{f}(x)\)</span> gives us the vector Jacobian product (reverse-mode differentiation)</p></li>
<li><p>The <span class="math notranslate nohighlight">\(j\)</span>-th column of <span class="math notranslate nohighlight">\(J_{f}(x)\)</span> gives us the Jacobian vector product (forward-mode differentiation)</p></li>
</ul>
<p>Examining the case for when <span class="math notranslate nohighlight">\(n&lt;m\)</span>, then it is more efficient to compute each column using the jacobian vector product in a right-to-left manner, i.e. the right multiplication a column vector gives us</p>
<div class="math notranslate nohighlight" id="equation-jvp-chain">
<span class="eqno">(8.11)<a class="headerlink" href="#equation-jvp-chain" title="Permalink to this equation">#</a></span>\[
J_{f}(x) v = \underbrace{J_{f_{4}}(x_{4})}_{m \times m_{3}} \underbrace{J_{f_{3}}(x_{3})}_{m_{3} \times m_{2}} \underbrace{J_{f_{2}}(x_{2})}_{m_{2} \times m_{1}} \underbrace{J_{f_{1}}(x_{1})}_{m_{1} \times n} \underbrace{v}_{n \times 1},
\]</div>
<p>which is then computed with forward-mode differentiation. The pseudoalgorithm is given below.</p>
<figure class="align-center" id="gradients-forward-alg">
<a class="reference internal image-reference" href="../_images/gradients_forward_alg.png"><img alt="../_images/gradients_forward_alg.png" src="../_images/gradients_forward_alg.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.4 </span><span class="caption-text">Forward-mode algorith.</span><a class="headerlink" href="#gradients-forward-alg" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Returning to the cost-advantage of forward-mode differentiation, in this specific case the cost of computation is <span class="math notranslate nohighlight">\(\mathcal{O}(n)\)</span>. If we now have the case where <span class="math notranslate nohighlight">\(n&gt;m\)</span>, then it is more efficient to compute <span class="math notranslate nohighlight">\(J_{f}(x)\)</span> for each row using the vector Jacobian product (VJP) in a left-to-right manner, i.e.</p>
<div class="math notranslate nohighlight" id="equation-vjp-chain">
<span class="eqno">(8.12)<a class="headerlink" href="#equation-vjp-chain" title="Permalink to this equation">#</a></span>\[
u^{\top} J_{f}(x) = \underbrace{u^{\top}}_{1 \times m} \underbrace{J_{f_{4}}(x_{4})}_{m \times m_{3}} \underbrace{J_{f_{3}}(x_{3})}_{m_{3} \times m_{2}} \underbrace{J_{f_{2}}(x_{2})}_{m_{2} \times m_{1}} \underbrace{J_{f_{1}}(x_{1})}_{m_{1} \times n}
\]</div>
<p>for the solving of which reverse-mode differentiation is the most well-suited. The pseudoalgorithm for which can be found below</p>
<figure class="align-center" id="gradients-reverse-alg">
<a class="reference internal image-reference" href="../_images/gradients_reverse_alg.png"><img alt="../_images/gradients_reverse_alg.png" src="../_images/gradients_reverse_alg.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.5 </span><span class="caption-text">Reverse-mode algorith.</span><a class="headerlink" href="#gradients-reverse-alg" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The cost of computation in this case is <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span>.</p>
</section>
<section id="a-practical-example">
<h2><span class="section-number">8.4. </span>A Practical Example<a class="headerlink" href="#a-practical-example" title="Permalink to this heading">#</a></h2>
<p>Considering a simple feed-forward model with 4 layers, we now have the following computation setup represented as a directed acyclic graph:</p>
<figure class="align-center" id="gradients-ff-example">
<a class="reference internal image-reference" href="../_images/gradients_ff_example.png"><img alt="../_images/gradients_ff_example.png" src="../_images/gradients_ff_example.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.6 </span><span class="caption-text">Compute graph of a 4-layer feed-forward network.</span><a class="headerlink" href="#gradients-ff-example" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The MLP with one hidden layer is written down as</p>
<div class="math notranslate nohighlight" id="equation-mlp-equation">
<span class="eqno">(8.13)<a class="headerlink" href="#equation-mlp-equation" title="Permalink to this equation">#</a></span>\[
\mathcal{L}\left( (x, y), \theta \right) = \frac{1}{2} || y - W_{4} \varphi_3(W_{3} \varphi_2(W_{3} \varphi_1(W_{1}x))) ||^{2}_{2},
\]</div>
<p>which is then represented as the following feedforward model:</p>
<div class="math notranslate nohighlight" id="equation-mlp-equation-splits">
<span class="eqno">(8.14)<a class="headerlink" href="#equation-mlp-equation-splits" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \mathcal{L} &amp;= f_{4} \circ f_{3} \circ f_{2} \circ f_{1} \\
    x_{2} &amp;= f_{1}(x, \theta_{1}) = \varphi_1(W_{1}x) \\
    x_{3} &amp;= f_{2}(x_{2}, \theta_{2}) = \varphi_2(W_2 x_{2}) \\
    x_{4} &amp;= f_{3}(x_{3}, \theta_{3}) = \varphi_3(W_3 x_{3}) \\
    o &amp;= f_{4}(x_{4}, \theta_{4}) = W_4 x_{4} \\
    \mathcal{L} &amp;= f_{4}(o, y) = \frac{1}{2} || o - y ||^{2}.
\end{align}
\end{split}\]</div>
<p>The <span class="math notranslate nohighlight">\(\theta_{k}\)</span> are the optional parameters for each layer. As, by construction, the final layer returns a scalar, it is much more efficient to use reverse-mode differentiation to compute the gradient vectors in this case. We begin by computing the gradients of the loss with respect to the parameters in the earlier layers</p>
<div class="math notranslate nohighlight" id="equation-mlp-grads">
<span class="eqno">(8.15)<a class="headerlink" href="#equation-mlp-grads" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{align}
    \frac{\partial \mathcal{L}}{\partial \theta_{4}} &amp;= \frac{\partial \mathcal{L}}{\partial o} \frac{\partial o}{\partial \theta_{4}} \\
    \frac{\partial \mathcal{L}}{\partial \theta_{3}} &amp;= \frac{\partial \mathcal{L}}{\partial o} \frac{\partial o}{\partial x_{4}} \frac{\partial x_{4}}{\partial \theta_{3}} \\
    \frac{\partial \mathcal{L}}{\partial \theta_{2}} &amp;= \frac{\partial \mathcal{L}}{\partial o} \frac{\partial o}{\partial x_{4}} \frac{\partial x_{4}}{\partial x_{3}} \frac{\partial x_{3}}{\partial \theta_{2}} \\
    \frac{\partial \mathcal{L}}{\partial \theta_{1}} &amp;= \frac{\partial \mathcal{L}}{\partial o} \frac{\partial o}{\partial x_{4}} \frac{\partial x_{4}}{\partial x_{3}} \frac{\partial x_{3}}{\partial x_{2}} \frac{\partial x_{2}}{\partial \theta_{1}}.
\end{align}
\end{split}\]</div>
<p>This recursive computation procedure can subsequently be condensed down to a pseudoalgorithm:</p>
<figure class="align-center" id="gradients-reverse-mlp">
<a class="reference internal image-reference" href="../_images/gradients_reverse_mlp.png"><img alt="../_images/gradients_reverse_mlp.png" src="../_images/gradients_reverse_mlp.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.7 </span><span class="caption-text">Reverse-mode differentiation through an MLP.</span><a class="headerlink" href="#gradients-reverse-mlp" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>What is missing from this pseudoalgorithm is the definition of the vector Jacobian product of each layer, which depends on the type and function of each layer. Or in a slightly more intricate case, please see the example below for what this computation looks like in the case of backpropagation.</p>
<figure class="align-center" id="gradients-ff-example2">
<a class="reference internal image-reference" href="../_images/gradients_ff_example2.png"><img alt="../_images/gradients_ff_example2.png" src="../_images/gradients_ff_example2.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.8 </span><span class="caption-text">More detailed compute graph of a feed-forward network.</span><a class="headerlink" href="#gradients-ff-example2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="what-are-the-core-levers-of-the-alternative-approaches">
<h2><span class="section-number">8.5. </span>What are the Core-Levers of the Alternative Approaches<a class="headerlink" href="#what-are-the-core-levers-of-the-alternative-approaches" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Do we actually need accurate gradients for the training, or can we actually get away with much much coarser gradients to power our training?</p></li>
<li><p>Approximate the reverse-mode gradients with a construction of cheap forward-mode gradients</p>
<ul>
<li><p>By construction of a Monte-Carlo estimator for the reverse-mode gradient using forward-mode gradient samples</p></li>
<li><p>Randomizing the forward-mode gradients and then constructing an estimator</p></li>
</ul>
</li>
<li><p>Taking gradients at different program abstraction levels. Taking the example of JAX we have access to the following main program abstraction levels at which gradients can be computed</p>
<ul>
<li><p>Python frontend</p></li>
<li><p>Jaxpr</p></li>
<li><p>MHLO</p></li>
<li><p>XLA</p></li>
</ul>
</li>
</ul>
</section>
<section id="further-references">
<h2><span class="section-number">8.6. </span>Further References<a class="headerlink" href="#further-references" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf">Autograd: Effortless Gradients in Numpy</a></p></li>
<li><p><a class="reference external" href="https://openreview.net/pdf?id=BJJsrmfCZ">Automatic Differentiation in PyTorch</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1711.02712.pdf">Tangent: Automatic Differentiation Using Source Code Transformation in Python</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1502.05767.pdf">Automatic Differentiation in Machine Learning: A Survey</a></p></li>
<li><p><span id="id3">[<a class="reference internal" href="../references.html#id19" title="Dougal Maclaurin. Modeling, inference and optimization with composable differentiable procedures. 2016. URL: https://dougalmaclaurin.com/phd-thesis.pdf.">Maclaurin, 2016</a>]</span> - Chapter 4 Dougal MacLaurin’s PhD Thesis</p></li>
<li><p>A Playful Introduction to Gradients: <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html">Jax’s Autodiff Cookbook</a></p></li>
<li><p><a class="reference external" href="https://niessner.github.io/I2DL/">I2DL lecture</a> by Matthias Niessner - Lecture on Backpropagation</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="gp.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Gaussian Processes</p>
      </div>
    </a>
    <a class="right-next"
       href="mlp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Multilayer Perceptron</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-brief-incomplete-history">8.1. A Brief Incomplete History</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-tl-dr-of-gradients">8.2. The tl;dr of Gradients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-mode-differentiation">8.2.1. Forward-Mode Differentiation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reverse-mode-differentiation-backpropagation">8.2.2. Reverse-Mode Differentiation (Backpropagation)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-vs-reverse-mode">8.2.3. Forward- vs. Reverse-Mode</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#in-depth-look">8.3. In-Depth Look</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-practical-example">8.4. A Practical Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-core-levers-of-the-alternative-approaches">8.5. What are the Core-Levers of the Alternative Approaches</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">8.6. Further References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By N. Adams, L. Paehler, A. Toshev
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022,2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>