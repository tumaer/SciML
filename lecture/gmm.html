

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2. Gaussian Mixture Models &#8212; Introduction to Scientific Machine Learning for Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture/gmm';</script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Bayesian methods" href="bayes.html" />
    <link rel="prev" title="1. Linear Models" href="linear.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../about.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Scientific Machine Learning for Engineers - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Scientific Machine Learning for Engineers - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="linear.html">1. Linear Models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">2. Gaussian Mixture Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes.html">3. Bayesian methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">4. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tricks.html">5. Tricks of Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svm.html">6. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">7. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients.html">8. Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlp.html">9. Multilayer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">10. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html">11. Recurrent Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="ae.html">12. Encoder-Decoder Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercise</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/linear.html">1. Linear and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/bayes.html">2. Bayesian Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/optimization.html">3. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/svm.html">4. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/gp.html">5. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/cnn.html">6. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/rnn.html">7. Recurrent Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../admin.html">Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preliminary_knowledge.html">Preliminary Knowledge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software.html">Software Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../practical_exam.html">Practical Exam WS22/23</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tumaer/SciML" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tumaer/SciML/issues/new?title=Issue%20on%20page%20%2Flecture/gmm.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lecture/gmm.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gaussian Mixture Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-theory">2.1. Probability Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-building-blocks">2.1.1. Basic Building Blocks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables-and-their-properties">2.1.2. Random Variables and Their Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#catalogue-of-important-distributions">2.1.3. Catalogue of Important Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-family">2.1.4. Exponential Family</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.2. Gaussian Mixture Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization">2.2.1. Expectation-Maximization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-limitations">2.2.2. Applications and Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">2.3. Further References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2.3.1. Probability Theory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">2.3.2. Gaussian Mixture Models</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gaussian-mixture-models">
<h1><span class="section-number">2. </span>Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Permalink to this heading">#</a></h1>
<div class="tip admonition">
<p class="admonition-title">Learning outcome</p>
<ul class="simple">
<li><p>Explain the product/sum rule of probabilities and Bayes rule.</p></li>
<li><p>Explain the assumption behind GMMs and point to the respective terms in the GMM equation.</p></li>
<li><p>Give an intuition on why GMMs do not have a closed-form solution and sketch the steps of the most commonly used optimization method for GMMs.</p></li>
</ul>
</div>
<p>This lecture first recaps Probability Theory and then introduces Gaussian Mixture Models (GMM) for density estimation and clustering.</p>
<p>Concerning the following lecture introducing sampling, GMMs and sampling methods (e.g. MCMC) are two complementary approaches:</p>
<ul class="simple">
<li><p>GMMs estimate the probability density of a given set of samples</p></li>
<li><p>MCMC generates samples from a given probability density</p></li>
</ul>
<figure class="align-center" id="density-estim-vs-sampling">
<a class="reference internal image-reference" href="../_images/density_estimation_vs_sampling.png"><img alt="../_images/density_estimation_vs_sampling.png" src="../_images/density_estimation_vs_sampling.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.1 </span><span class="caption-text">Density estimation vs sampling.</span><a class="headerlink" href="#density-estim-vs-sampling" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>But first, we revise Probability Theory.</p>
<blockquote>
<div><p>Note: To refresh your math knowledge, we have prepared a set of exercises incl. solutions under <a class="reference internal" href="../preliminary_knowledge.html"><span class="doc std std-doc">Preliminary Knowledge</span></a>.</p>
</div></blockquote>
<section id="probability-theory">
<h2><span class="section-number">2.1. </span>Probability Theory<a class="headerlink" href="#probability-theory" title="Permalink to this heading">#</a></h2>
<section id="basic-building-blocks">
<h3><span class="section-number">2.1.1. </span>Basic Building Blocks<a class="headerlink" href="#basic-building-blocks" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><span class="math notranslate nohighlight">\(\Omega\)</span> - <em>sample space</em>; the set of all outcomes of a random experiment.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(E)\)</span> - <em>probability measure of an event <span class="math notranslate nohighlight">\(E \in \Omega\)</span></em>; a function <span class="math notranslate nohighlight">\(\mathbb{P}: \Omega \rightarrow \mathbb{R}\)</span>  satisfies the following three properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(0 \le \mathbb{P}(E) \le 1 \quad \forall E \in \Omega\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(\Omega)=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(\cup_{i=1}^n E_i) = \sum_{i=1}^n \mathbb{P}(E_i) \;\)</span> for disjoint events <span class="math notranslate nohighlight">\({E_1, ..., E_n}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(A, B)\)</span> - <em>joint probability</em>; probability that both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> occur simultaneously.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(A | B)\)</span> - <em>conditional probability</em>; probability that <span class="math notranslate nohighlight">\(A\)</span> occurs, if <span class="math notranslate nohighlight">\(B\)</span> has occurred.</p></li>
<li><p>Product rule of probabilities:</p>
<ul>
<li><p>general case:</p>
<div class="math notranslate nohighlight" id="equation-product-rule-general">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-product-rule-general" title="Permalink to this equation">#</a></span>\[\mathbb{P}(A, B) = \mathbb{P}(A | B)\cdot  \mathbb{P}(B) = \mathbb{P}(B | A) \cdot \mathbb{P}(A)\]</div>
</li>
<li><p>independent events:</p>
<div class="math notranslate nohighlight" id="equation-product-rule-indep">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-product-rule-indep" title="Permalink to this equation">#</a></span>\[\mathbb{P}(A, B) = \mathbb{P}(A) \cdot \mathbb{P}(B)\]</div>
</li>
</ul>
</li>
<li><p>Sum rule of probabilities:</p>
<div class="math notranslate nohighlight" id="equation-sum-rule">
<span class="eqno">(2.3)<a class="headerlink" href="#equation-sum-rule" title="Permalink to this equation">#</a></span>\[\mathbb{P}(A)=\sum_{B}\mathbb{P}(A, B)\]</div>
</li>
<li><p>Bayes rule: solving the general case of the product rule for <span class="math notranslate nohighlight">\(\mathbb{P}(A)\)</span> results in:</p>
<div class="math notranslate nohighlight" id="equation-bayes-rule">
<span class="eqno">(2.4)<a class="headerlink" href="#equation-bayes-rule" title="Permalink to this equation">#</a></span>\[ \mathbb{P}(B|A) = \frac{\mathbb{P}(A|B) \mathbb{P}(B)}{\mathbb{P}(A)} = \frac{\mathbb{P}(A|B) \mathbb{P}(B)}{\sum_{i=1}^n \mathbb{P}(A|B_i)\mathbb{P}(B_i)}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(B|A)\)</span> - <em>posterior</em></p></li>
<li><p><span class="math notranslate nohighlight">\(p(A|B)\)</span> - <em>likelihood</em></p></li>
<li><p><span class="math notranslate nohighlight">\(p(B)\)</span> - <em>prior</em></p></li>
<li><p><span class="math notranslate nohighlight">\(p(A)\)</span> - <em>evidence</em></p></li>
</ul>
</li>
</ul>
</section>
<section id="random-variables-and-their-properties">
<h3><span class="section-number">2.1.2. </span>Random Variables and Their Properties<a class="headerlink" href="#random-variables-and-their-properties" title="Permalink to this heading">#</a></h3>
<ul>
<li><p><em>Random variable</em> (r.v.) <span class="math notranslate nohighlight">\(X\)</span> is a function <span class="math notranslate nohighlight">\(X:\Omega \rightarrow \mathbb{R}\)</span>. This is the formal way to move from abstract events to real-valued numbers. <span class="math notranslate nohighlight">\(X\)</span> is essentially a variable that does not have a fixed value but can have different values with certain probabilities.</p></li>
<li><p>Continuous r.v.s:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(F_X(x)\)</span> - <em>Cumulative distribution function</em> (CDF); probability that the r.v. <span class="math notranslate nohighlight">\(X\)</span> is smaller than some value <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-cdf">
<span class="eqno">(2.5)<a class="headerlink" href="#equation-cdf" title="Permalink to this equation">#</a></span>\[F_X(x) = \mathbb{P}(X\le x), \quad \text{with } F_X(-\infty)=0, \; F_X(\infty)=1\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(p_X(x)\)</span> - <em>Probability density function</em> (PDF):</p>
<div class="math notranslate nohighlight" id="equation-pdf">
<span class="eqno">(2.6)<a class="headerlink" href="#equation-pdf" title="Permalink to this equation">#</a></span>\[p_X(x)=\frac{dF_X(x)}{dx}\ge 0 \;\text{ and } \; \int_{-\infty}^{+\infty}p_X(x) dx =1\]</div>
</li>
</ul>
</li>
</ul>
<figure class="align-center" id="pdf-cdf">
<a class="reference internal image-reference" href="../_images/pdf_cdf.png"><img alt="../_images/pdf_cdf.png" src="../_images/pdf_cdf.png" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.2 </span><span class="caption-text">PDF and CDF functions.</span><a class="headerlink" href="#pdf-cdf" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Discrete r.v.s:</p>
<ul class="simple">
<li><p><em>Probability mass function</em> (PMF) - same as the pdf but for a discrete r.v. <span class="math notranslate nohighlight">\(X\)</span>. Integrals become sums.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mu = E[X]\)</span> - <em>mean value</em> or <em>expected value</em></p>
<div class="math notranslate nohighlight" id="equation-mean">
<span class="eqno">(2.7)<a class="headerlink" href="#equation-mean" title="Permalink to this equation">#</a></span>\[E[X] = \int_{-\infty}^{+\infty}x \, p_X(x) \, dx\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\sigma^2 = Var[X]\)</span> - <em>variance</em></p>
<div class="math notranslate nohighlight" id="equation-variance">
<span class="eqno">(2.8)<a class="headerlink" href="#equation-variance" title="Permalink to this equation">#</a></span>\[Var[X] = \int_{-\infty}^{+\infty}x^2 \, p_X(x) \, dx = E[(X-\mu)^2]\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(Cov[X,Y]=E[(X-\mu_X)(Y-\mu_Y)]\)</span> - <em>covariance</em></p></li>
<li><p><em>Change of variables</em> - if <span class="math notranslate nohighlight">\(X \sim p_X\)</span> and <span class="math notranslate nohighlight">\(Y=h(X)\)</span>, then the distribution of <span class="math notranslate nohighlight">\(Y\)</span> becomes:</p>
<div class="math notranslate nohighlight" id="equation-change-of-vars">
<span class="eqno">(2.9)<a class="headerlink" href="#equation-change-of-vars" title="Permalink to this equation">#</a></span>\[p_Y(y)=p_X(x)\left|\frac{\text{d}x}{\text{d}y}\right| = p_X(h^{-1}(y)) \left|\frac{\text{d}h^{-1}(y)}{\text{d}y}\right|\]</div>
</li>
</ul>
<p><strong>Exercise: Change of Variables</strong></p>
<p>Given the r.v. <span class="math notranslate nohighlight">\(X\)</span> with pdf <span class="math notranslate nohighlight">\(p_X(x)=3x^2\)</span> defined on <span class="math notranslate nohighlight">\(X \in (0,1)\)</span> and the function <span class="math notranslate nohighlight">\(Y=X^2\)</span>, find the pdf of <span class="math notranslate nohighlight">\(Y\)</span>.
Hint: use <span class="math notranslate nohighlight">\(X=h^{-1}(Y)\)</span> as shown <a class="reference external" href="https://online.stat.psu.edu/stat414/lesson/22/22.2">here</a>.</p>
</section>
<section id="catalogue-of-important-distributions">
<h3><span class="section-number">2.1.3. </span>Catalogue of Important Distributions<a class="headerlink" href="#catalogue-of-important-distributions" title="Permalink to this heading">#</a></h3>
<p><strong>Discrete</strong></p>
<ul>
<li><p><em>Binomial</em>, <span class="math notranslate nohighlight">\(X\in\{0,1,...,n\}\)</span>. Describes how often we get <span class="math notranslate nohighlight">\(k\)</span> positive outcomes out of <span class="math notranslate nohighlight">\(n\)</span> independent experiments. Parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is the success probability of each trial.</p>
<div class="math notranslate nohighlight" id="equation-binomial">
<span class="eqno">(2.10)<a class="headerlink" href="#equation-binomial" title="Permalink to this equation">#</a></span>\[p(x=k|n, \lambda)=\binom{n}{k}\lambda^k(1-\lambda)^{n-k}, \quad \text{ with } k\in \{0,1,2,..., n\}.\]</div>
<p>With the binomial coefficient <span class="math notranslate nohighlight">\(\binom{n}{k}=\frac{n!}{k!(n-k)!}\)</span>.</p>
</li>
<li><p><em>Bernoulli</em> - special case of Binomial with <span class="math notranslate nohighlight">\(n=1\)</span>.</p></li>
<li><p><em>Categorical</em> - generalizes Bernoulli to a categorical random variable with <span class="math notranslate nohighlight">\(k\)</span> categories each with probability <span class="math notranslate nohighlight">\(\pi_i\ge 0\)</span> for <span class="math notranslate nohighlight">\(i \in \{1,...,k\}\)</span>, such that <span class="math notranslate nohighlight">\(\sum {\pi_i}=1\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-categorical-pmf">
<span class="eqno">(2.11)<a class="headerlink" href="#equation-categorical-pmf" title="Permalink to this equation">#</a></span>\[\begin{split}
    \begin{align}
    p(x=i | \mathbb{\pi}) &amp;=\pi_i\\
     &amp;= \prod_{i=1}^k \pi_i^{[x=i]}
    \end{align}
    \end{split}\]</div>
<p>The two pdf formulations above are equivalent, and the second one uses the <a class="reference external" href="https://en.wikipedia.org/wiki/Iverson_bracket">Iverson bracket</a>.</p>
</li>
<li><p><em>Multinomial</em> - a generalization of the Binomial and the Categorical distributions. It models the number of occurrences for each of <span class="math notranslate nohighlight">\(k\)</span> classes when sampling from a categorical distribution is repeated <span class="math notranslate nohighlight">\(n\)</span> times. For <span class="math notranslate nohighlight">\(k=2, n=1\)</span>, this becomes Bernoulli; for <span class="math notranslate nohighlight">\(k&gt; 2,n=1\)</span>, it is the Categorical; for <span class="math notranslate nohighlight">\(k=2, n&gt;1\)</span>, it is the Binomial.</p>
<div class="math notranslate nohighlight" id="equation-multinomial-pmf">
<span class="eqno">(2.12)<a class="headerlink" href="#equation-multinomial-pmf" title="Permalink to this equation">#</a></span>\[p(x_i,...,x_k | n, \mathbf{\pi}) = \frac{n!}{x_1! \dots x_k!} \pi_1^{x_i} \dots \pi_k^{x_k}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(x_i\)</span> denotes the number of occurrences of class <span class="math notranslate nohighlight">\(i\)</span>, with <span class="math notranslate nohighlight">\(\sum x_i=n\)</span> and <span class="math notranslate nohighlight">\(x_i \ge 0\)</span>.</p>
</li>
</ul>
<p><strong>Continuous</strong></p>
<ul>
<li><p><em>Normal</em> (aka <em>Gaussian</em>), <span class="math notranslate nohighlight">\(X \in \mathbb{R}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-gaussian">
<span class="eqno">(2.13)<a class="headerlink" href="#equation-gaussian" title="Permalink to this equation">#</a></span>\[p(x| \mu, \sigma)=\mathcal{N}(x|\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</div>
</li>
<li><p><em>Multivariate Gaussian</em> <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{\mu}, \mathbf{\Sigma})\)</span> of <span class="math notranslate nohighlight">\(\mathbf{X}\in \mathbb{R}^n\)</span> with mean <span class="math notranslate nohighlight">\(\mathbf{\mu}\in \mathbb{R}^n \)</span> and covariance <span class="math notranslate nohighlight">\(\mathbb{\Sigma} \in \mathbb{R}_{+}^{n\times n}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-multivariate-gaussian">
<span class="eqno">(2.14)<a class="headerlink" href="#equation-multivariate-gaussian" title="Permalink to this equation">#</a></span>\[p_X(x)= \frac{1}{(2\pi)^{n/2}\sqrt{\det (\mathbf{\Sigma})}} \exp \left(-\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^{\top}\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right).\]</div>
</li>
</ul>
</section>
<section id="exponential-family">
<h3><span class="section-number">2.1.4. </span>Exponential Family<a class="headerlink" href="#exponential-family" title="Permalink to this heading">#</a></h3>
<p>The exponential family of distributions is a large family of distributions with shared properties, some of which we have already encountered in the previous lecture. Prominent members of the exponential family include:</p>
<ul class="simple">
<li><p>Bernoulli</p></li>
<li><p>Gaussian</p></li>
<li><p>Dirichlet</p></li>
<li><p>Gamma</p></li>
<li><p>Poisson</p></li>
<li><p>Beta</p></li>
</ul>
<p>At their core, all members of the exponential family fit the same general probability distribution form:</p>
<div class="math notranslate nohighlight" id="equation-exponential-pdfs">
<span class="eqno">(2.15)<a class="headerlink" href="#equation-exponential-pdfs" title="Permalink to this equation">#</a></span>\[p(x|\eta) = h(x) \exp \left\{ \eta^{\top} t(x) - a(\eta) \right\},\]</div>
<p>where the individual components are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\eta\)</span> - <em>natural parameter</em></p></li>
<li><p><span class="math notranslate nohighlight">\(t(x)\)</span> - <em>sufficient statistic</em></p></li>
<li><p><span class="math notranslate nohighlight">\(h(x)\)</span> - <em>probability support measure</em></p></li>
<li><p><span class="math notranslate nohighlight">\(a(\eta)\)</span> - <em>log normalizer</em>; guarantees that the probability density integrates to 1.</p></li>
</ul>
<blockquote>
<div><p>If you are unfamiliar with the concept of <em>probability measures</em>, then <span class="math notranslate nohighlight">\(h(x)\)</span> can safely be disregarded. Conceptually it describes the area in the probability space over which the probability distribution is defined.</p>
</div></blockquote>
<p><strong>Why is this family of distributions relevant to this course?</strong></p>
<blockquote>
<div><p>The exponential family has a direct connection to graphical models, which are a formalism favored by many people to visualize machine learning models and the way individual components interact with each other. As such, they are highly instructive and, at the same time, foundational to many probabilistic approaches covered in this course.</p>
</div></blockquote>
<p>Let’s inspect the practical example of the Gaussian distribution to see how the theory translates into practice. Taking the probability density function which we have also previously worked with</p>
<div class="math notranslate nohighlight" id="equation-gaussian-2">
<span class="eqno">(2.16)<a class="headerlink" href="#equation-gaussian-2" title="Permalink to this equation">#</a></span>\[p(x|\mu, \sigma^{2}) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left\{ \frac{(x - \mu)^{2}}{2 \sigma^{2}} \right\}.\]</div>
<p>We can then expand the square in the exponent of the Gaussian to isolate the individual components of the exponential family</p>
<div class="math notranslate nohighlight" id="equation-gaussian-expanded">
<span class="eqno">(2.17)<a class="headerlink" href="#equation-gaussian-expanded" title="Permalink to this equation">#</a></span>\[p(x|\mu, \sigma^{2}) = \frac{1}{\sqrt{2 \pi}} \exp \left\{ \frac{\mu}{\sigma^{2}}x - \frac{1}{2 \sigma^{2}}x^{2} - \frac{1}{2 \sigma^{2}} \mu^{2} - \ln \sigma \right\}.\]</div>
<p>Then the individual components of the Gaussian are</p>
<div class="math notranslate nohighlight" id="equation-gaussian-as-exponential">
<span class="eqno">(2.18)<a class="headerlink" href="#equation-gaussian-as-exponential" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{align}
\eta &amp;= \langle \frac{\mu}{\sigma^{2}}, - \frac{1}{2 \sigma^{2}} \rangle \\
t(x) &amp;= \langle x, x^{2} \rangle \\
a(\eta) &amp;= \frac{\mu^{2}}{2 \sigma^{2}} + \ln \sigma \\
h(x) &amp;= \frac{1}{\sqrt{2 \pi}}
\end{align}\end{split}\]</div>
<p>For the <em>sufficient statistics</em>, we then need to derive the derivative of the log normalizer, i.e</p>
<div class="math notranslate nohighlight" id="equation-gaussian-as-exponential-suff-stats">
<span class="eqno">(2.19)<a class="headerlink" href="#equation-gaussian-as-exponential-suff-stats" title="Permalink to this equation">#</a></span>\[\frac{d}{d\eta}a(\eta) = \mathbb{E}\left[ t(X) \right],\]</div>
<p>which yields</p>
<div class="math notranslate nohighlight" id="equation-gaussian-as-exponential-suff-stats-2">
<span class="eqno">(2.20)<a class="headerlink" href="#equation-gaussian-as-exponential-suff-stats-2" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{align}
\frac{da(\eta)}{d\eta_{1}} &amp;= \mu = \mathbb{E}[X] \\
\frac{da(\eta)}{d\eta_{2}} &amp;= \sigma^2 - \mu^2 = \mathbb{E}[X^{2}]. 
\end{align}\end{split}\]</div>
<p><strong>Exercise: Exponential Family 1</strong></p>
<p>Show that the Bernoulli distribution is a member of the exponential family.</p>
<p><strong>Exercise: Exponential Family 2</strong></p>
<p>Show that the Dirichlet distribution is a member of the exponential family.</p>
</section>
</section>
<section id="id1">
<h2><span class="section-number">2.2. </span>Gaussian Mixture Models<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>Assume that we have a set of measurements <span class="math notranslate nohighlight">\(\{x^{(1)}, \dots x^{(m)}\}\)</span>. This is one of the few unsupervised learning examples in this lecture, thus, we do not know the true labels <span class="math notranslate nohighlight">\(y\)</span>. Yet we assume that the data can be split into clusters, see <a class="reference internal" href="#em-algorithm"><span class="std std-numref">Fig. 2.3</span></a>.</p>
<p>Gaussian Mixture Models (GMMs) assume that the data comes from a mixture of <span class="math notranslate nohighlight">\(K\)</span> Gaussian distributions in the form</p>
<div class="math notranslate nohighlight" id="equation-gmm-model">
<span class="eqno">(2.21)<a class="headerlink" href="#equation-gmm-model" title="Permalink to this equation">#</a></span>\[p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k),\]</div>
<p>with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\pi = (\pi_1,...,\pi_K)\)</span> called <em>mixing coefficients</em>, or cluster probabilities,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu = (\mu_1,...,\mu_K)\)</span> the <em>cluster means</em>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma = (\Sigma_1,...,\Sigma_K)\)</span> the <em>cluster covariance matrices</em>.</p></li>
</ul>
<p>We define a <span class="math notranslate nohighlight">\(K\)</span>-dimensional r.v. <span class="math notranslate nohighlight">\(z\)</span> which satisfies <span class="math notranslate nohighlight">\(z\in \{0,1\}\)</span> and <span class="math notranslate nohighlight">\(\sum_k z_k=1\)</span> (i.e. with only one of its dimensions being 1, while all others are 0), such that <span class="math notranslate nohighlight">\(z_k~\sim \text{Categorical}(\pi_k)\)</span> and <span class="math notranslate nohighlight">\(p(z_k=1) = \pi_k\)</span>. For Eq. <a class="reference internal" href="#equation-gmm-model">(2.21)</a> to be a valid probability density, the parameters <span class="math notranslate nohighlight">\(\{\pi_k\}\)</span> must satisfy <span class="math notranslate nohighlight">\(0\le\pi_k\le 1\)</span> and <span class="math notranslate nohighlight">\(\sum_k \pi_k=1\)</span>.</p>
<p>The marginal distribution of <span class="math notranslate nohighlight">\(z\)</span> can be equivalently written as</p>
<div class="math notranslate nohighlight" id="equation-gmm-marginal-z">
<span class="eqno">(2.22)<a class="headerlink" href="#equation-gmm-marginal-z" title="Permalink to this equation">#</a></span>\[p(z)=\prod_{k=1}^{K} \pi_k^{z_k},\]</div>
<p>whereas the conditional <span class="math notranslate nohighlight">\(p(x|z_k=1) = \mathcal{N}(x|\mu_k, \Sigma_k)\)</span> becomes</p>
<div class="math notranslate nohighlight" id="equation-gmm-conditional">
<span class="eqno">(2.23)<a class="headerlink" href="#equation-gmm-conditional" title="Permalink to this equation">#</a></span>\[p(x|z) = \prod_{k=1}^{K}\mathcal{N}(x|\mu_k, \Sigma_k)^{z_k}.\]</div>
<p>If we then express the distribution of interest <span class="math notranslate nohighlight">\(p(x)\)</span> as the marginalized joint distribution, we obtain</p>
<div class="math notranslate nohighlight" id="equation-gmm-marginalization">
<span class="eqno">(2.24)<a class="headerlink" href="#equation-gmm-marginalization" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
p(x) &amp;= \sum_z p(x,z) \\
&amp; = \sum_z p(x|z) p(z) \\
&amp; = \sum_{z} \prod_{k=1}^{K} \left( \pi_k\mathcal{N}(x| \mu_k, \Sigma_k)\right)^{z_k} \\
&amp; = \sum_{k=1}^K \pi_k\mathcal{N}(x| \mu_k, \Sigma_k).
\end{aligned}
\end{split}\]</div>
<p>Thus, the unknown parameters are <span class="math notranslate nohighlight">\(\{\pi_k, \mu_k, \Sigma_k\}_{k=1:K}\)</span>. We can write the log-likelihood of the data as</p>
<div class="math notranslate nohighlight" id="equation-gmm-mle">
<span class="eqno">(2.25)<a class="headerlink" href="#equation-gmm-mle" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\ell(x | \pi,\mu,\Sigma) &amp;= \sum_{i=1}^{m}\ln p(x^{(i)}|\pi,\mu,\Sigma) \\
&amp;= \sum_{i=1}^{m}\ln \left\{ \sum_{k=1}^K \pi_k \mathcal{N}(x^{(i)}|\mu_k,\Sigma_k) \right\}.
\end{aligned}\end{split}\]</div>
<p>However, if we try to solve this problem analytically, we will see that there is no closed-form solution (because of the sum inside the <span class="math notranslate nohighlight">\(\ln\)</span>). The problem is that we do not know which <span class="math notranslate nohighlight">\(z_k\)</span> each of the measurements comes from.</p>
<section id="expectation-maximization">
<h3><span class="section-number">2.2.1. </span>Expectation-Maximization<a class="headerlink" href="#expectation-maximization" title="Permalink to this heading">#</a></h3>
<figure class="align-center" id="em-algorithm">
<a class="reference internal image-reference" href="../_images/em_algorithm.png"><img alt="../_images/em_algorithm.png" src="../_images/em_algorithm.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.3 </span><span class="caption-text">EM algorithm for a GMM with <span class="math notranslate nohighlight">\(K=2\)</span> (Source: <span id="id2">[<a class="reference internal" href="../references.html#id2" title="Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. ISBN 978-0-387-31073-2. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span>, Section 9.2).</span><a class="headerlink" href="#em-algorithm" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>There is an iterative algorithm that can solve the maximum likelihood problem by alternating between two steps. The algorithm goes as follows:</p>
<ol class="arabic" start="0">
<li><p>Guess the number of modes <span class="math notranslate nohighlight">\(K\)</span></p></li>
<li><p>Randomly initialize the means <span class="math notranslate nohighlight">\(\mu_k\)</span>, covariances <span class="math notranslate nohighlight">\(\Sigma_k\)</span>, and mixing coefficients <span class="math notranslate nohighlight">\(\pi_k\)</span>, and evaluate the likelihood</p></li>
<li><p><strong>(E-step)</strong>. Evaluate <span class="math notranslate nohighlight">\(w_k^{(i)}\)</span> assuming constant <span class="math notranslate nohighlight">\(\pi, \mu, \Sigma\)</span> (see Eq. <a class="reference internal" href="#equation-gmm-responsibilities">(2.29)</a> after the algorithm)</p>
<div class="math notranslate nohighlight" id="equation-gmm-e-step">
<span class="eqno">(2.26)<a class="headerlink" href="#equation-gmm-e-step" title="Permalink to this equation">#</a></span>\[w_k^{(i)} := p(z^{(i)}=k| x^{(i)}, \pi, \mu, \Sigma).\]</div>
</li>
<li><p><strong>(M-step)</strong>. Update the parameters by solving the maximum likelihood problems for fixed <span class="math notranslate nohighlight">\(z_k\)</span> values.</p>
<div class="math notranslate nohighlight" id="equation-gmm-m-step">
<span class="eqno">(2.27)<a class="headerlink" href="#equation-gmm-m-step" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{aligned}
    \pi_k &amp;:= \frac{1}{m}\sum_{i=1}^m w_k^{(i)} \\
    \mu_k &amp;:= \frac{\sum_{i=1}^{m} w_k^{(i)}x^{(i)}}{\sum_{i=1}^{m} w_k^{(i)}} \\
    \Sigma_k &amp;:= \frac{\sum_{i=1}^{m} w_k^{(i)}(x^{(i)}-\mu_k)(x^{(i)}-\mu_k)^{\top}}{\sum_{i=1}^{m} w_k^{(i)}}
    \end{aligned}
    \end{split}\]</div>
</li>
<li><p>Evaluate the log-likelihood</p>
<div class="math notranslate nohighlight" id="equation-gmm-lig-likelihood">
<span class="eqno">(2.28)<a class="headerlink" href="#equation-gmm-lig-likelihood" title="Permalink to this equation">#</a></span>\[l(x | \pi,\mu,\Sigma) = \sum_{i=1}^{m}\ln \left\{ \sum_{k=1}^K \pi_k \mathcal{N}(x^{(i)}|\mu_k,\Sigma_k) \right\}\]</div>
<p>and check for convergence. If not converged, return to step 2.</p>
</li>
</ol>
<p>In the E-step, we compute the posterior probability of <span class="math notranslate nohighlight">\(z^{(i)}_k\)</span> given the data point <span class="math notranslate nohighlight">\(x^{(i)}\)</span> and the current <span class="math notranslate nohighlight">\(\pi\)</span>, <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span> values as</p>
<div class="math notranslate nohighlight" id="equation-gmm-responsibilities">
<span class="eqno">(2.29)<a class="headerlink" href="#equation-gmm-responsibilities" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
p(z^{(i)}=k| x^{(i)},\pi,\mu,\Sigma) &amp;= \frac{p(x^{(i)}|z^{(i)}=k, \mu, \Sigma)p(z^{(i)}=k,\pi)}{\sum_{l=1}^K p(x^{(i)}|z^{(i)}=l, \mu, \Sigma)p(z^{(i)}=l,\pi)} \\
 &amp;= \frac{\pi_k \mathcal{N}(x^{(i)}|\mu_k, \Sigma_k)}{\sum_{l=1}^K \pi_l \mathcal{N}(x^{(i)}|\mu_l, \Sigma_l)}
\end{aligned}\end{split}\]</div>
<p>The values of <span class="math notranslate nohighlight">\(p(x^{(i)}|z^{(i)}=k, \mu, \Sigma)\)</span> can be computed by evaluating the <span class="math notranslate nohighlight">\(k\)</span>th Gaussian with parameters <span class="math notranslate nohighlight">\(\mu_k\)</span> and <span class="math notranslate nohighlight">\(\Sigma_k\)</span>. And <span class="math notranslate nohighlight">\(p(z^{(i)}=k,\pi)\)</span> is just <span class="math notranslate nohighlight">\(\pi_k\)</span>.</p>
<p><strong>Example: Iris</strong></p>
<p>An example of how to apply a GMM to a version of the Iris dataset is given below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Adapted from: https://www.geeksforgeeks.org/gaussian-mixture-model/</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="c1"># Load the first two input columns of the iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">];</span> <span class="n">features</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Fit a GMM model with 3 clusters</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num iterations of EM algorithm =&quot;</span><span class="p">,</span> <span class="n">gmm</span><span class="o">.</span><span class="n">n_iter_</span><span class="p">)</span> <span class="c1"># around 8</span>
<span class="n">Yhat</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Plot the true vs learned clusters</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;True clusters&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Yhat</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Learned clusters&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>This leads to the following learned clusters:</p>
<figure class="align-center" id="iris-clusters">
<a class="reference internal image-reference" href="../_images/iris_clusters.png"><img alt="../_images/iris_clusters.png" src="../_images/iris_clusters.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.4 </span><span class="caption-text">Iris clusters with <span class="math notranslate nohighlight">\(K=3\)</span>.</span><a class="headerlink" href="#iris-clusters" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Exercise: derive the M-step update equations following the maximum likelihood approach.</strong></p>
<blockquote>
<div><p>Hint: look at <span id="id3">[<a class="reference internal" href="../references.html#id2" title="Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. ISBN 978-0-387-31073-2. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span>, Section 9.2.</p>
</div></blockquote>
</section>
<section id="applications-and-limitations">
<h3><span class="section-number">2.2.2. </span>Applications and Limitations<a class="headerlink" href="#applications-and-limitations" title="Permalink to this heading">#</a></h3>
<p>Once we have fitted a GMM on <span class="math notranslate nohighlight">\(p(x)\)</span>, we can use it for:</p>
<ol class="arabic simple">
<li><p>Sampling: there are efficient ways to draw samples from the Gaussian distribution.</p></li>
<li><p>Density estimation: by evaluating the probability <span class="math notranslate nohighlight">\(p(\tilde{x})\)</span> of a new point <span class="math notranslate nohighlight">\(\tilde{x}\)</span>, we can compute how probable it is that this point comes from the same distribution as the training data.</p></li>
<li><p>Clustering: so far we have talked about density estimation, but GMMs are typically used for clustering. Given a new query point <span class="math notranslate nohighlight">\(\tilde{x}\)</span>, we can evaluate each of the <span class="math notranslate nohighlight">\(K\)</span> Gaussians and scale their probability by the respective <span class="math notranslate nohighlight">\(\pi_k\)</span>. These will be the probabilities of <span class="math notranslate nohighlight">\(\tilde{x}\)</span> to be part of cluster <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
</ol>
<p>Most limitations of this approach arise from the assumption that the individual clusters follow the Gaussian distribution:</p>
<ul class="simple">
<li><p>If the data does not follow a Gaussian distribution, e.g. heavy-tailed distribution with outliers, then too much weight will be given to the outliers.</p></li>
<li><p>If there is an outlier, eventually one mode will focus only on this one data point. But if a Gaussian describes only one data point, then its variance will be zero and we recover a singularity/Dirac function.</p></li>
<li><p>The choice of <span class="math notranslate nohighlight">\(K\)</span> is crucial, and this parameter needs to be optimized in an outer loop.</p></li>
<li><p>GMMs do not scale well to high dimensions.</p></li>
</ul>
</section>
</section>
<section id="further-references">
<h2><span class="section-number">2.3. </span>Further References<a class="headerlink" href="#further-references" title="Permalink to this heading">#</a></h2>
<section id="id4">
<h3><span class="section-number">2.3.1. </span>Probability Theory<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><span id="id5">[<a class="reference internal" href="../references.html#id2" title="Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. ISBN 978-0-387-31073-2. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span>, Chapters 1 and 2, and Appendix B</p></li>
<li><p><span id="id6">[<a class="reference internal" href="../references.html#id4" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: http://probml.github.io/book1.">Murphy, 2022</a>]</span>, Chapters 2 and 3</p></li>
<li><p><span id="id7">[<a class="reference internal" href="../references.html#id7" title="Andrew Ng. Cs229 lecture notes. Running file URL: https://cs229.stanford.edu/main_notes.pdf, 2022. URL: https://cs229.stanford.edu/notes2022fall/main_notes.pdf.">Ng, 2022</a>]</span>, Section 3.1 - the exponential family</p></li>
</ul>
</section>
<section id="id8">
<h3><span class="section-number">2.3.2. </span>Gaussian Mixture Models<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><span id="id9">[<a class="reference internal" href="../references.html#id7" title="Andrew Ng. Cs229 lecture notes. Running file URL: https://cs229.stanford.edu/main_notes.pdf, 2022. URL: https://cs229.stanford.edu/notes2022fall/main_notes.pdf.">Ng, 2022</a>]</span>, Chapter 11 - main GMM reference</p></li>
<li><p><span id="id10">[<a class="reference internal" href="../references.html#id2" title="Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. ISBN 978-0-387-31073-2. URL: https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.">Bishop, 2006</a>]</span>, Section 9.2 - detailed derivations</p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=q71Niz856KE&amp;ab_channel=Serrano.Academy">Video</a> with more visual intuition</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="linear.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1. </span>Linear Models</p>
      </div>
    </a>
    <a class="right-next"
       href="bayes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Bayesian methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-theory">2.1. Probability Theory</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-building-blocks">2.1.1. Basic Building Blocks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables-and-their-properties">2.1.2. Random Variables and Their Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#catalogue-of-important-distributions">2.1.3. Catalogue of Important Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-family">2.1.4. Exponential Family</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.2. Gaussian Mixture Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization">2.2.1. Expectation-Maximization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-limitations">2.2.2. Applications and Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">2.3. Further References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2.3.1. Probability Theory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">2.3.2. Gaussian Mixture Models</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By N. Adams, L. Paehler, A. Toshev
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022,2023,2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>