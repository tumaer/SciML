

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>4. Optimization &#8212; Introduction to Scientific Machine Learning for Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture/optimization';</script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Tricks of Optimization" href="tricks.html" />
    <link rel="prev" title="3. Bayesian methods" href="bayes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../about.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Scientific Machine Learning for Engineers - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Scientific Machine Learning for Engineers - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about.html">
                    About this Lecture
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="linear.html">1. Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="gmm.html">2. Gaussian Mixture Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes.html">3. Bayesian methods</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tricks.html">5. Tricks of Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svm.html">6. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="gp.html">7. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradients.html">8. Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlp.html">9. Multilayer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="cnn.html">10. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="rnn.html">11. Recurrent Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="ae.html">12. Encoder-Decoder Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Exercise</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../exercise/linear.html">1. Linear and Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/bayes.html">2. Bayesian Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/optimization.html">3. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/svm.html">4. Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/gp.html">5. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/cnn.html">6. Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../exercise/rnn.html">7. Recurrent Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../admin.html">Admin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preliminary_knowledge.html">Preliminary Knowledge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software.html">Software Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../practical_exam.html">Practical Exam WS22/23</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tumaer/SciML" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tumaer/SciML/issues/new?title=Issue%20on%20page%20%2Flecture/optimization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lecture/optimization.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-optimization">4.1. Basics of Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity">4.1.1. Convexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-functions">4.1.2. Cost Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-based-methods">4.2. Gradient-based Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">4.2.1. Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum">4.2.1.1. Momentum</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adam">4.2.2. Adam</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">4.2.3. Stochastic Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#minibatching">4.2.3.1. Minibatching</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#second-order-methods">4.3. Second-Order Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-s-method">4.3.1. Newton’s method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-quasi-newton-approach">4.3.2. The Quasi-Newton Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-secant-method">4.3.2.1. The Secant Method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quasi-newton-methods">4.3.2.2. Quasi-Newton Methods</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bfgs">4.3.2.3. BFGS</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#l-bfgs">4.3.2.4. L-BFGS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-free-optimization-dfo">4.4. Derivative-Free Optimization (DFO)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">4.5. Further References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimization">
<h1><span class="section-number">4. </span>Optimization<a class="headerlink" href="#optimization" title="Permalink to this heading">#</a></h1>
<p>In the lecture on <a class="reference internal" href="linear.html"><span class="doc std std-doc">Linear Models</span></a>, we already saw the main building blocks of a supervised learning algorithm:</p>
<ol class="arabic simple">
<li><p><strong>Model</strong> <span class="math notranslate nohighlight">\(h\)</span> of the relationship between inputs <span class="math notranslate nohighlight">\(x\)</span> and outputs <span class="math notranslate nohighlight">\(y\)</span>,</p></li>
<li><p><strong>Loss</strong> (aka <strong>Cost</strong>, <strong>Error</strong>) function <span class="math notranslate nohighlight">\(J(\vartheta)\)</span> quantifying the discrepancy between <span class="math notranslate nohighlight">\(h_{\vartheta}(x^{(i)})\)</span> and <span class="math notranslate nohighlight">\(y^{(i)}\)</span> for each of the measurement pairs <span class="math notranslate nohighlight">\(\left\{(x^{(i)}, y^{\text {(i)}})\right\}_{i=1,...m}\)</span>, and</p></li>
<li><p>Optimization algorithm (aka <strong>Optimizer</strong>) minimizing the loss.</p></li>
</ol>
<p>Point 1 is a matter of what we know about the world in advance and how we include that knowledge into the model, e.g. choose CNNs when working with images because the same pattern might appear at different locations of an image. Later in the lecture, we will look at different models each of which is by construction better suited for different problem types.</p>
<p>After selecting a model <span class="math notranslate nohighlight">\(h_{\vartheta}\)</span>, points 2 and 3 are critical to the success of the learning as the loss function (point 2) defines how we measure success and the optimizer (point 3) guides the process of moving from a random initial guess of the parameters to a parameter configuration with a smaller loss. These two points are the topic of this lecture and the lecture <a class="reference internal" href="tricks.html"><span class="doc std std-doc">Tricks of Optimization</span></a>.</p>
<blockquote>
<div><p>Note: all algorithms discussed below assume an unconstrained parameter space, i.e. <span class="math notranslate nohighlight">\(\vartheta \in \mathbb{R}^n\)</span>. There are dedicated algorithms for solving constraint optimization problems, e.g. <span class="math notranslate nohighlight">\(a \le \vartheta \le b\)</span>, but these are beyond the scope of our lecture.</p>
</div></blockquote>
<section id="basics-of-optimization">
<h2><span class="section-number">4.1. </span>Basics of Optimization<a class="headerlink" href="#basics-of-optimization" title="Permalink to this heading">#</a></h2>
<p>First, we define the general (unconstrained) minimization problem</p>
<div class="math notranslate nohighlight" id="equation-minimization-problem">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-minimization-problem" title="Permalink to this equation">#</a></span>\[\text{argmin}_{\vartheta} \; J(\vartheta),\]</div>
<p>where <span class="math notranslate nohighlight">\(J:\mathbb{R}^n \rightarrow \mathbb{R}\)</span> is a real-valued function. We call the optimal solution of this problem the <em>minimizer</em> of <span class="math notranslate nohighlight">\(J\)</span> and denote it as <span class="math notranslate nohighlight">\(\vartheta_{\star}\)</span>. The minimizer is defined as <span class="math notranslate nohighlight">\(J(\vartheta_{\star}) \le J(\vartheta)\)</span> for all <span class="math notranslate nohighlight">\(\vartheta\)</span>. If the relation <span class="math notranslate nohighlight">\(J(\vartheta_{\star}) \le J(\vartheta)\)</span> holds only in a local neighborhood <span class="math notranslate nohighlight">\(||\vartheta - \vartheta_{\star}|| \le \epsilon\)</span>, for some <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span>, then we call <span class="math notranslate nohighlight">\(\vartheta_{\star}\)</span> a local optimizer.</p>
<p>In the figure below we an example of different loss functions.</p>
<figure class="align-center" id="minima-examples">
<a class="reference internal image-reference" href="../_images/minima_examples.svg"><img alt="../_images/minima_examples.svg" src="../_images/minima_examples.svg" width="500px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.1 </span><span class="caption-text">Function with (left) one global minimum, (middle) infinitely many global minima, and (right) multiple local as well as one global minimum (Source: <span id="id1">[<a class="reference internal" href="../references.html#id10" title="Moritz Hardt and Benjamin Recht. Patterns, predictions, and actions: Foundations of machine learning. Princeton University Press, 2022. URL: https://mlstory.org/.">Hardt and Recht, 2022</a>]</span>, Chapter 5).</span><a class="headerlink" href="#minima-examples" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="convexity">
<h3><span class="section-number">4.1.1. </span>Convexity<a class="headerlink" href="#convexity" title="Permalink to this heading">#</a></h3>
<p>Convexity is a property of a function and has a simple geometric meaning.</p>
<p>If the function <span class="math notranslate nohighlight">\(J\)</span> fulfills the following inequality</p>
<div class="math notranslate nohighlight" id="equation-convex-function">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-convex-function" title="Permalink to this equation">#</a></span>\[J(\alpha \vartheta_l+(1-\alpha)\vartheta_r) \le \alpha J(\vartheta_l)+(1-\alpha)J(\vartheta_r), \quad \forall \vartheta_l, \vartheta_r \text{ and } \alpha \in [0,1],\]</div>
<p>then <span class="math notranslate nohighlight">\(J\)</span> is said to be convex. Geometrically, this inequality implies that a line segment between <span class="math notranslate nohighlight">\((\vartheta_l, J(\vartheta_l))\)</span> and <span class="math notranslate nohighlight">\((\vartheta_r, J(\vartheta_r))\)</span> lies above the graph of <span class="math notranslate nohighlight">\(J\)</span> in the range <span class="math notranslate nohighlight">\((\vartheta_l, \vartheta_r)\)</span>.</p>
<figure class="align-center" id="convex-nonconvex">
<a class="reference internal image-reference" href="../_images/convex_nonconvex.svg"><img alt="../_images/convex_nonconvex.svg" src="../_images/convex_nonconvex.svg" width="350px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.2 </span><span class="caption-text">Example of convex and nonconvex function (Source: <span id="id2">[<a class="reference internal" href="../references.html#id10" title="Moritz Hardt and Benjamin Recht. Patterns, predictions, and actions: Foundations of machine learning. Princeton University Press, 2022. URL: https://mlstory.org/.">Hardt and Recht, 2022</a>]</span>, Chapter 5).</span><a class="headerlink" href="#convex-nonconvex" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>There is exhaustive literature on convex functions and convex optimization, e.g. <span id="id3">[<a class="reference internal" href="../references.html#id11" title="Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, 2004. ISBN 978-0-521-83378-3. URL: https://web.stanford.edu/~boyd/cvxbook/.">Boyd and Vandenberghe, 2004</a>]</span>, due to the mathematical properties of such functions. An important result from this theory is that gradient descent is guaranteed to find an optimum of a convex function.</p>
<p>Two examples of convex functions are the Least Mean Square loss in linear regression and the negative log-likelihood in logistic regression. However, modern deep learning is in general non-convex. Thus, we optimize towards a local minimum in the proximity of an initial configuration.</p>
<blockquote>
<div><p>Note: Convexity is a property of the loss <span class="math notranslate nohighlight">\(J(\vartheta)\)</span> w.r.t. <span class="math notranslate nohighlight">\(\vartheta\)</span>. This means, for <span class="math notranslate nohighlight">\(J(\vartheta)\)</span> to be convex, the combination of model and loss functions has to result in a convex function in <span class="math notranslate nohighlight">\(\vartheta\)</span>.</p>
</div></blockquote>
<p>Apparently, the choice of <span class="math notranslate nohighlight">\(h(x)\)</span> plays an important role in the shape of <span class="math notranslate nohighlight">\(J(\vartheta)\)</span>, but how does the choice of loss function influence <span class="math notranslate nohighlight">\(J\)</span>?</p>
</section>
<section id="cost-functions">
<h3><span class="section-number">4.1.2. </span>Cost Functions<a class="headerlink" href="#cost-functions" title="Permalink to this heading">#</a></h3>
<p>We will discuss extensions of the loss function in the lecture <a class="reference internal" href="tricks.html"><span class="doc std std-doc">Tricks of Optimization</span></a>. For now, we give an example of common loss functions by problem type:</p>
<ul class="simple">
<li><p>Regression loss</p>
<ul>
<li><p>L1 loss: <span class="math notranslate nohighlight">\(\; \; \; J(h_{\vartheta}(x), y)=1/m \sum_{i=1}^m |y-h_{\vartheta}(x)|\)</span></p></li>
<li><p>MSE loss: <span class="math notranslate nohighlight">\(J(h_{\vartheta}(x), y)=1/m \sum_{i=1}^m (y-h_{\vartheta}(x))^2\)</span></p></li>
</ul>
</li>
<li><p>Classification loss</p>
<ul>
<li><p>Cross Entropy loss <span class="math notranslate nohighlight">\(J(h_{\vartheta}(x),y)= -\sum_{i=1}^m\sum_{k=1}^K(y_{ik}\cdot \log h_{\vartheta, k}(x_i))\)</span></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="gradient-based-methods">
<h2><span class="section-number">4.2. </span>Gradient-based Methods<a class="headerlink" href="#gradient-based-methods" title="Permalink to this heading">#</a></h2>
<p>If we now consider the functions from <a class="reference internal" href="#minima-examples"><span class="std std-numref">Fig. 4.1</span></a> as highly simplified objective functions which we want to minimize, then we see that the gradient descent method we saw in lecture <a class="reference internal" href="linear.html"><span class="doc std std-doc">Linear Models</span></a> is well suited. While being the obvious choice for the two cases on the left, the picture becomes a little muddier in the example on the right.</p>
<blockquote>
<div><p>Notation alert: For the derivation of the gradien-based optimization techniques we use the stands notation by which the function we want to find the minimum of becomes <span class="math notranslate nohighlight">\(J \rightarrow f\)</span> and the variable <span class="math notranslate nohighlight">\(\vartheta \rightarrow x\)</span>. Don’t confuse this <span class="math notranslate nohighlight">\(x\)</span> with the input measurements <span class="math notranslate nohighlight">\(\{x^{(i)},y^{(i)}\}_{i=0,...,m}\)</span>.</p>
</div></blockquote>
<section id="gradient-descent">
<h3><span class="section-number">4.2.1. </span>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this heading">#</a></h3>
<p>While the foundational concept, gradient descent is rarely used in its pure form, but mostly in its stochastic form these days. If we first consider it in its most foundational form in 1-dimension, then we can take a function <span class="math notranslate nohighlight">\(f\)</span>, and Taylor-expand it</p>
<div class="math notranslate nohighlight" id="equation-taylor-epsilon2">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-taylor-epsilon2" title="Permalink to this equation">#</a></span>\[f(x+\varepsilon) = f(x) + \varepsilon f'(x) + \mathcal{O}(\varepsilon^{2}).\]</div>
<p>Then, our intuition would dictate that moving a small <span class="math notranslate nohighlight">\(\varepsilon\)</span> in the direction of the negative gradient will decrease f. Taking a step size <span class="math notranslate nohighlight">\(\eta &gt; 0\)</span>, and using using our ability to freely choose <span class="math notranslate nohighlight">\(\varepsilon\)</span> to set it as</p>
<div class="math notranslate nohighlight" id="equation-epsilon-gd">
<span class="eqno">(4.4)<a class="headerlink" href="#equation-epsilon-gd" title="Permalink to this equation">#</a></span>\[\varepsilon = - \eta f'(x),\]</div>
<p>to then be plugged back into the Taylor expansion leads to</p>
<div class="math notranslate nohighlight" id="equation-taylor-gd">
<span class="eqno">(4.5)<a class="headerlink" href="#equation-taylor-gd" title="Permalink to this equation">#</a></span>\[f(x - \eta f'(x)) = f(x) - \eta f'^{2}(x) + \mathcal{O}(\eta^{2}f'^{2}(x)).\]</div>
<p>Unless our gradient vanishes, we can then minimize <span class="math notranslate nohighlight">\(f\)</span> as <span class="math notranslate nohighlight">\(\eta f'^{2}(x)&gt;0\)</span>. Choosing a small enough <span class="math notranslate nohighlight">\(\eta\)</span> can then make the higher-order terms irrelevant to arrive at</p>
<div class="math notranslate nohighlight" id="equation-gd-inequality">
<span class="eqno">(4.6)<a class="headerlink" href="#equation-gd-inequality" title="Permalink to this equation">#</a></span>\[f(x - \eta f'(x)) \leq f(x)\]</div>
<p>I.e.</p>
<div class="math notranslate nohighlight" id="equation-gd-rule">
<span class="eqno">(4.7)<a class="headerlink" href="#equation-gd-rule" title="Permalink to this equation">#</a></span>\[x \leftarrow x - \eta f'(x)\]</div>
<p>is the right algorithm to iterate over <span class="math notranslate nohighlight">\(x\)</span> s.t. the value of our (objective) function <span class="math notranslate nohighlight">\(f(x)\)</span> declines. We hence end up with an algorithm in which we have to choose an initial value for <span class="math notranslate nohighlight">\(x\)</span>, a constant <span class="math notranslate nohighlight">\(\eta &gt; 0\)</span>, and then continuously iterate <span class="math notranslate nohighlight">\(x\)</span> until we reach our stopping criterion.</p>
<p><span class="math notranslate nohighlight">\(\eta\)</span> is most commonly known as our <em>learning rate</em> and has to be set by us. Now, if <span class="math notranslate nohighlight">\(\eta\)</span> is too small <span class="math notranslate nohighlight">\(x\)</span> will update too slowly and require us to perform many more costly iterations than we’d ideally like to. But if we choose a learning rate that is too large, then the error term <span class="math notranslate nohighlight">\(\mathcal{O}(\eta^{2}f'^{2}(x))\)</span> at the back of the Taylor-expansion will explode, and we will overshoot the minimum.</p>
<p>Now, if we take a non-convex function for <span class="math notranslate nohighlight">\(f\)</span> which might even have infinitely many local minima, then the choice of our learning rate and initialization becomes even more important. Take the following function <span class="math notranslate nohighlight">\(f\)</span> for example:</p>
<div class="math notranslate nohighlight" id="equation-f-xcosx">
<span class="eqno">(4.8)<a class="headerlink" href="#equation-f-xcosx" title="Permalink to this equation">#</a></span>\[f(x) = x \cdot \cos(cx).\]</div>
<p>Then the optimization problem might end up looking like the following:</p>
<figure class="align-center" id="gd-1d">
<a class="reference internal image-reference" href="../_images/gd_1d.svg"><img alt="../_images/gd_1d.svg" src="../_images/gd_1d.svg" width="400px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.3 </span><span class="caption-text">Optimizing <span class="math notranslate nohighlight">\(f(x) = x \cdot \cos(cx)\)</span> (Source: <span id="id4">[<a class="reference internal" href="../references.html#id12" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning. arXiv preprint arXiv:2106.11342, 2021. URL: https://d2l.ai/.">Zhang <em>et al.</em>, 2021</a>]</span>, <a class="reference external" href="https://d2l.ai/chapter_optimization/gd.html">here</a>).</span><a class="headerlink" href="#gd-1d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong><span class="math notranslate nohighlight">\(x\)</span> as vector</strong></p>
<p>If we now consider the case where we do not only have a one-dimensional function <span class="math notranslate nohighlight">\(f\)</span>, but instead a function s.t.</p>
<div class="math notranslate nohighlight" id="equation-f-ml">
<span class="eqno">(4.9)<a class="headerlink" href="#equation-f-ml" title="Permalink to this equation">#</a></span>\[f: \mathbb{R}^{d} \rightarrow \mathbb{R},\]</div>
<p>i.e. a vector is mapped to a scalar, then the gradient is a vector of <span class="math notranslate nohighlight">\(d\)</span> partial derivatives</p>
<div class="math notranslate nohighlight" id="equation-grad-f-ml">
<span class="eqno">(4.10)<a class="headerlink" href="#equation-grad-f-ml" title="Permalink to this equation">#</a></span>\[\nabla f({\bf{x}}) = \left[ \frac{\partial f({\bf{x}})}{\partial x_{1}}, \frac{\partial f({\bf{x}})}{\partial x_{2}}, \ldots, \frac{\partial f({\bf{x}})}{\partial x_{d}} \right]^{\top},\]</div>
<p>with each term indicating the rate of change in each of the <span class="math notranslate nohighlight">\(d\)</span> dimensions. Then, we can use the Taylor-approximation as before and derive the gradient descent algorithm for the multivariate case</p>
<div class="math notranslate nohighlight" id="equation-gd-rule-ml">
<span class="eqno">(4.11)<a class="headerlink" href="#equation-gd-rule-ml" title="Permalink to this equation">#</a></span>\[{\bf{x}} \leftarrow {\bf{x}} - \eta \nabla f({\bf{x}})\]</div>
<p>If we then construct an objective function such as</p>
<div class="math notranslate nohighlight" id="equation-f-x2p2x2">
<span class="eqno">(4.12)<a class="headerlink" href="#equation-f-x2p2x2" title="Permalink to this equation">#</a></span>\[f({\bf{x}}) = x_{1}^{2} + 2 x_{2}^{2},\]</div>
<p>then our optimization could take the following shape.</p>
<figure class="align-center" id="gd-2d">
<a class="reference internal image-reference" href="../_images/gd_2d.svg"><img alt="../_images/gd_2d.svg" src="../_images/gd_2d.svg" width="400px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.4 </span><span class="caption-text">Optimizing <span class="math notranslate nohighlight">\(f({\bf{x}}) = x_{1}^{2} + 2 x_{2}^{2}\)</span> (Source: <span id="id5">[<a class="reference internal" href="../references.html#id12" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning. arXiv preprint arXiv:2106.11342, 2021. URL: https://d2l.ai/.">Zhang <em>et al.</em>, 2021</a>]</span>, <a class="reference external" href="https://d2l.ai/chapter_optimization/gd.html">here</a>).</span><a class="headerlink" href="#gd-2d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Having up until now relied on a fixed learning rate <span class="math notranslate nohighlight">\(\eta\)</span>, we now want to expand upon the previous algorithm by <em>adaptively</em> choosing <span class="math notranslate nohighlight">\(\eta\)</span>. For this, we have to go back to <strong>Newton’s method</strong>.</p>
<p>For this, we have to further expand the initial Taylor-expansion to the third-order term</p>
<div class="math notranslate nohighlight" id="equation-taylor-epsilon3">
<span class="eqno">(4.13)<a class="headerlink" href="#equation-taylor-epsilon3" title="Permalink to this equation">#</a></span>\[f({\bf{x}} + {\bf{\varepsilon}}) = f({\bf{x}}) + {\bf{\varepsilon}}^{\top} \nabla f({\bf{x}}) + \frac{1}{2} {\bf{\varepsilon}}^{\top} \nabla^{2} f({\bf{x}}) {\bf{\varepsilon}} + \mathcal{O}(||{\bf{\varepsilon}}||^{3}).\]</div>
<p>If we now look closer at <span class="math notranslate nohighlight">\(\nabla^{2} f({\bf{x}})\)</span>, sometimes also called the Hessian, then we recognize that for larger problems this term might be infeasible to compute due to the required <span class="math notranslate nohighlight">\(\mathcal{O}(d^{2})\)</span> computations.</p>
<p>Following the condition <span class="math notranslate nohighlight">\(\nabla_{\epsilon} f(\bf{x} + \epsilon)=0\)</span> for the minimum to calculate <span class="math notranslate nohighlight">\(\varepsilon\)</span>, we then arrive at the ideal value of</p>
<div class="math notranslate nohighlight" id="equation-epsilon-gd-optimal">
<span class="eqno">(4.14)<a class="headerlink" href="#equation-epsilon-gd-optimal" title="Permalink to this equation">#</a></span>\[{\bf{\varepsilon}} = - (\nabla^{2} f({\bf{x}}))^{-1} \nabla f({\bf{x}})\]</div>
<p>I.e. we need to invert <span class="math notranslate nohighlight">\(\nabla^{2} f({\bf{x}})\)</span>, the Hessian. Computing and storing this important array turns out to be really expensive! To reduce these costs we are looking towards the <em>preconditioning</em> of our optimization algorithm. For preconditioning we then only need to compute the diagonal entries, hence leading to the following update equation</p>
<div class="math notranslate nohighlight" id="equation-gd-precond">
<span class="eqno">(4.15)<a class="headerlink" href="#equation-gd-precond" title="Permalink to this equation">#</a></span>\[{\bf{x}} \leftarrow {\bf{x}} - \eta \text{diag }(\nabla^{2} f({\bf{x}}))^{-1} \nabla f({\bf{x}}).\]</div>
<p>What the preconditioning then achieves is to select a specific learning rate for every single variable.</p>
<section id="momentum">
<h4><span class="section-number">4.2.1.1. </span>Momentum<a class="headerlink" href="#momentum" title="Permalink to this heading">#</a></h4>
<p>If we now have a mismatch in the scales of two variables contained in our objective function, then we end up with an unsolvable optimization problem. To solve it, we require the <em>momentum method</em></p>
<div class="math notranslate nohighlight" id="equation-gd-mom">
<span class="eqno">(4.16)<a class="headerlink" href="#equation-gd-mom" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
{\bf{v}}_{t} &amp;\leftarrow \beta {\bf{v}}_{t-1} + {\bf{g}}_{t} \\
{\bf{x}}_{t} &amp;\leftarrow {\bf{x}}_{t-1} - \eta_{t} {\bf{v}}_{t},
\end{aligned}
\end{split}\]</div>
<p>with gradients of the loss <span class="math notranslate nohighlight">\(\mathbf{g}_t = \nabla f_t(x)\)</span> and the momentum variable <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span>. The momentum <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span> accumulates past gradients resembling how a ball rolling down a hill integrates over past forces. For <span class="math notranslate nohighlight">\(\beta=0\)</span>, we then have the regular gradient descent update. To now choose the perfect effective sample weight, we have to take the limit of</p>
<div class="math notranslate nohighlight" id="equation-gd-mom-beta-sum">
<span class="eqno">(4.17)<a class="headerlink" href="#equation-gd-mom-beta-sum" title="Permalink to this equation">#</a></span>\[{\bf{v}}_{t}\ = \sum_{\tau = 0}^{t-1} \beta^{\tau} {\bf{g}}_{t-\tau}.\]</div>
<p>Taking the limit <span class="math notranslate nohighlight">\(t \to \infty\)</span> results in the <a class="reference external" href="https://en.wikipedia.org/wiki/Geometric_series">geometric series</a> solution</p>
<div class="math notranslate nohighlight" id="equation-gd-mom-effective-beta">
<span class="eqno">(4.18)<a class="headerlink" href="#equation-gd-mom-effective-beta" title="Permalink to this equation">#</a></span>\[\sum_{\tau=0}^{\infty} \beta^{\tau} = \frac{1}{1 - \beta}\]</div>
<p>Hence using the momentum GD results in a step size <span class="math notranslate nohighlight">\(\frac{\eta}{1 - \beta}\)</span>, which at the same time gives us much better gradient descent directions to follow to minimize our objective function.</p>
<figure class="align-center" id="gd-with-momentum">
<a class="reference internal image-reference" href="../_images/gd_with_momentum.svg"><img alt="../_images/gd_with_momentum.svg" src="../_images/gd_with_momentum.svg" width="400px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.5 </span><span class="caption-text">Momentum parameter (Source: <span id="id6">[<a class="reference internal" href="../references.html#id12" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning. arXiv preprint arXiv:2106.11342, 2021. URL: https://d2l.ai/.">Zhang <em>et al.</em>, 2021</a>]</span>, <a class="reference external" href="https://d2l.ai/chapter_optimization/momentum.html">here</a>).</span><a class="headerlink" href="#gd-with-momentum" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="adam">
<h3><span class="section-number">4.2.2. </span>Adam<a class="headerlink" href="#adam" title="Permalink to this heading">#</a></h3>
<p>The <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam algorithm</a>, then extends beyond traditional gradient descent by combining multiple tricks into a highly robust algorithm, which is one of the most often used optimization algorithms in machine learning.</p>
<p>Expanding upon the previous use of momentum, Adam further utilizes the 1st and 2nd momentum of the gradient, i.e.</p>
<div class="math notranslate nohighlight" id="equation-adam-momenta">
<span class="eqno">(4.19)<a class="headerlink" href="#equation-adam-momenta" title="Permalink to this equation">#</a></span>\[\begin{split}
\begin{aligned}
{\bf{v}}_{t} &amp;\leftarrow \beta_{1} {\bf{v}}_{t-1} + (1 - \beta_{1}) {\bf{g}}_{t} \\
{\bf{s}}_{t} &amp;\leftarrow \beta_{2} {\bf{s}}_{t-1} + (1 - \beta_{2}) {\bf{g}}_{t}^{2},
\end{aligned}
\end{split}\]</div>
<p>where both <span class="math notranslate nohighlight">\(\beta_{1}\)</span>, and <span class="math notranslate nohighlight">\(\beta_{2}\)</span> are non-negative. A typical initialization here would be something along the lines of <span class="math notranslate nohighlight">\(\beta_{1} = 0.9\)</span>, and <span class="math notranslate nohighlight">\(\beta_{2} = 0.999\)</span> s.t. the variance estimate moves much slower than the momentum term. As an initialization of <span class="math notranslate nohighlight">\({\bf{v}}_{0} = {\bf{s}}_{0} = 0\)</span> can lead to bias in the optimization algorithm towards small initial values, we have to re-normalize the state variables with</p>
<div class="math notranslate nohighlight" id="equation-adam-renormalize">
<span class="eqno">(4.20)<a class="headerlink" href="#equation-adam-renormalize" title="Permalink to this equation">#</a></span>\[{\hat{\bf{v}}}_{t} = \frac{{\bf{v}}_{t}}{1 - \beta_{1}^{t}} \text{,  and  } {\hat{\bf{s}}}_{t} = \frac{{\bf{s}}_{t}}{1 - \beta_{2}^{t}}.\]</div>
<p>The Adam optimization algorithm then rescales the gradient to obtain</p>
<div class="math notranslate nohighlight" id="equation-adam-grad">
<span class="eqno">(4.21)<a class="headerlink" href="#equation-adam-grad" title="Permalink to this equation">#</a></span>\[{\bf{g}}'_{t} = \frac{\eta {\hat{\bf{v}}}_{t}}{\sqrt{{\hat{\bf{s}}}_{t}} + \varepsilon}.\]</div>
<p>The update formula for Adam is then</p>
<div class="math notranslate nohighlight" id="equation-adam-rule">
<span class="eqno">(4.22)<a class="headerlink" href="#equation-adam-rule" title="Permalink to this equation">#</a></span>\[{\bf{x}}_{t} \leftarrow {\bf{x}}_{t-1} - {\bf{g}}'_{t}.\]</div>
<blockquote>
<div><p>The strength of the Adam optimization algorithm is the stability of its update rule.</p>
</div></blockquote>
<p>If the momentum GD algorithm can be understood as a ball rolling down a hill, the Adam algorithm behaves like a ball with friction (see <a class="reference external" href="https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</a>).</p>
<figure class="align-center" id="optimizer">
<a class="reference internal image-reference" href="../_images/optimizer.gif"><img alt="../_images/optimizer.gif" src="../_images/optimizer.gif" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.6 </span><span class="caption-text">Comparison of 1st order optimization algorithms (Source: <a class="reference external" href="https://github.com/Jaewan-Yun/optimizer-visualization">github.com/Jaewan-Yun/optimizer-visualization</a>).</span><a class="headerlink" href="#optimizer" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="stochastic-gradient-descent">
<h3><span class="section-number">4.2.3. </span>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this heading">#</a></h3>
<p>In machine learning, we could take the loss across an average of the entire training set. Writing down the objective function for the training set with <span class="math notranslate nohighlight">\(n\)</span> entries leads to</p>
<div class="math notranslate nohighlight" id="equation-loss-full-dataset">
<span class="eqno">(4.23)<a class="headerlink" href="#equation-loss-full-dataset" title="Permalink to this equation">#</a></span>\[f({\bf{x}}) = \frac{1}{n} \sum_{i=1}^{n}f_{i}({\bf{x}}).\]</div>
<p>Here, <span class="math notranslate nohighlight">\(f_i\)</span> refers to evaluating the loss for training sample <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(\bf{x}\)</span> denotes the model parameter vector. The gradient of the objective function is then</p>
<div class="math notranslate nohighlight" id="equation-loss-full-dataset-grad">
<span class="eqno">(4.24)<a class="headerlink" href="#equation-loss-full-dataset-grad" title="Permalink to this equation">#</a></span>\[\nabla f({\bf{x}}) = \frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}({\bf{x}}).\]</div>
<p>With the cost of each independent variable iteration being <span class="math notranslate nohighlight">\(\mathcal{O}(n)\)</span> for gradient descent, stochastic gradient replaces this with a sampling step where we uniformly sample an index <span class="math notranslate nohighlight">\(i \in \{1, \ldots, n\}\)</span> at random, and then compute the gradient for the sampled index, and update <span class="math notranslate nohighlight">\({\bf{x}}\)</span></p>
<div class="math notranslate nohighlight" id="equation-sgd-rule-signle-input">
<span class="eqno">(4.25)<a class="headerlink" href="#equation-sgd-rule-signle-input" title="Permalink to this equation">#</a></span>\[{\bf{x}} \leftarrow {\bf{x}} - \eta \nabla f_{i}({\bf{x}}).\]</div>
<p>With this randomly sampled update, the cost for each iteration drops to <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span>. Due to the sampling we now have to think of our gradient as an expectation, i.e. by drawing uniform random samples we are essentially creating an unbiased estimator of the gradient</p>
<div class="math notranslate nohighlight" id="equation-sgd-grad-expectation">
<span class="eqno">(4.26)<a class="headerlink" href="#equation-sgd-grad-expectation" title="Permalink to this equation">#</a></span>\[\mathbb{E}_{i} \nabla f_{i}({\bf{x}}) = \frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}({\bf{x}}) = \nabla f({\bf{x}})\]</div>
<p>Looking at an example stochastic gradient descent optimization process in <a class="reference internal" href="#sgd-2d"><span class="std std-numref">Fig. 4.7</span></a>, we come to realize that the stochasticity induces too much noise for our chosen learning rate. To handle that we later introduce learning rate scheduling.</p>
<figure class="align-center" id="sgd-2d">
<a class="reference internal image-reference" href="../_images/sgd_2d.svg"><img alt="../_images/sgd_2d.svg" src="../_images/sgd_2d.svg" width="400px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.7 </span><span class="caption-text">SGD trajectory (Source: <span id="id7">[<a class="reference internal" href="../references.html#id12" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning. arXiv preprint arXiv:2106.11342, 2021. URL: https://d2l.ai/.">Zhang <em>et al.</em>, 2021</a>]</span>, <a class="reference external" href="https://d2l.ai/chapter_optimization/gd.html">here</a>).</span><a class="headerlink" href="#sgd-2d" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="minibatching">
<h4><span class="section-number">4.2.3.1. </span>Minibatching<a class="headerlink" href="#minibatching" title="Permalink to this heading">#</a></h4>
<blockquote>
<div><p>For data which is very similar, gradient descent is inefficient, whereas stochastic gradient descent relies on the power of vectorization.</p>
</div></blockquote>
<p>The answer to these ailments is the use of minibatches to exploit the memory and cache hierarchy a modern computer exposes to us. In essence, we seek to avoid the many single matrix-vector multiplications to reduce the overhead and improve our computational cost. If we compute the gradient in stochastic gradient descent as</p>
<div class="math notranslate nohighlight" id="equation-sgd-grad">
<span class="eqno">(4.27)<a class="headerlink" href="#equation-sgd-grad" title="Permalink to this equation">#</a></span>\[{\bf{g}}_t = \nabla f({\bf{x}}, i),\]</div>
<p>then, for computational efficiency, we will now perform this update in its batched form</p>
<div class="math notranslate nohighlight" id="equation-sgd-minibatch-grad">
<span class="eqno">(4.28)<a class="headerlink" href="#equation-sgd-minibatch-grad" title="Permalink to this equation">#</a></span>\[{\bf{g}}_t = \nabla \frac{1}{|\mathcal{B}_{t}|} \sum_{i \in \mathcal{B}_{t}} f({\bf{x}}, i).\]</div>
<p>As both <span class="math notranslate nohighlight">\(\bf{g}_t\)</span> and <span class="math notranslate nohighlight">\(i\)</span> are drawn uniformly at random from the training set, we retain our unbiased gradient estimator. For size <span class="math notranslate nohighlight">\(b\)</span> of the dataset, i.e <span class="math notranslate nohighlight">\(b = | \mathcal{B}_{t} |\)</span> we obtain a reduction of the standard deviation by <span class="math notranslate nohighlight">\(b^{-\frac{1}{2}}\)</span>, while this is desirable we should in practice choose the minibatch-size s.t. our underlying hardware gets utilized as optimally as possible.</p>
</section>
</section>
</section>
<section id="second-order-methods">
<h2><span class="section-number">4.3. </span>Second-Order Methods<a class="headerlink" href="#second-order-methods" title="Permalink to this heading">#</a></h2>
<p>With first-order methods only taking the gradient itself into account there is much information we are leaving untapped in attempting to solve our optimization problem, such as gradient curvature for which we’d need 2nd order gradients.</p>
<p>The reason for that is in part historic, automatic differentiation (to be explained in-depth in a later lecture) suffers from an exponential compute-graph blow-up when computing higher-order gradients. The automatic differentiation engines, the first popularized machine learning engines from AlexNet, LeNet etc. were built on, were unable to handle higher-order gradients for the above reason, and only modern automatic differentiation engines have been able to circumvent that problem. As such 2nd-order gradient methods have seen a recent resurgence with methods like <a class="reference external" href="https://research.google/pubs/pub47079/">Shampoo</a>, <a class="reference external" href="https://arxiv.org/pdf/2210.15255.pdf">RePAST</a>, and <a class="reference external" href="https://openreview.net/forum?id=cScb-RrBQC">Fishy</a>.</p>
<section id="newton-s-method">
<h3><span class="section-number">4.3.1. </span>Newton’s method<a class="headerlink" href="#newton-s-method" title="Permalink to this heading">#</a></h3>
<p>(also <em>Newton-Raphson method</em>)</p>
<p>A first example of the step toward second-order methods is the Newton method. It is an iterative method to find a zero of a differentiable function defined as <span class="math notranslate nohighlight">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span>. The Newton-method begins with an initial guess <span class="math notranslate nohighlight">\(x_{0}\)</span> to then iteratively update with</p>
<div class="math notranslate nohighlight" id="equation-newtons-method">
<span class="eqno">(4.29)<a class="headerlink" href="#equation-newtons-method" title="Permalink to this equation">#</a></span>\[x_{t+1} = x_{t} - \frac{f(x_{t})}{f'(x_{t})}\]</div>
<p>Graphically speaking this amounts to the case of the tangent line of the function intersecting with the x-axis.</p>
<figure class="align-center" id="newtons-method">
<a class="reference internal image-reference" href="../_images/newtons_method.png"><img alt="../_images/newtons_method.png" src="../_images/newtons_method.png" style="width: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.8 </span><span class="caption-text">Newton’s method (Source: <span id="id8">[<a class="reference internal" href="../references.html#id14" title="Bernd Gärtner and Martin Jaggi. Optimization for machine learning. 2023. URL: https://github.com/epfml/OptML_course/blob/master/lecture_notes/lecture-notes.pdf.">Gärtner and Jaggi, 2023</a>]</span>, Chapter 7).</span><a class="headerlink" href="#newtons-method" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The fallacies of this method are the cases where <span class="math notranslate nohighlight">\(f'(x_{t}) = 0\)</span>, and it will diverge when <span class="math notranslate nohighlight">\(|f'(x_{t})|\)</span> is very small. Going beyond the first-order update, we can then utilize the second-order gradient if our (objective) function <span class="math notranslate nohighlight">\(f\)</span> is twice differentiable. Hence turning the update step into</p>
<div class="math notranslate nohighlight" id="equation-newtons-method-optim">
<span class="eqno">(4.30)<a class="headerlink" href="#equation-newtons-method-optim" title="Permalink to this equation">#</a></span>\[x_{t+1} = x_{t} - \frac{f'(x_{t})}{f''(x_{t})}, \quad t&gt;0\]</div>
<p>Generalizing this update scheme in notation, we can use vector calculus to obtain</p>
<div class="math notranslate nohighlight" id="equation-newtons-method-vec">
<span class="eqno">(4.31)<a class="headerlink" href="#equation-newtons-method-vec" title="Permalink to this equation">#</a></span>\[x_{t+1} = x_{t} - \nabla^{2} f(x_{t})^{-1} \nabla f(x_{t}), \quad t\geq 0\]</div>
<p><span class="math notranslate nohighlight">\(\nabla^{2} f(x_{t})^{-1}\)</span> constitutes the inverse of the Hessian at <span class="math notranslate nohighlight">\(x_{t}\)</span>. As alluded to earlier the failure modes can then be formalized in the following fashion:</p>
<ul class="simple">
<li><p>The Hessian is not invertible.</p></li>
<li><p>Gets out of control if the Hessian has a small norm.</p></li>
</ul>
<p>If we further generalize the notation to a general update scheme</p>
<div class="math notranslate nohighlight" id="equation-newtons-method-simplified">
<span class="eqno">(4.32)<a class="headerlink" href="#equation-newtons-method-simplified" title="Permalink to this equation">#</a></span>\[x_{t+1} = x_{t} - \tilde{H}(x_{t}) \nabla f(x_{t})\]</div>
<p>then we can simplify this update scheme to the gradient descent we already encountered in the last lecture by setting <span class="math notranslate nohighlight">\(\tilde{H}(x_{t}) = \gamma I\)</span>, where <span class="math notranslate nohighlight">\(I\)</span> is the identity matrix. With <span class="math notranslate nohighlight">\(\tilde{H}=\nabla^2f(x)^{-1}\)</span> we recover Newton’s method. Newton’s method hence constitutes an adaptive gradient descent approach, where the adaptation happens with respect to the local geometry of the objective function at <span class="math notranslate nohighlight">\(x_{t}\)</span>.</p>
<blockquote>
<div><p>Where gradient descent requires the right step-size, Newton’s method converges naturally to the local minimum without the requirement of step-size tuning.</p>
</div></blockquote>
<p>To expand upon this, assume we have a linear system of equations</p>
<div class="math notranslate nohighlight" id="equation-linear-system">
<span class="eqno">(4.33)<a class="headerlink" href="#equation-linear-system" title="Permalink to this equation">#</a></span>\[M {\bf{x}} = {\bf{q}},\]</div>
<p>then Newton’s method can solve this system in <strong>one</strong> step whereas gradient descent requires multiple steps with the right step size chosen. The downside to this is the expensive step of inverting matrix <span class="math notranslate nohighlight">\(M\)</span>. In our general case, this means we need to invert the expensive matrix <span class="math notranslate nohighlight">\(\nabla^{2} f(x_{0})\)</span>.</p>
<p>Another advantage of Newton’s method is that it does not suffer from individual coordinates being at completely different scales, e.g. the <span class="math notranslate nohighlight">\(y\)</span>-direction changes very fast, whereas the <span class="math notranslate nohighlight">\(z\)</span>-direction only changes very slowly. Gradient descent only handles these cases suboptimally, whereas Newton’s method does not suffer from this shortcoming.</p>
<blockquote>
<div><p>Note: The Hessian has to be positive definite, otherwise we would end up in a local maximum.</p>
</div></blockquote>
<p>A matrix <span class="math notranslate nohighlight">\(H\)</span> is positive definite if the real number <span class="math notranslate nohighlight">\(z^{\top} H z\)</span> is positive for every nonzero real-valued vector <span class="math notranslate nohighlight">\(z\)</span>.</p>
</section>
<section id="the-quasi-newton-approach">
<h3><span class="section-number">4.3.2. </span>The Quasi-Newton Approach<a class="headerlink" href="#the-quasi-newton-approach" title="Permalink to this heading">#</a></h3>
<p>With the main computational bottleneck of Newton’s approach being that the inversion of the Hessian matrix costs <span class="math notranslate nohighlight">\(\mathcal{O}(d^{3})\)</span> for a matrix of size <span class="math notranslate nohighlight">\(d \times d\)</span>, there exist multiple approaches which try to circumvent this costly operation.</p>
<section id="the-secant-method">
<h4><span class="section-number">4.3.2.1. </span>The Secant Method<a class="headerlink" href="#the-secant-method" title="Permalink to this heading">#</a></h4>
<p>The secant method is an alternative to Newton’s method, which consciously does not use derivatives and has hence much less stringent requirements on our objective function such as it not needing to be differentiable. We can replace the derivative in Newton’s method with the finite difference approximation, i.e.</p>
<div class="math notranslate nohighlight" id="equation-fd-approx">
<span class="eqno">(4.34)<a class="headerlink" href="#equation-fd-approx" title="Permalink to this equation">#</a></span>\[\frac{f(x_{t}) - f(x_{t-1})}{x_{t} - x_{t-1}} \approx f'(x_{t})\]</div>
<p>for a small region around <span class="math notranslate nohighlight">\(x_{t}\)</span>. The secant update step then takes the following form</p>
<div class="math notranslate nohighlight" id="equation-secant-update">
<span class="eqno">(4.35)<a class="headerlink" href="#equation-secant-update" title="Permalink to this equation">#</a></span>\[x_{t+1} = x_{t} - f(x_{t})\frac{x_{t} - x_{t-1}}{f(x_{t}) - f(x_{t-1})}, \quad t\geq 1\]</div>
<p>to approximate the Newton step at the detriment of having to choose two starting values <span class="math notranslate nohighlight">\(x_{0}\)</span>, and <span class="math notranslate nohighlight">\(x_{1}\)</span> here. Figuratively speaking, the approach looks like this:</p>
<figure class="align-center" id="secant-method">
<a class="reference internal image-reference" href="../_images/secant_method.png"><img alt="../_images/secant_method.png" src="../_images/secant_method.png" style="width: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.9 </span><span class="caption-text">Secant method (Source: <span id="id9">[<a class="reference internal" href="../references.html#id14" title="Bernd Gärtner and Martin Jaggi. Optimization for machine learning. 2023. URL: https://github.com/epfml/OptML_course/blob/master/lecture_notes/lecture-notes.pdf.">Gärtner and Jaggi, 2023</a>]</span>, Chapter 8).</span><a class="headerlink" href="#secant-method" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>What this approach then does is to construct the line through the two points <span class="math notranslate nohighlight">\((x_{t-1}, f(x_{t-1}))\)</span>, and <span class="math notranslate nohighlight">\((x_{t}, f(x_{t}))\)</span> on the graph of f, the next iteration is then given by the point where the line intersects the x-axis.</p>
<p>When our function is first-order differentiable, we can also use the secant method to derive a second-derivative-free version of Newton’s method for optimization.</p>
<div class="math notranslate nohighlight" id="equation-secant-newton">
<span class="eqno">(4.36)<a class="headerlink" href="#equation-secant-newton" title="Permalink to this equation">#</a></span>\[x_{t+1}=x_{t} - f'(x_{t})\frac{x_{t} - x_{t-1}}{f'(x_{t}) - f'(x_{t-1})}, \quad t \geq 1.\]</div>
</section>
<section id="quasi-newton-methods">
<h4><span class="section-number">4.3.2.2. </span>Quasi-Newton Methods<a class="headerlink" href="#quasi-newton-methods" title="Permalink to this heading">#</a></h4>
<p>If we consider the so-called secant condition, then we have the following approximation</p>
<div class="math notranslate nohighlight" id="equation-secant-approx">
<span class="eqno">(4.37)<a class="headerlink" href="#equation-secant-approx" title="Permalink to this equation">#</a></span>\[H_{t} = \frac{f'(x_{t}) - f'(x_{t-1})}{x_{t} - x_{t-1}} \approx f''(x_{t}),\]</div>
<p>then the secant method works with the following update step using the above approximation</p>
<div class="math notranslate nohighlight" id="equation-secant-update2">
<span class="eqno">(4.38)<a class="headerlink" href="#equation-secant-update2" title="Permalink to this equation">#</a></span>\[x_{t+1} = x_{t} - H_{t}^{-1}f'(x_{t}), \quad t \geq 1.\]</div>
<p>For this approximation to hold we need to satisfy the <em>secant condition</em></p>
<div class="math notranslate nohighlight" id="equation-secant-conditions">
<span class="eqno">(4.39)<a class="headerlink" href="#equation-secant-conditions" title="Permalink to this equation">#</a></span>\[f'(x_{t}) - f'(x_{t-1}) = H_{t}(x_{t} - x_{t-1}).\]</div>
<p>Or generalized to higher dimensions</p>
<div class="math notranslate nohighlight" id="equation-secant-condition-vec">
<span class="eqno">(4.40)<a class="headerlink" href="#equation-secant-condition-vec" title="Permalink to this equation">#</a></span>\[\nabla f({\bf{x_{t}}}) - \nabla f({\bf{x_{t-1}}}) = H_{t}({\bf{x_{t}}} - {\bf{x_{t-1}}}).\]</div>
<p>Whenever this condition is now fulfilled in conjunction with a <em>symmetric matrix</em>, then we have a <strong>Quasi-Newton method</strong>. As our matrix <span class="math notranslate nohighlight">\(H_{t} \approx \nabla^{2}f(x_{t})\)</span> only fluctuates very little during periods of very fast convergence, and Newton’s method is optimal with one step, then we can presume</p>
<div class="math notranslate nohighlight" id="equation-quasi-newton-assumption1">
<span class="eqno">(4.41)<a class="headerlink" href="#equation-quasi-newton-assumption1" title="Permalink to this equation">#</a></span>\[H_{t} \approx H_{t-1}\]</div>
<p>and equally as much</p>
<div class="math notranslate nohighlight" id="equation-quasi-newton-assumption2">
<span class="eqno">(4.42)<a class="headerlink" href="#equation-quasi-newton-assumption2" title="Permalink to this equation">#</a></span>\[H_{t}^{-1} \approx H_{t-1}^{-1},\]</div>
<p>then we can (following Greenstadt’s approach) model <span class="math notranslate nohighlight">\(H_{t}\)</span> in the following fashion</p>
<div class="math notranslate nohighlight" id="equation-quasi-newton-h-decompositions">
<span class="eqno">(4.43)<a class="headerlink" href="#equation-quasi-newton-h-decompositions" title="Permalink to this equation">#</a></span>\[H_{t}^{-1} = H^{-1}_{t-1} + E_{t}.\]</div>
<p>I.e. our matrix <span class="math notranslate nohighlight">\(H_{t}\)</span>, the Hessian, only changes by a minor error matrix <span class="math notranslate nohighlight">\(E_t\)</span>. These errors should also be as small as possible.</p>
</section>
<section id="bfgs">
<h4><span class="section-number">4.3.2.3. </span>BFGS<a class="headerlink" href="#bfgs" title="Permalink to this heading">#</a></h4>
<p>(Broyden–Fletcher–Goldfarb–Shanno)</p>
<p>For the BFGS algorithm, this update matrix then assumes the form</p>
<div class="math notranslate nohighlight" id="equation-bfgs-approx">
<span class="eqno">(4.44)<a class="headerlink" href="#equation-bfgs-approx" title="Permalink to this equation">#</a></span>\[E = \frac{1}{{\bf{y}}^{\top} {\bf{\sigma}}} \left( -H {\bf{y}} {\bf{\sigma}}^{\top} - {\bf{\sigma}} {\bf{y}}^{\top} H + (1 + \frac{{\bf{y}}^{\top}H{\bf{y}}}{{\bf{y}}^{\top} {\bf{\sigma}}}) {\bf{\sigma}} {\bf{\sigma}}^{\top} \right),\]</div>
<p>where we simplified notation to <span class="math notranslate nohighlight">\(H=H_{t-1}^{-1}\)</span>, <span class="math notranslate nohighlight">\({\bf{\sigma}} = x_{t} - x_{t-1}\)</span>, and <span class="math notranslate nohighlight">\(y = \nabla f(x_{t}) - \nabla f(x_{t-1})\)</span>.</p>
<p>One of the core advantages of BFGS is that if <span class="math notranslate nohighlight">\(H':=H_{t}^{-1}\)</span> is positive definite, then the update <span class="math notranslate nohighlight">\(E\)</span> maintains this positive definite attribute and as such behaves like a proper inverse Hessian. In addition, the cost of computation drops from the original <span class="math notranslate nohighlight">\(\mathcal{O}(d^{3})\)</span> for a matrix of size <span class="math notranslate nohighlight">\(d \times d\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(d^{2})\)</span> for the BFGS approach. Scaling the update step, the individual iteration then becomes</p>
<div class="math notranslate nohighlight" id="equation-bfgs-update">
<span class="eqno">(4.45)<a class="headerlink" href="#equation-bfgs-update" title="Permalink to this equation">#</a></span>\[x_{t+1} = x_{t} - \alpha_{t} H_{t}^{-1} \nabla f(x_{t}), \quad t \geq 1\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> can be chosen such that a line search is performed, and where <span class="math notranslate nohighlight">\(H_{t}^{-1}\)</span> is the BFGS approximation. In a pseudo-algorithmic form this then looks the following way:</p>
<figure class="align-center" id="bfgs-alg">
<a class="reference internal image-reference" href="../_images/bfgs_alg.png"><img alt="../_images/bfgs_alg.png" src="../_images/bfgs_alg.png" style="width: 280px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.10 </span><span class="caption-text">BFGS algorithm (Source: <span id="id10">[<a class="reference internal" href="../references.html#id14" title="Bernd Gärtner and Martin Jaggi. Optimization for machine learning. 2023. URL: https://github.com/epfml/OptML_course/blob/master/lecture_notes/lecture-notes.pdf.">Gärtner and Jaggi, 2023</a>]</span>, Chapter 8).</span><a class="headerlink" href="#bfgs-alg" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="l-bfgs">
<h4><span class="section-number">4.3.2.4. </span>L-BFGS<a class="headerlink" href="#l-bfgs" title="Permalink to this heading">#</a></h4>
<p>(Limited-memory BFGS)</p>
<p>Especially in high dimensions a cost of <span class="math notranslate nohighlight">\(\mathcal{O}(d^{2})\)</span> may still be too prohibitive. Only using information from the past <span class="math notranslate nohighlight">\(m\)</span> iterations for <span class="math notranslate nohighlight">\(m\)</span> being a small value, L-BFGS then approximates the entire <span class="math notranslate nohighlight">\(H'\nabla f(x_{t})\)</span> term. L-BFGS then builds on modeling <span class="math notranslate nohighlight">\(H'\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-lbfgs-approx">
<span class="eqno">(4.46)<a class="headerlink" href="#equation-lbfgs-approx" title="Permalink to this equation">#</a></span>\[H' = \left( I - \frac{\sigma y^{\top}}{y^{\top}\sigma} \right) H \left( I - \frac{y \sigma^{\top}}{y^{\top}\sigma} \right) + \frac{\sigma \sigma^{\top}}{y^{\top} \sigma}.\]</div>
<p>Then, we are able to utilize an oracle to compute</p>
<div class="math notranslate nohighlight" id="equation-lbfgs-s">
<span class="eqno">(4.47)<a class="headerlink" href="#equation-lbfgs-s" title="Permalink to this equation">#</a></span>\[s= H g\]</div>
<p>for any vector <span class="math notranslate nohighlight">\(g\)</span>. Then <span class="math notranslate nohighlight">\(s' = H'g'\)</span> can be computed with one oracle call and <span class="math notranslate nohighlight">\(\mathcal{O}(d)\)</span> additional arithmetic operations, assuming that <span class="math notranslate nohighlight">\(\sigma\)</span>, and <span class="math notranslate nohighlight">\(y\)</span> are known. We then define <span class="math notranslate nohighlight">\(\sigma\)</span>, and <span class="math notranslate nohighlight">\(y\)</span> in the following fashion</p>
<div class="math notranslate nohighlight" id="equation-lbfsg-approx2">
<span class="eqno">(4.48)<a class="headerlink" href="#equation-lbfsg-approx2" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{align}
    \sigma_{k} &amp;= x_{k} - x_{k-1} \\
    y_{k} &amp;= \nabla f(x_{k}) - \nabla f(x_{k-1})
\end{align}\end{split}\]</div>
<p>The L-BFGS algorithm is then given by</p>
<figure class="align-center" id="lbfgs-alg">
<a class="reference internal image-reference" href="../_images/lbfgs_alg.png"><img alt="../_images/lbfgs_alg.png" src="../_images/lbfgs_alg.png" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.11 </span><span class="caption-text">L-BFGS algorithm (Source: <span id="id11">[<a class="reference internal" href="../references.html#id14" title="Bernd Gärtner and Martin Jaggi. Optimization for machine learning. 2023. URL: https://github.com/epfml/OptML_course/blob/master/lecture_notes/lecture-notes.pdf.">Gärtner and Jaggi, 2023</a>]</span>, Chapter 8).</span><a class="headerlink" href="#lbfgs-alg" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>where in the case of the recursion bottoming out prematurely at a point <span class="math notranslate nohighlight">\(k=t-m\)</span>, then we pretend that we just started the computation at that point and use <span class="math notranslate nohighlight">\(H_{0}\)</span>.</p>
</section>
</section>
</section>
<section id="derivative-free-optimization-dfo">
<h2><span class="section-number">4.4. </span>Derivative-Free Optimization (DFO)<a class="headerlink" href="#derivative-free-optimization-dfo" title="Permalink to this heading">#</a></h2>
<p>(also <em>Blackbox optimizations</em>)</p>
<p>If we are unable to compute gradients for any reason, then we need to rely on derivative-free optimization (DFO). This is most commonly used in blackbox function optimization, or discrete optimization.</p>
<blockquote>
<div><p>If you as an engineer would have to optimize the design of some model given to you, but where you are not allowed to touch the code or even look at the code of the model, but only query it for outputs, then this would constitute a case of blackbox function optimization.</p>
</div></blockquote>
<p>There exist a number of approaches for such problems, which all depend on the cost of evaluation of our function.</p>
<ul class="simple">
<li><p>Expensive function</p>
<ul>
<li><p>Bayesian optimization</p></li>
</ul>
</li>
<li><p>Cheap function</p>
<ul>
<li><p>Stochastic local search</p></li>
<li><p>Evolutionary search</p></li>
</ul>
</li>
</ul>
<p>In local search, for example, we replace the entire gradient update with</p>
<div class="math notranslate nohighlight" id="equation-local-search">
<span class="eqno">(4.49)<a class="headerlink" href="#equation-local-search" title="Permalink to this equation">#</a></span>\[x_{t+1} = \underset{x \in nbr(x_{t})}{\text{argmax}}\]</div>
<p>where <span class="math notranslate nohighlight">\(nbr\)</span> is the neighborhood of the point <span class="math notranslate nohighlight">\(x_{t}\)</span>. This approach is also colloquially known as hill climbing, steepest ascent, or greedy search. In stochastic local search, we would then define a probability distribution over the uphill neighbors proportional to how much they improve our function and then sample at random. A second stochastic option is to start again from a different random starting point whenever we reach a local maximum. This approach is known as random restart hill climbing.</p>
<p>An effective strategy here is also random search, which should be the go-to baseline one attempts first when approaching a new problem. Here an iterate <span class="math notranslate nohighlight">\(x_{t+1}\)</span> is chosen uniformly at random from the set of iterates. An alternative, which has been proven to be less efficient (see <a class="reference external" href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a> by Bergstra and Bengio, 2012), is the grid search, which chooses hyperparameters equidistantly in the same range used for random search.</p>
<figure class="align-center" id="grid-vs-random-search">
<a class="reference internal image-reference" href="../_images/grid_vs_random_search.png"><img alt="../_images/grid_vs_random_search.png" src="../_images/grid_vs_random_search.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.12 </span><span class="caption-text">Grid search vs random search (Source: <a class="reference external" href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a>).</span><a class="headerlink" href="#grid-vs-random-search" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>If instead of throwing away our “old” good candidates keep them in a <em>population</em> of good candidates, then we arrive at <em>evolutionary algorithms</em>. Here we maintain a population of <span class="math notranslate nohighlight">\(K\)</span> good candidates, which we then try to improve at each step. The advantage here is that evolutionary algorithms are embarrassingly parallel and are as such highly scalable.</p>
</section>
<section id="further-references">
<h2><span class="section-number">4.5. </span>Further References<a class="headerlink" href="#further-references" title="Permalink to this heading">#</a></h2>
<p><strong>Basics of Optimization</strong></p>
<ul class="simple">
<li><p><span id="id12">[<a class="reference internal" href="../references.html#id10" title="Moritz Hardt and Benjamin Recht. Patterns, predictions, and actions: Foundations of machine learning. Princeton University Press, 2022. URL: https://mlstory.org/.">Hardt and Recht, 2022</a>]</span>, Chapter 5</p></li>
<li><p><span id="id13">[<a class="reference internal" href="../references.html#id11" title="Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, 2004. ISBN 978-0-521-83378-3. URL: https://web.stanford.edu/~boyd/cvxbook/.">Boyd and Vandenberghe, 2004</a>]</span>, Chapter 1</p></li>
</ul>
<p><strong>First-Order Optimization</strong></p>
<ul class="simple">
<li><p><span id="id14">[<a class="reference internal" href="../references.html#id10" title="Moritz Hardt and Benjamin Recht. Patterns, predictions, and actions: Foundations of machine learning. Princeton University Press, 2022. URL: https://mlstory.org/.">Hardt and Recht, 2022</a>]</span>, Chapter 5</p></li>
<li><p><span id="id15">[<a class="reference internal" href="../references.html#id12" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning. arXiv preprint arXiv:2106.11342, 2021. URL: https://d2l.ai/.">Zhang <em>et al.</em>, 2021</a>]</span>, Chapter Optimization Algorithms</p></li>
<li><p><span id="id16">[<a class="reference internal" href="../references.html#id11" title="Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, 2004. ISBN 978-0-521-83378-3. URL: https://web.stanford.edu/~boyd/cvxbook/.">Boyd and Vandenberghe, 2004</a>]</span>, Chapter 9</p></li>
<li><p><a class="reference external" href="https://ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a>, S. Ruder, 2016</p></li>
</ul>
<p><strong>Second-Order Optimization</strong></p>
<ul class="simple">
<li><p><span id="id17">[<a class="reference internal" href="../references.html#id14" title="Bernd Gärtner and Martin Jaggi. Optimization for machine learning. 2023. URL: https://github.com/epfml/OptML_course/blob/master/lecture_notes/lecture-notes.pdf.">Gärtner and Jaggi, 2023</a>]</span>, Chapters 7 and 8</p></li>
<li><p><span id="id18">[<a class="reference internal" href="../references.html#id13" title="Jorge Nocedal and Stephen J. Wright. Numerical optimization. Springer, 2006. ISBN 978-0-387-30303-1. URL: https://link.springer.com/book/10.1007/978-0-387-40065-5.">Nocedal and Wright, 2006</a>]</span>, Chapters 3 and 6</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lecture"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="bayes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Bayesian methods</p>
      </div>
    </a>
    <a class="right-next"
       href="tricks.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Tricks of Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-optimization">4.1. Basics of Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity">4.1.1. Convexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-functions">4.1.2. Cost Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-based-methods">4.2. Gradient-based Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">4.2.1. Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum">4.2.1.1. Momentum</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adam">4.2.2. Adam</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">4.2.3. Stochastic Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#minibatching">4.2.3.1. Minibatching</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#second-order-methods">4.3. Second-Order Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-s-method">4.3.1. Newton’s method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-quasi-newton-approach">4.3.2. The Quasi-Newton Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-secant-method">4.3.2.1. The Secant Method</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quasi-newton-methods">4.3.2.2. Quasi-Newton Methods</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bfgs">4.3.2.3. BFGS</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#l-bfgs">4.3.2.4. L-BFGS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-free-optimization-dfo">4.4. Derivative-Free Optimization (DFO)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-references">4.5. Further References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By N. Adams, L. Paehler, A. Toshev
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022,2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>